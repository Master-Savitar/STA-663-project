{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\__ init \\__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:42.169612Z",
     "start_time": "2021-06-28T08:48:42.167637Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:44.022961Z",
     "start_time": "2021-06-28T08:48:43.099245Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import random\n",
    "\n",
    "# For automatic dataset downloading\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from io import StringIO, BytesIO\n",
    "import shutil\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:44.044934Z",
     "start_time": "2021-06-28T08:48:44.036918Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def data_iterator(data, batch_size):\n",
    "#     \"\"\"\n",
    "#     A simple data iterator from https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/\n",
    "#     :param data: list of numpy tensors that need to be randomly batched across their first dimension.\n",
    "#     :param batch_size: int, batch_size of data_iterator.\n",
    "#     Assumes same first dimension size of all numpy tensors.\n",
    "#     :return: iterator over batches of numpy tensors\n",
    "#     \"\"\"\n",
    "#     # shuffle labels and features\n",
    "#     max_idx = len(data[0])\n",
    "#     idxs = np.arange(0, max_idx)\n",
    "#     np.random.shuffle(idxs)       \n",
    "#     shuf_data = [dat[idxs] for dat in data]\n",
    "\n",
    "#     # Does not yield last remainder of size less than batch_size\n",
    "#     for i in range(max_idx//batch_size):\n",
    "#         data_batch = [dat[i*batch_size:(i+1)*batch_size] for dat in shuf_data]\n",
    "#         yield data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**np.random.shuffle( )** will change `idxs` forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:44.779629Z",
     "start_time": "2021-06-28T08:48:44.773630Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_data(data):\n",
    "    \"\"\"\n",
    "    Map data to proper indices in case they are not in a continues [0, N) range\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.int32 arrays\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mapped_data : np.int32 arrays\n",
    "    n : length of mapped_data\n",
    "\n",
    "    \"\"\"\n",
    "    uniq = list(set(data))\n",
    "\n",
    "    id_dict = {old: new for new, old in enumerate(sorted(uniq))}\n",
    "    data = np.array(list(map(lambda x: id_dict[x], data)))\n",
    "    n = len(uniq)\n",
    "\n",
    "    return data, id_dict, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:45.562550Z",
     "start_time": "2021-06-28T08:48:45.555575Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def download_dataset(dataset, files, data_dir):\n",
    "#     \"\"\" Downloads dataset if files are not present. \"\"\"\n",
    "\n",
    "#     if not np.all([os.path.isfile(data_dir + f) for f in files]):\n",
    "#         url = \"http://files.grouplens.org/datasets/movielens/\" + dataset.replace('_', '-') + '.zip'\n",
    "#         request = urlopen(url)\n",
    "\n",
    "#         print('Downloading %s dataset' % dataset)\n",
    "#         if dataset in ['ml_100k', 'ml_1m']:\n",
    "#             target_dir = 'data/' + dataset.replace('_', '-')\n",
    "#         elif dataset == 'ml_10m':\n",
    "#             target_dir = 'data/' + 'ml-10M100K'\n",
    "#         else:\n",
    "#             raise ValueError('Invalid dataset option %s' % dataset)\n",
    "\n",
    "#         with ZipFile(BytesIO(request.read())) as zip_ref:\n",
    "#             zip_ref.extractall('data/')\n",
    "\n",
    "#         source = [target_dir + '/' + s for s in os.listdir(target_dir)]\n",
    "#         destination = data_dir+'/'\n",
    "#         for f in source:\n",
    "#             shutil.copy(f, destination)\n",
    "\n",
    "#         shutil.rmtree(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:46.085947Z",
     "start_time": "2021-06-28T08:48:46.054020Z"
    },
    "code_folding": [
     147,
     218,
     237
    ]
   },
   "outputs": [],
   "source": [
    "def load_data(fname, seed=1234, verbose=True):\n",
    "    \"\"\" Loads dataset and creates adjacency matrix\n",
    "    and feature matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: str, dataset\n",
    "    seed: int, dataset shuffling seed\n",
    "    verbose: to print out statements or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    num_users : int\n",
    "        Number of users and items respectively\n",
    "\n",
    "    num_items : int\n",
    "\n",
    "    u_nodes : np.int32 arrays\n",
    "        User indices\n",
    "\n",
    "    v_nodes : np.int32 array\n",
    "        item (movie) indices\n",
    "\n",
    "    ratings : np.float32 array\n",
    "        User/item ratings s.t. ratings[k] is the rating given by user u_nodes[k] to\n",
    "        item v_nodes[k]. Note that that the all pairs u_nodes[k]/v_nodes[k] are unique, but\n",
    "        not necessarily all u_nodes[k] or all v_nodes[k] separately.\n",
    "\n",
    "    u_features: np.float32 array, or None\n",
    "        If present in dataset, contains the features of the users.\n",
    "\n",
    "    v_features: np.float32 array, or None\n",
    "        If present in dataset, contains the features of the users.\n",
    "\n",
    "    seed: int,\n",
    "        For datashuffling seed with pythons own random.shuffle, as in CF-NADE.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    u_features = None\n",
    "    v_features = None\n",
    "\n",
    "    print('Loading dataset', fname)\n",
    "\n",
    "    data_dir = 'data/' + fname\n",
    "\n",
    "    if fname == 'ml_100k':\n",
    "\n",
    "        # Check if files exist and download otherwise\n",
    "        files = ['/u.data', '/u.item', '/u.user']\n",
    "\n",
    "        download_dataset(fname, files, data_dir)\n",
    "\n",
    "        sep = '\\t'\n",
    "        filename = data_dir + files[0]\n",
    "\n",
    "        dtypes = {\n",
    "            'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "            'ratings': np.float32, 'timestamp': np.float64}\n",
    "        \n",
    "        # ----------------------------------------------------------------------------------------------------- # \n",
    "        # read\n",
    "        data = pd.read_csv(\n",
    "            filename, sep=sep, header=None,\n",
    "            names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "        \n",
    "        # ----------------------------------------------------------------------------------------------------- # \n",
    "        # shuffle here like cf-nade paper with python's own random class\n",
    "        # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "        # df.as_matrix() was depriciated after the version 0.23.0\n",
    "        # so I use df.to_numpy() instead\n",
    "        data_array = data.to_numpy().tolist()  \n",
    "        random.seed(seed)\n",
    "        random.shuffle(data_array)\n",
    "        data_array = np.array(data_array)\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "        v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "        ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "        \n",
    "        # map_data will give each entry new index\n",
    "        # which will be in in a continues [0, N) range\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        # adjust the datatype of u_nodes_ratings, v_nodes_ratings as well as ratings\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "        ratings = ratings.astype(np.float64)\n",
    "        \n",
    "        # ----------------------------------------------------------------------------------------------------- # \n",
    "        # Movie features (genres)\n",
    "        sep = r'|'\n",
    "        movie_file = data_dir + files[1]\n",
    "        movie_headers = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "                         'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "                         'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                         'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                         'Thriller', 'War', 'Western']\n",
    "        movie_df = pd.read_csv(movie_file, sep=sep, header=None,\n",
    "                               names=movie_headers, engine='python')\n",
    "\n",
    "        # genre_headers is still a numpy.array and starts from \"Action\"\n",
    "        genre_headers = movie_df.columns.values[6:]    \n",
    "        num_genres = genre_headers.shape[0]    # the number of features for items(movies)\n",
    "\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, g_vec in zip(movie_df['movie id'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "            # Check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            # movie_id is obtained from movie_df(movie_file), which corresponds to the old id in mapping\n",
    "            # we need to compare it with the key of v_dict and v_dict[movie_id] is the new id for item(movie)\n",
    "            if movie_id in v_dict.keys():\n",
    "                v_features[v_dict[movie_id], :] = g_vec\n",
    "        \n",
    "        # ----------------------------------------------------------------------------------------------------- # \n",
    "        # User features\n",
    "        sep = r'|'\n",
    "        users_file = data_dir + files[2]\n",
    "        users_headers = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        occupation = set(users_df['occupation'].values.tolist())\n",
    "\n",
    "        gender_dict = {'M': 0., 'F': 1.}\n",
    "        # i will start from 2 rather than 0 since we set `start` to 2\n",
    "        occupation_dict = {f: i for i, f in enumerate(occupation, start=2)}\n",
    "\n",
    "        num_feats = 2 + len(occupation_dict)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            # the datatype of row is Series\n",
    "            u_id = row['user id']\n",
    "            if u_id in u_dict.keys():\n",
    "                # age\n",
    "                u_features[u_dict[u_id], 0] = row['age']\n",
    "                # gender\n",
    "                u_features[u_dict[u_id], 1] = gender_dict[row['gender']]\n",
    "                # occupation\n",
    "                # we use one-hot to encode `occupation`\n",
    "                u_features[u_dict[u_id], occupation_dict[row['occupation']]] = 1.\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------- # \n",
    "        # use sparse matrix to store users' and items' features\n",
    "        u_features = sp.csr_matrix(u_features)\n",
    "        v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    elif fname == 'ml_1m':\n",
    "\n",
    "        # Check if files exist and download otherwise\n",
    "        files = ['/ratings.dat', '/movies.dat', '/users.dat']\n",
    "        download_dataset(fname, files, data_dir)\n",
    "\n",
    "        sep = r'\\:\\:'\n",
    "        filename = data_dir + files[0]\n",
    "\n",
    "        dtypes = {\n",
    "            'u_nodes': np.int64, 'v_nodes': np.int64,\n",
    "            'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "        # use engine='python' to ignore warning about switching to python backend when using regexp for sep\n",
    "        data = pd.read_csv(filename, sep=sep, header=None,\n",
    "                           names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], converters=dtypes, engine='python')\n",
    "\n",
    "        # shuffle here like cf-nade paper with python's own random class\n",
    "        # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "        data_array = data.as_matrix().tolist()\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data_array)\n",
    "        data_array = np.array(data_array)\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "        v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "        ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int64)\n",
    "        ratings = ratings.astype(np.float32)\n",
    "\n",
    "        # Load movie features\n",
    "        movies_file = data_dir + files[1]\n",
    "\n",
    "        movies_headers = ['movie_id', 'title', 'genre']\n",
    "        movies_df = pd.read_csv(movies_file, sep=sep, header=None,\n",
    "                                names=movies_headers, engine='python')\n",
    "\n",
    "        # Extracting all genres\n",
    "        genres = []\n",
    "        for s in movies_df['genre'].values:\n",
    "            genres.extend(s.split('|'))\n",
    "\n",
    "        genres = list(set(genres))\n",
    "        num_genres = len(genres)\n",
    "\n",
    "        genres_dict = {g: idx for idx, g in enumerate(genres)}\n",
    "\n",
    "        # Creating 0 or 1 valued features for all genres\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, s in zip(movies_df['movie_id'].values.tolist(), movies_df['genre'].values.tolist()):\n",
    "            # Check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                gen = s.split('|')\n",
    "                for g in gen:\n",
    "                    v_features[v_dict[movie_id], genres_dict[g]] = 1.\n",
    "\n",
    "        # Load user features\n",
    "        users_file = data_dir + files[2]\n",
    "        users_headers = ['user_id', 'gender', 'age', 'occupation', 'zip-code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        # Extracting all features\n",
    "        cols = users_df.columns.values[1:]\n",
    "\n",
    "        cntr = 0\n",
    "        feat_dicts = []\n",
    "        for header in cols:\n",
    "            d = dict()\n",
    "            feats = np.unique(users_df[header].values).tolist()\n",
    "            d.update({f: i for i, f in enumerate(feats, start=cntr)})\n",
    "            feat_dicts.append(d)\n",
    "            cntr += len(d)\n",
    "\n",
    "        num_feats = sum(len(d) for d in feat_dicts)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user_id']\n",
    "            if u_id in u_dict.keys():\n",
    "                for k, header in enumerate(cols):\n",
    "                    u_features[u_dict[u_id], feat_dicts[k][row[header]]] = 1.\n",
    "\n",
    "        u_features = sp.csr_matrix(u_features)\n",
    "        v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    elif fname == 'ml_10m':\n",
    "\n",
    "        # Check if files exist and download otherwise\n",
    "        files = ['/ratings.dat']\n",
    "        download_dataset(fname, files, data_dir)\n",
    "\n",
    "        sep = r'\\:\\:'\n",
    "\n",
    "        filename = data_dir + files[0]\n",
    "\n",
    "        dtypes = {\n",
    "            'u_nodes': np.int64, 'v_nodes': np.int64,\n",
    "            'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "        # use engine='python' to ignore warning about switching to python backend when using regexp for sep\n",
    "        data = pd.read_csv(filename, sep=sep, header=None,\n",
    "                           names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], converters=dtypes, engine='python')\n",
    "\n",
    "        # shuffle here like cf-nade paper with python's own random class\n",
    "        # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "        data_array = data.as_matrix().tolist()\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data_array)\n",
    "        data_array = np.array(data_array)\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "        v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "        ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int64)\n",
    "        ratings = ratings.astype(np.float32)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Dataset name not recognized: ' + fname)\n",
    "\n",
    "    if verbose:\n",
    "        print('Number of users = %d' % num_users)\n",
    "        print('Number of items = %d' % num_items)\n",
    "        print('Number of links = %d' % ratings.shape[0])\n",
    "        print('Fraction of positive links = %.4f' % (float(ratings.shape[0]) / (num_users * num_items),))\n",
    "\n",
    "    return num_users, num_items, u_nodes_ratings, v_nodes_ratings, ratings, u_features, v_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initializations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:48.962490Z",
     "start_time": "2021-06-28T08:48:46.786277Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:48.987399Z",
     "start_time": "2021-06-28T08:48:48.978415Z"
    },
    "code_folding": [
     0,
     8,
     22
    ]
   },
   "outputs": [],
   "source": [
    "def weight_variable_truncated_normal(input_dim, output_dim, name=\"\"):\n",
    "    \"\"\"Create a weight variable with truncated normal distribution, values\n",
    "    that are more than 2 stddev away from the mean are redrawn.\"\"\"\n",
    "\n",
    "    initial = tf.truncated_normal([input_dim, output_dim], stddev=0.5)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def weight_variable_random_uniform(input_dim, output_dim=None, name=\"\"):\n",
    "    \"\"\"Create a weight variable with variables drawn from a\n",
    "    random uniform distribution. Parameters used are taken from paper by\n",
    "    Xavier Glorot and Yoshua Bengio:\n",
    "    http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\"\"\"\n",
    "    if output_dim is not None:\n",
    "        init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "        initial = tf.random_uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    else:\n",
    "        init_range = np.sqrt(6.0 / input_dim)\n",
    "        initial = tf.random_uniform([input_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def weight_variable_random_uniform_relu(input_dim, output_dim, name=\"\"):\n",
    "    \"\"\"Create a weight variable with variables drawn from a\n",
    "    random uniform distribution. Parameters used are taken from paper by\n",
    "    Xavier Glorot and Yoshua Bengio:\n",
    "    http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n",
    "    and are optimized for ReLU activation function.\"\"\"\n",
    "\n",
    "    init_range = np.sqrt(2.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:50.018814Z",
     "start_time": "2021-06-28T08:48:50.010836Z"
    },
    "code_folding": [
     0,
     6,
     12,
     18,
     32
    ]
   },
   "outputs": [],
   "source": [
    "def bias_variable_truncated_normal(shape, name=\"\"):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.5)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def bias_variable_zero(shape, name=\"\"):\n",
    "    \"\"\"Create a bias variable initialized as zero.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def bias_variable_one(shape, name=\"\"):\n",
    "    \"\"\"Create a bias variable initialized as ones.\"\"\"\n",
    "    initial = tf.ones(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def orthogonal(shape, scale=1.1, name=None):\n",
    "    \"\"\"\n",
    "    From Lasagne. Reference: Saxe et al., http://arxiv.org/abs/1312.6120\n",
    "    \"\"\"\n",
    "    flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "    a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "\n",
    "    # pick the one with the correct shape\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.reshape(shape)\n",
    "    return tf.Variable(scale * q[:shape[0], :shape[1]], name=name, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def bias_variable_const(shape, val, name=\"\"):\n",
    "    \"\"\"Create a bias variable initialized as zero.\"\"\"\n",
    "    value = tf.to_float(val)\n",
    "    initial = tf.fill(shape, value, name=name)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:51.244997Z",
     "start_time": "2021-06-28T08:48:51.240010Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from gcmc.initializations import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:52.032297Z",
     "start_time": "2021-06-28T08:48:52.028306Z"
    }
   },
   "outputs": [],
   "source": [
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:52.568611Z",
     "start_time": "2021-06-28T08:48:52.564607Z"
    }
   },
   "outputs": [],
   "source": [
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:53.050875Z",
     "start_time": "2021-06-28T08:48:53.046879Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs\n",
    "    \"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:53.484093Z",
     "start_time": "2021-06-28T08:48:53.480129Z"
    }
   },
   "outputs": [],
   "source": [
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
    "    \"\"\"\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "\n",
    "    return pre_out * tf.div(1., keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    " \n",
    "+ In Python, the single-asterisk form of ***args** can be used as a parameter to send **a non-keyworded variable-length argument list** to functions.\n",
    "+ The double asterisk form of ****kwargs** is used to pass **a keyworded, variable-length argument dictionary** to a function.\n",
    "+ When ordering arguments within a function or function call, arguments need to occur in a particular order:\n",
    "    + Formal positional arguments\n",
    "    + *args\n",
    "    + Keyword arguments\n",
    "    + **kwargs\n",
    "    \n",
    "    def example2(arg_1, arg_2, \\*args, kw_1=\"shark\", kw_2=\"blobfish\", **kwargs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:54.781234Z",
     "start_time": "2021-06-28T08:48:54.773249Z"
    }
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "            Layers with common name share variables. (TODO)\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # **kwargs will pass a keyword, variable-length argument dictionary\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        # if \"logging\" doesn't exist in kwargs, logging will be False\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:55.642213Z",
     "start_time": "2021-06-28T08:48:55.627255Z"
    }
   },
   "outputs": [],
   "source": [
    "class StackGCN(Layer):\n",
    "    \"\"\"Graph convolution layer for bipartite graphs and sparse inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_dim, output_dim, \n",
    "                 support, support_t, num_support, \n",
    "                 u_features_nonzero=None, v_features_nonzero=None, \n",
    "                 sparse_inputs=False, dropout=0.,\n",
    "                 act=tf.nn.relu, \n",
    "                 share_user_item_weights=True, \n",
    "                 **kwargs):\n",
    "        \n",
    "        # Python 2.7\n",
    "        # super(StackGCN, self).__init__(**kwargs)\n",
    "        # Python 3.6\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        assert output_dim % num_support == 0, 'output_dim must be multiple of num_support for stackGC layer'\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights_u'] = weight_variable_random_uniform(input_dim, output_dim, name='weights_u')\n",
    "\n",
    "            if not share_user_item_weights:\n",
    "                self.vars['weights_v'] = weight_variable_random_uniform(input_dim, output_dim, name='weights_v')\n",
    "\n",
    "            else:\n",
    "                self.vars['weights_v'] = self.vars['weights_u']\n",
    "\n",
    "        self.weights_u = tf.split(value=self.vars['weights_u'], axis=1, num_or_size_splits=num_support)\n",
    "        self.weights_v = tf.split(value=self.vars['weights_v'], axis=1, num_or_size_splits=num_support)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.u_features_nonzero = u_features_nonzero\n",
    "        self.v_features_nonzero = v_features_nonzero\n",
    "        if sparse_inputs:\n",
    "            assert u_features_nonzero is not None and v_features_nonzero is not None, \\\n",
    "                'u_features_nonzero and v_features_nonzero can not be None when sparse_inputs is True'\n",
    "\n",
    "        # support should be the adjacency matrix, which is stored in the form of sparse matrix\n",
    "        self.support = tf.sparse_split(axis=1, num_split=num_support, sp_input=support)\n",
    "        self.support_transpose = tf.sparse_split(axis=1, num_split=num_support, sp_input=support_t)\n",
    "\n",
    "        # activation function (relu, etc.)\n",
    "        self.act = act\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x_u = inputs[0]\n",
    "        x_v = inputs[1]\n",
    "\n",
    "        if self.sparse_inputs:\n",
    "            # dropout_sparse is a function defined for sparse tensors\n",
    "            x_u = dropout_sparse(x_u, 1 - self.dropout, self.u_features_nonzero)\n",
    "            x_v = dropout_sparse(x_v, 1 - self.dropout, self.v_features_nonzero)\n",
    "        else:\n",
    "            x_u = tf.nn.dropout(x_u, 1 - self.dropout)\n",
    "            x_v = tf.nn.dropout(x_v, 1 - self.dropout)\n",
    "\n",
    "        supports_u = []\n",
    "        supports_v = []\n",
    "\n",
    "        # self.support has been splited along the axis 1\n",
    "        for i in range(len(self.support)):\n",
    "            tmp_u = dot(x_u, self.weights_u[i], sparse=self.sparse_inputs)\n",
    "            tmp_v = dot(x_v, self.weights_v[i], sparse=self.sparse_inputs)\n",
    "\n",
    "            support = self.support[i]\n",
    "            support_transpose = self.support_transpose[i]\n",
    "\n",
    "            supports_u.append(tf.sparse_tensor_dense_matmul(support, tmp_v))\n",
    "            supports_v.append(tf.sparse_tensor_dense_matmul(support_transpose, tmp_u))\n",
    "\n",
    "        z_u = tf.concat(axis=1, values=supports_u)\n",
    "        z_v = tf.concat(axis=1, values=supports_v)\n",
    "\n",
    "        u_outputs = self.act(z_u)\n",
    "        v_outputs = self.act(z_v)\n",
    "\n",
    "        return u_outputs, v_outputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs_u', inputs[0])\n",
    "                tf.summary.histogram(self.name + '/inputs_v', inputs[1])\n",
    "            outputs_u, outputs_v = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs_u', outputs_u)\n",
    "                tf.summary.histogram(self.name + '/outputs_v', outputs_v)\n",
    "            return outputs_u, outputs_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:56.096902Z",
     "start_time": "2021-06-28T08:48:56.075958Z"
    }
   },
   "outputs": [],
   "source": [
    "class OrdinalMixtureGCN(Layer):\n",
    "\n",
    "    \"\"\"Graph convolution layer for bipartite graphs and sparse inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, \n",
    "                 support, support_t, num_support, \n",
    "                 u_features_nonzero=None, v_features_nonzero=None, \n",
    "                 sparse_inputs=False, dropout=0.,\n",
    "                 act=tf.nn.relu, bias=False, \n",
    "                 share_user_item_weights=False, \n",
    "                 self_connections=False, **kwargs):\n",
    "        \n",
    "        # usage in Python 2\n",
    "        # super(OrdinalMixtureGCN, self).__init__(**kwargs)\n",
    "        # Python 3\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "\n",
    "            self.vars['weights_u'] = tf.stack([weight_variable_random_uniform(input_dim, output_dim,\n",
    "                                                                             name='weights_u_%d' % i)\n",
    "                                              for i in range(num_support)], axis=0)\n",
    "\n",
    "            if bias:\n",
    "                self.vars['bias_u'] = bias_variable_const([output_dim], 0.01, name=\"bias_u\")\n",
    "\n",
    "            if not share_user_item_weights:\n",
    "                self.vars['weights_v'] = tf.stack([weight_variable_random_uniform(input_dim, output_dim,\n",
    "                                                                                 name='weights_v_%d' % i)\n",
    "                                                  for i in range(num_support)], axis=0)\n",
    "\n",
    "                if bias:\n",
    "                    self.vars['bias_v'] = bias_variable_const([output_dim], 0.01, name=\"bias_v\")\n",
    "\n",
    "            else:\n",
    "                self.vars['weights_v'] = self.vars['weights_u']\n",
    "                if bias:\n",
    "                    self.vars['bias_v'] = self.vars['bias_u']\n",
    "\n",
    "        self.weights_u = self.vars['weights_u']\n",
    "        self.weights_v = self.vars['weights_v']\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.u_features_nonzero = u_features_nonzero\n",
    "        self.v_features_nonzero = v_features_nonzero\n",
    "        if sparse_inputs:\n",
    "            assert u_features_nonzero is not None and v_features_nonzero is not None, \\\n",
    "                'u_features_nonzero and v_features_nonzero can not be None when sparse_inputs is True'\n",
    "\n",
    "        self.self_connections = self_connections\n",
    "\n",
    "        self.bias = bias\n",
    "        support = tf.sparse_split(axis=1, num_split=num_support, sp_input=support)\n",
    "\n",
    "        support_t = tf.sparse_split(axis=1, num_split=num_support, sp_input=support_t)\n",
    "\n",
    "        if self_connections:\n",
    "            self.support = support[:-1]\n",
    "            self.support_transpose = support_t[:-1]\n",
    "            self.u_self_connections = support[-1]\n",
    "            self.v_self_connections = support_t[-1]\n",
    "            self.weights_u = self.weights_u[:-1]\n",
    "            self.weights_v = self.weights_v[:-1]\n",
    "            self.weights_u_self_conn = self.weights_u[-1]\n",
    "            self.weights_v_self_conn = self.weights_v[-1]\n",
    "\n",
    "        else:\n",
    "            self.support = support\n",
    "            self.support_transpose = support_t\n",
    "            self.u_self_connections = None\n",
    "            self.v_self_connections = None\n",
    "            self.weights_u_self_conn = None\n",
    "            self.weights_v_self_conn = None\n",
    "\n",
    "        self.support_nnz = []\n",
    "        self.support_transpose_nnz = []\n",
    "        for i in range(len(self.support)):\n",
    "            nnz = tf.reduce_sum(tf.shape(self.support[i].values))\n",
    "            self.support_nnz.append(nnz)\n",
    "            self.support_transpose_nnz.append(nnz)\n",
    "\n",
    "        self.act = act\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "\n",
    "        if self.sparse_inputs:\n",
    "            x_u = dropout_sparse(inputs[0], 1 - self.dropout, self.u_features_nonzero)\n",
    "            x_v = dropout_sparse(inputs[1], 1 - self.dropout, self.v_features_nonzero)\n",
    "        else:\n",
    "            x_u = tf.nn.dropout(inputs[0], 1 - self.dropout)\n",
    "            x_v = tf.nn.dropout(inputs[1], 1 - self.dropout)\n",
    "\n",
    "        supports_u = []\n",
    "        supports_v = []\n",
    "\n",
    "        # self-connections with identity matrix as support\n",
    "        if self.self_connections:\n",
    "            uw = dot(x_u, self.weights_u_self_conn, sparse=self.sparse_inputs)\n",
    "            supports_u.append(tf.sparse_tensor_dense_matmul(self.u_self_connections, uw))\n",
    "\n",
    "            vw = dot(x_v, self.weights_v_self_conn, sparse=self.sparse_inputs)\n",
    "            supports_v.append(tf.sparse_tensor_dense_matmul(self.v_self_connections, vw))\n",
    "\n",
    "        wu = 0.\n",
    "        wv = 0.\n",
    "        for i in range(len(self.support)):\n",
    "            wu += self.weights_u[i]\n",
    "            wv += self.weights_v[i]\n",
    "\n",
    "            # multiply feature matrices with weights\n",
    "            tmp_u = dot(x_u, wu, sparse=self.sparse_inputs)\n",
    "\n",
    "            tmp_v = dot(x_v, wv, sparse=self.sparse_inputs)\n",
    "\n",
    "            support = self.support[i]\n",
    "            support_transpose = self.support_transpose[i]\n",
    "\n",
    "            # then multiply with rating matrices\n",
    "            supports_u.append(tf.sparse_tensor_dense_matmul(support, tmp_v))\n",
    "            supports_v.append(tf.sparse_tensor_dense_matmul(support_transpose, tmp_u))\n",
    "\n",
    "        z_u = tf.add_n(supports_u)\n",
    "        z_v = tf.add_n(supports_v)\n",
    "\n",
    "        if self.bias:\n",
    "            z_u = tf.nn.bias_add(z_u, self.vars['bias_u'])\n",
    "            z_v = tf.nn.bias_add(z_v, self.vars['bias_v'])\n",
    "\n",
    "        u_outputs = self.act(z_u)\n",
    "        v_outputs = self.act(z_v)\n",
    "\n",
    "        return u_outputs, v_outputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs_u', inputs[0])\n",
    "                tf.summary.histogram(self.name + '/inputs_v', inputs[1])\n",
    "            outputs_u, outputs_v = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs_u', outputs_u)\n",
    "                tf.summary.histogram(self.name + '/outputs_v', outputs_v)\n",
    "            return outputs_u, outputs_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:56.946522Z",
     "start_time": "2021-06-28T08:48:56.934563Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer for two types of nodes in a bipartite graph. \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_dim, output_dim, \n",
    "                 dropout=0., \n",
    "                 act=tf.nn.relu, \n",
    "                 share_user_item_weights=False,\n",
    "                 bias=False, \n",
    "                 **kwargs):\n",
    "\n",
    "        # super(Dense, self).__init__(**kwargs)    Python 2.7\n",
    "        # in Python 3.7\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            if not share_user_item_weights:\n",
    "                # without weight sharing \n",
    "                self.vars['weights_u'] = weight_variable_random_uniform(input_dim, output_dim, name=\"weights_u\")\n",
    "                self.vars['weights_v'] = weight_variable_random_uniform(input_dim, output_dim, name=\"weights_v\")\n",
    "\n",
    "                if bias:\n",
    "                    self.vars['user_bias'] = bias_variable_truncated_normal([output_dim], name=\"bias_u\")\n",
    "                    self.vars['item_bias'] = bias_variable_truncated_normal([output_dim], name=\"bias_v\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                self.vars['weights_u'] = weight_variable_random_uniform(input_dim, output_dim, name=\"weights\")\n",
    "                self.vars['weights_v'] = self.vars['weights_u']\n",
    "\n",
    "                if bias:\n",
    "                    self.vars['user_bias'] = bias_variable_truncated_normal([output_dim], name=\"bias_u\")\n",
    "                    self.vars['item_bias'] = self.vars['user_bias']\n",
    "\n",
    "        self.bias = bias\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.act = act    # activation function \n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x_u = inputs[0]\n",
    "        x_u = tf.nn.dropout(x_u, 1 - self.dropout)\n",
    "        x_u = tf.matmul(x_u, self.vars['weights_u'])\n",
    "\n",
    "        x_v = inputs[1]\n",
    "        x_v = tf.nn.dropout(x_v, 1 - self.dropout)\n",
    "        x_v = tf.matmul(x_v, self.vars['weights_v'])\n",
    "\n",
    "        u_outputs = self.act(x_u)\n",
    "        v_outputs = self.act(x_v)\n",
    "\n",
    "        if self.bias:\n",
    "            u_outputs += self.vars['user_bias']\n",
    "            v_outputs += self.vars['item_bias']\n",
    "\n",
    "        return u_outputs, v_outputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/inputs_u', inputs[0])\n",
    "                tf.summary.histogram(self.name + '/inputs_v', inputs[1])\n",
    "                \n",
    "            outputs_u, outputs_v = self._call(inputs)\n",
    "            \n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs_u', outputs_u)\n",
    "                tf.summary.histogram(self.name + '/outputs_v', outputs_v)\n",
    "            return outputs_u, outputs_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BilinearMixture (Decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:57.853420Z",
     "start_time": "2021-06-28T08:48:57.838428Z"
    }
   },
   "outputs": [],
   "source": [
    "class BilinearMixture(Layer):\n",
    "    \"\"\"\n",
    "    Decoder model layer for link-prediction with ratings\n",
    "    To use in combination with bipartite layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, \n",
    "                 u_indices, v_indices, \n",
    "                 input_dim, \n",
    "                 num_users, num_items, \n",
    "                 user_item_bias=False,\n",
    "                 dropout=0., act=tf.nn.softmax, num_weights=3,\n",
    "                 diagonal=True, **kwargs):\n",
    "        \n",
    "        # usage in Python 2\n",
    "        # super(BilinearMixture, self).__init__(**kwargs)\n",
    "        # Python 3.6\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "\n",
    "            for i in range(num_weights):\n",
    "                if diagonal:\n",
    "                    #  Diagonal weight matrices for each class stored as vectors\n",
    "                    self.vars['weights_%d' % i] = weight_variable_random_uniform(1, input_dim, name='weights_%d' % i)\n",
    "\n",
    "                else:\n",
    "                    self.vars['weights_%d' % i] = orthogonal([input_dim, input_dim], name='weights_%d' % i)\n",
    "\n",
    "            # self.var[\"weights_scalars\"] is the matrix composed of a_{rs} noted in (8)\n",
    "            self.vars['weights_scalars'] = weight_variable_random_uniform(num_weights, num_classes,\n",
    "                                                                          name='weights_u_scalars')\n",
    "\n",
    "            if user_item_bias:\n",
    "                self.vars['user_bias'] = bias_variable_zero([num_users, num_classes], name='user_bias')\n",
    "                self.vars['item_bias'] = bias_variable_zero([num_items, num_classes], name='item_bias')\n",
    "\n",
    "        self.user_item_bias = user_item_bias\n",
    "\n",
    "        if diagonal:\n",
    "            self._multiply_inputs_weights = tf.multiply\n",
    "        else:\n",
    "            self._multiply_inputs_weights = tf.matmul\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_weights = num_weights\n",
    "        self.u_indices = u_indices\n",
    "        self.v_indices = v_indices\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "\n",
    "        u_inputs = tf.nn.dropout(inputs[0], 1 - self.dropout)\n",
    "        v_inputs = tf.nn.dropout(inputs[1], 1 - self.dropout)\n",
    "\n",
    "        u_inputs = tf.gather(u_inputs, self.u_indices)\n",
    "        v_inputs = tf.gather(v_inputs, self.v_indices)\n",
    "\n",
    "        if self.user_item_bias:\n",
    "            u_bias = tf.gather(self.vars['user_bias'], self.u_indices)\n",
    "            v_bias = tf.gather(self.vars['item_bias'], self.v_indices)\n",
    "        else:\n",
    "            u_bias = None\n",
    "            v_bias = None\n",
    "\n",
    "        basis_outputs = []\n",
    "        # calculate for each rating level\n",
    "        for i in range(self.num_weights):\n",
    "\n",
    "            u_w = self._multiply_inputs_weights(u_inputs, self.vars['weights_%d' % i])\n",
    "            x = tf.reduce_sum(tf.multiply(u_w, v_inputs), axis=1)\n",
    "\n",
    "            basis_outputs.append(x)\n",
    "\n",
    "        # Store outputs in (Nu x Nv) x num_classes tensor and apply activation function\n",
    "        basis_outputs = tf.stack(basis_outputs, axis=1)\n",
    "\n",
    "        outputs = tf.matmul(basis_outputs,  self.vars['weights_scalars'], transpose_b=False)\n",
    "\n",
    "        if self.user_item_bias:\n",
    "            outputs += u_bias\n",
    "            outputs += v_bias\n",
    "\n",
    "        outputs = self.act(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs_u', inputs[0])\n",
    "                tf.summary.histogram(self.name + '/inputs_v', inputs[1])\n",
    "\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:58.656290Z",
     "start_time": "2021-06-28T08:48:58.652268Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:59.062427Z",
     "start_time": "2021-06-28T08:48:59.058439Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    Accuracy for multiclass model.\n",
    "    :param preds: predictions\n",
    "    :param labels: ground truth labelt\n",
    "    :return: average accuracy\n",
    "    \"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.to_int64(labels))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:59.552666Z",
     "start_time": "2021-06-28T08:48:59.544651Z"
    }
   },
   "outputs": [],
   "source": [
    "def expected_rmse(logits, labels, class_values=None):\n",
    "    \"\"\"\n",
    "    Computes the root mean square error with the predictions\n",
    "    computed as average predictions. Note that without the average\n",
    "    this cannot be used as a loss function as it would not be differentiable.\n",
    "    :param logits: predicted logits\n",
    "    :param labels: ground truth label\n",
    "    :param class_values: rating values corresponding to each class.\n",
    "    :return: rmse\n",
    "    \"\"\"\n",
    "\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    if class_values is None:\n",
    "        scores = tf.to_float(tf.range(start=0, limit=logits.get_shape()[1]) + 1)\n",
    "        y = tf.to_float(labels) + 1.  # assumes class values are 1, ..., num_classes\n",
    "    else:\n",
    "        scores = class_values\n",
    "        y = tf.gather(class_values, labels)\n",
    "\n",
    "    pred_y = tf.reduce_sum(probs * scores, 1)\n",
    "\n",
    "    diff = tf.subtract(y, pred_y)\n",
    "    exp_rmse = tf.square(diff)\n",
    "    exp_rmse = tf.cast(exp_rmse, dtype=tf.float32)\n",
    "\n",
    "    return tf.sqrt(tf.reduce_mean(exp_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:48:59.968027Z",
     "start_time": "2021-06-28T08:48:59.961057Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmse(logits, labels, class_values=None):\n",
    "    \"\"\"\n",
    "    Computes the mean square error with the predictions\n",
    "    computed as average predictions. Note that without the average\n",
    "    this cannot be used as a loss function as it would not be differentiable.\n",
    "    :param logits: predicted logits\n",
    "    :param labels: ground truth labels for the ratings, 1-D array containing 0-num_classes-1 ratings\n",
    "    :param class_values: rating values corresponding to each class.\n",
    "    :return: mse\n",
    "    \"\"\"\n",
    "\n",
    "    if class_values is None:\n",
    "        y = tf.to_float(labels) + 1.  # assumes class values are 1, ..., num_classes\n",
    "    else:\n",
    "        y = tf.gather(class_values, labels)\n",
    "\n",
    "    pred_y = logits\n",
    "\n",
    "    diff = tf.subtract(y, pred_y)\n",
    "    mse = tf.square(diff)\n",
    "    mse = tf.cast(mse, dtype=tf.float32)\n",
    "\n",
    "    return tf.sqrt(tf.reduce_mean(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:00.379416Z",
     "start_time": "2021-06-28T08:49:00.375424Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(outputs, labels):\n",
    "    \"\"\" computes average softmax cross entropy \"\"\"\n",
    "\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=labels)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:01.025195Z",
     "start_time": "2021-06-28T08:49:01.020177Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from gcmc.layers import *\n",
    "\n",
    "from gcmc.metrics import softmax_accuracy, expected_rmse, softmax_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:01.305535Z",
     "start_time": "2021-06-28T08:49:01.302547Z"
    }
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:01.790692Z",
     "start_time": "2021-06-28T08:49:01.775717Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecommenderGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:03.035316Z",
     "start_time": "2021-06-28T08:49:03.020350Z"
    },
    "code_folding": [
     67
    ]
   },
   "outputs": [],
   "source": [
    "class RecommenderGAE(Model):\n",
    "    def __init__(self, \n",
    "                 placeholders, \n",
    "                 input_dim, \n",
    "                 num_classes, \n",
    "                 num_support,\n",
    "                 learning_rate, \n",
    "                 num_basis_functions, \n",
    "                 hidden, \n",
    "                 num_users, num_items, \n",
    "                 accum,\n",
    "                 self_connections=False, \n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.inputs = (placeholders['u_features'], placeholders['v_features'])\n",
    "        self.u_features_nonzero = placeholders['u_features_nonzero']\n",
    "        self.v_features_nonzero = placeholders['v_features_nonzero']\n",
    "        self.support = placeholders['support']\n",
    "        self.support_t = placeholders['support_t']\n",
    "        self.dropout = placeholders['dropout']\n",
    "        self.labels = placeholders['labels']\n",
    "        self.u_indices = placeholders['user_indices']\n",
    "        self.v_indices = placeholders['item_indices']\n",
    "        self.class_values = placeholders['class_values']\n",
    "\n",
    "        self.hidden = hidden\n",
    "        self.num_basis_functions = num_basis_functions\n",
    "        self.num_classes = num_classes\n",
    "        self.num_support = num_support\n",
    "        self.input_dim = input_dim\n",
    "        self.self_connections = self_connections\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.accum = accum\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # standard settings: beta1=0.9, beta2=0.999, epsilon=1.e-8\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1.e-8)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "        moving_average_decay = 0.995\n",
    "        self.variable_averages = tf.train.ExponentialMovingAverage(moving_average_decay, self.global_step)\n",
    "        self.variables_averages_op = self.variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "        with tf.control_dependencies([self.opt_op]):\n",
    "            self.training_op = tf.group(self.variables_averages_op)\n",
    "\n",
    "        self.embeddings = self.activations[2]\n",
    "\n",
    "        self._rmse()\n",
    "\n",
    "    def _loss(self):\n",
    "        self.loss += softmax_cross_entropy(self.outputs, self.labels)\n",
    "\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = softmax_accuracy(self.outputs, self.labels)\n",
    "\n",
    "    def _rmse(self):\n",
    "        self.rmse = expected_rmse(self.outputs, self.labels, self.class_values)\n",
    "\n",
    "        tf.summary.scalar('rmse_score', self.rmse)\n",
    "\n",
    "    def _build(self):\n",
    "        if self.accum == 'sum':\n",
    "            self.layers.append(OrdinalMixtureGCN(input_dim=self.input_dim,\n",
    "                                                 output_dim=self.hidden[0],\n",
    "                                                 support=self.support,\n",
    "                                                 support_t=self.support_t,\n",
    "                                                 num_support=self.num_support,\n",
    "                                                 u_features_nonzero=self.u_features_nonzero,\n",
    "                                                 v_features_nonzero=self.v_features_nonzero,\n",
    "                                                 sparse_inputs=True,\n",
    "                                                 act=tf.nn.relu,\n",
    "                                                 bias=False,\n",
    "                                                 dropout=self.dropout,\n",
    "                                                 logging=self.logging,\n",
    "                                                 share_user_item_weights=True,\n",
    "                                                 self_connections=False))\n",
    "\n",
    "        elif self.accum == 'stack':\n",
    "            self.layers.append(StackGCN(input_dim=self.input_dim,\n",
    "                                        output_dim=self.hidden[0],\n",
    "                                        support=self.support,\n",
    "                                        support_t=self.support_t,\n",
    "                                        num_support=self.num_support,\n",
    "                                        u_features_nonzero=self.u_features_nonzero,\n",
    "                                        v_features_nonzero=self.v_features_nonzero,\n",
    "                                        sparse_inputs=True,\n",
    "                                        act=tf.nn.relu,\n",
    "                                        dropout=self.dropout,\n",
    "                                        logging=self.logging,\n",
    "                                        share_user_item_weights=True))\n",
    "        else:\n",
    "            raise ValueError('accumulation function option invalid, can only be stack or sum.')\n",
    "\n",
    "        self.layers.append(Dense(input_dim=self.hidden[0],\n",
    "                                 output_dim=self.hidden[1],\n",
    "                                 act=lambda x: x,\n",
    "                                 dropout=self.dropout,\n",
    "                                 logging=self.logging,\n",
    "                                 share_user_item_weights=True))\n",
    "\n",
    "        self.layers.append(BilinearMixture(num_classes=self.num_classes,\n",
    "                                           u_indices=self.u_indices,\n",
    "                                           v_indices=self.v_indices,\n",
    "                                           input_dim=self.hidden[1],\n",
    "                                           num_users=self.num_users,\n",
    "                                           num_items=self.num_items,\n",
    "                                           user_item_bias=False,\n",
    "                                           dropout=0.,\n",
    "                                           act=lambda x: x,\n",
    "                                           num_weights=self.num_basis_functions,\n",
    "                                           logging=self.logging,\n",
    "                                           diagonal=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecommenderSideInfoGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:03.990475Z",
     "start_time": "2021-06-28T08:49:03.967537Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class RecommenderSideInfoGAE(Model):\n",
    "    def __init__(self,  placeholders, input_dim, feat_hidden_dim, num_classes, num_support,\n",
    "                 learning_rate, num_basis_functions, hidden, num_users, num_items, accum,\n",
    "                 num_side_features, self_connections=False, **kwargs):\n",
    "        # super(RecommenderSideInfoGAE, self).__init__(**kwargs)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.inputs = (placeholders['u_features'], placeholders['v_features'])\n",
    "        self.u_features_side = placeholders['u_features_side']\n",
    "        self.v_features_side = placeholders['v_features_side']\n",
    "\n",
    "        self.u_features_nonzero = placeholders['u_features_nonzero']\n",
    "        self.v_features_nonzero = placeholders['v_features_nonzero']\n",
    "        self.support = placeholders['support']\n",
    "        self.support_t = placeholders['support_t']\n",
    "        self.dropout = placeholders['dropout']\n",
    "        self.labels = placeholders['labels']\n",
    "        self.u_indices = placeholders['user_indices']\n",
    "        self.v_indices = placeholders['item_indices']\n",
    "        self.class_values = placeholders['class_values']\n",
    "\n",
    "        self.num_side_features = num_side_features\n",
    "        self.feat_hidden_dim = feat_hidden_dim\n",
    "        if num_side_features > 0:\n",
    "            self.u_features_side = placeholders['u_features_side']\n",
    "            self.v_features_side = placeholders['v_features_side']\n",
    "\n",
    "        else:\n",
    "            self.u_features_side = None\n",
    "            self.v_features_side = None\n",
    "\n",
    "        self.hidden = hidden\n",
    "        self.num_basis_functions = num_basis_functions\n",
    "        self.num_classes = num_classes\n",
    "        self.num_support = num_support\n",
    "        self.input_dim = input_dim\n",
    "        self.self_connections = self_connections\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.accum = accum\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # standard settings: beta1=0.9, beta2=0.999, epsilon=1.e-8\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1.e-8)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "        moving_average_decay = 0.995\n",
    "        self.variable_averages = tf.train.ExponentialMovingAverage(moving_average_decay, self.global_step)\n",
    "        self.variables_averages_op = self.variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "        with tf.control_dependencies([self.opt_op]):\n",
    "            self.training_op = tf.group(self.variables_averages_op)\n",
    "\n",
    "        self.embeddings = self.activations[0]\n",
    "\n",
    "        self._rmse()\n",
    "\n",
    "    def _loss(self):\n",
    "        self.loss += softmax_cross_entropy(self.outputs, self.labels)\n",
    "\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = softmax_accuracy(self.outputs, self.labels)\n",
    "\n",
    "    def _rmse(self):\n",
    "        self.rmse = expected_rmse(self.outputs, self.labels, self.class_values)\n",
    "\n",
    "        tf.summary.scalar('rmse_score', self.rmse)\n",
    "\n",
    "    def _build(self):\n",
    "        if self.accum == 'sum':\n",
    "            self.layers.append(OrdinalMixtureGCN(input_dim=self.input_dim,\n",
    "                                                 output_dim=self.hidden[0],\n",
    "                                                 support=self.support,\n",
    "                                                 support_t=self.support_t,\n",
    "                                                 num_support=self.num_support,\n",
    "                                                 u_features_nonzero=self.u_features_nonzero,\n",
    "                                                 v_features_nonzero=self.v_features_nonzero,\n",
    "                                                 sparse_inputs=True,\n",
    "                                                 act=tf.nn.relu,\n",
    "                                                 bias=False,\n",
    "                                                 dropout=self.dropout,\n",
    "                                                 logging=self.logging,\n",
    "                                                 share_user_item_weights=True,\n",
    "                                                 self_connections=self.self_connections))\n",
    "\n",
    "        elif self.accum == 'stack':\n",
    "            self.layers.append(StackGCN(input_dim=self.input_dim,\n",
    "                                        output_dim=self.hidden[0],\n",
    "                                        support=self.support,\n",
    "                                        support_t=self.support_t,\n",
    "                                        num_support=self.num_support,\n",
    "                                        u_features_nonzero=self.u_features_nonzero,\n",
    "                                        v_features_nonzero=self.v_features_nonzero,\n",
    "                                        sparse_inputs=True,\n",
    "                                        act=tf.nn.relu,\n",
    "                                        dropout=self.dropout,\n",
    "                                        logging=self.logging,\n",
    "                                        share_user_item_weights=True))\n",
    "\n",
    "        else:\n",
    "            raise ValueError('accumulation function option invalid, can only be stack or sum.')\n",
    "\n",
    "        self.layers.append(Dense(input_dim=self.num_side_features,\n",
    "                                 output_dim=self.feat_hidden_dim,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=0.,\n",
    "                                 logging=self.logging,\n",
    "                                 bias=True,\n",
    "                                 share_user_item_weights=False))\n",
    "\n",
    "        self.layers.append(Dense(input_dim=self.hidden[0]+self.feat_hidden_dim,\n",
    "                                 output_dim=self.hidden[1],\n",
    "                                 act=lambda x: x,\n",
    "                                 dropout=self.dropout,\n",
    "                                 logging=self.logging,\n",
    "                                 share_user_item_weights=False))\n",
    "\n",
    "        self.layers.append(BilinearMixture(num_classes=self.num_classes,\n",
    "                                           u_indices=self.u_indices,\n",
    "                                           v_indices=self.v_indices,\n",
    "                                           input_dim=self.hidden[1],\n",
    "                                           num_users=self.num_users,\n",
    "                                           num_items=self.num_items,\n",
    "                                           user_item_bias=False,\n",
    "                                           dropout=0.,\n",
    "                                           act=lambda x: x,\n",
    "                                           num_weights=self.num_basis_functions,\n",
    "                                           logging=self.logging,\n",
    "                                           diagonal=False))\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build split sequential layer model\n",
    "\n",
    "        # gcn layer\n",
    "        layer = self.layers[0]\n",
    "        gcn_hidden = layer(self.inputs)\n",
    "\n",
    "        # dense layer for features\n",
    "        layer = self.layers[1]\n",
    "        feat_hidden = layer([self.u_features_side, self.v_features_side])\n",
    "\n",
    "        # concat dense layer\n",
    "        layer = self.layers[2]\n",
    "\n",
    "        gcn_u = gcn_hidden[0]\n",
    "        gcn_v = gcn_hidden[1]\n",
    "        feat_u = feat_hidden[0]\n",
    "        feat_v = feat_hidden[1]\n",
    "\n",
    "        input_u = tf.concat(values=[gcn_u, feat_u], axis=1)\n",
    "        input_v = tf.concat(values=[gcn_v, feat_v], axis=1)\n",
    "\n",
    "        concat_hidden = layer([input_u, input_v])\n",
    "\n",
    "        self.activations.append(concat_hidden)\n",
    "\n",
    "        # Build sequential layer model\n",
    "        for layer in self.layers[3::]:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss, global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:04.903126Z",
     "start_time": "2021-06-28T08:49:04.896147Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import _pickle as pkl\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from gcmc.data_utils import load_data, map_data, download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:05.498962Z",
     "start_time": "2021-06-28T08:49:05.410196Z"
    },
    "code_folding": [
     0,
     18,
     44,
     59,
     90,
     103,
     186,
     311
    ]
   },
   "outputs": [],
   "source": [
    "def normalize_features(feat):\n",
    "\n",
    "    degree = np.asarray(feat.sum(1)).flatten()\n",
    "\n",
    "    # set zeros to inf to avoid dividing by zero\n",
    "    degree[degree == 0.] = np.inf\n",
    "\n",
    "    degree_inv = 1. / degree\n",
    "    degree_inv_mat = sp.diags([degree_inv], [0])\n",
    "    feat_norm = degree_inv_mat.dot(feat)\n",
    "\n",
    "    if feat_norm.nnz == 0:\n",
    "        print('ERROR: normalized adjacency matrix has only zero entries!!!!!')\n",
    "        exit\n",
    "\n",
    "    return feat_norm\n",
    "\n",
    "\n",
    "def load_matlab_file(path_file, name_field):\n",
    "    \"\"\"\n",
    "    load '.mat' files\n",
    "    inputs:\n",
    "        path_file, string containing the file path\n",
    "        name_field, string containig the field name (default='shape')\n",
    "    warning:\n",
    "        '.mat' files should be saved in the '-v7.3' format\n",
    "    \"\"\"\n",
    "    db = h5py.File(path_file, 'r')\n",
    "    ds = db[name_field]\n",
    "    try:\n",
    "        if 'ir' in ds.keys():\n",
    "            data = np.asarray(ds['data'])\n",
    "            ir = np.asarray(ds['ir'])\n",
    "            jc = np.asarray(ds['jc'])\n",
    "            out = sp.csc_matrix((data, ir, jc)).astype(np.float32)\n",
    "    except AttributeError:\n",
    "        # Transpose in case is a dense matrix because of the row- vs column- major ordering between python and matlab\n",
    "        out = np.asarray(ds).astype(np.float32).T\n",
    "\n",
    "    db.close()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def preprocess_user_item_features(u_features, v_features):\n",
    "    \"\"\"\n",
    "    Creates one big feature matrix out of user features and item features.\n",
    "    Stacks item features under the user features.\n",
    "    \"\"\"\n",
    "\n",
    "    zero_csr_u = sp.csr_matrix((u_features.shape[0], v_features.shape[1]), dtype=u_features.dtype)\n",
    "    zero_csr_v = sp.csr_matrix((v_features.shape[0], u_features.shape[1]), dtype=v_features.dtype)\n",
    "\n",
    "    u_features = sp.hstack([u_features, zero_csr_u], format='csr')\n",
    "    v_features = sp.hstack([zero_csr_v, v_features], format='csr')\n",
    "\n",
    "    return u_features, v_features\n",
    "\n",
    "\n",
    "def globally_normalize_bipartite_adjacency(adjacencies, verbose=False, symmetric=True):\n",
    "    \"\"\" Globally Normalizes set of bipartite adjacency matrices \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print('Symmetrically normalizing bipartite adj')\n",
    "    # degree_u and degree_v are row and column sums of adj+I\n",
    "\n",
    "    adj_tot = np.sum(adj for adj in adjacencies)\n",
    "    degree_u = np.asarray(adj_tot.sum(1)).flatten()\n",
    "    degree_v = np.asarray(adj_tot.sum(0)).flatten()\n",
    "\n",
    "    # set zeros to inf to avoid dividing by zero\n",
    "    degree_u[degree_u == 0.] = np.inf\n",
    "    degree_v[degree_v == 0.] = np.inf\n",
    "\n",
    "    degree_u_inv_sqrt = 1. / np.sqrt(degree_u)\n",
    "    degree_v_inv_sqrt = 1. / np.sqrt(degree_v)\n",
    "    degree_u_inv_sqrt_mat = sp.diags([degree_u_inv_sqrt], [0])\n",
    "    degree_v_inv_sqrt_mat = sp.diags([degree_v_inv_sqrt], [0])\n",
    "\n",
    "    degree_u_inv = degree_u_inv_sqrt_mat.dot(degree_u_inv_sqrt_mat)\n",
    "\n",
    "    if symmetric:\n",
    "        adj_norm = [degree_u_inv_sqrt_mat.dot(adj).dot(degree_v_inv_sqrt_mat) for adj in adjacencies]\n",
    "\n",
    "    else:\n",
    "        adj_norm = [degree_u_inv.dot(adj) for adj in adjacencies]\n",
    "\n",
    "    return adj_norm\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\" change of format for sparse matrix. This format is used\n",
    "    for the feed_dict where sparse matrices need to be linked to placeholders\n",
    "    representing sparse matrices. \"\"\"\n",
    "\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "\n",
    "def create_trainvaltest_split(dataset, seed=1234, testing=False, datasplit_path=None, datasplit_from_file=False,\n",
    "                              verbose=True):\n",
    "    \"\"\"\n",
    "    Splits data set into train/val/test sets from full bipartite adjacency matrix. Shuffling of dataset is done in\n",
    "    load_data function.\n",
    "    For each split computes 1-of-num_classes labels. Also computes training\n",
    "    adjacency matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    if datasplit_from_file and os.path.isfile(datasplit_path):\n",
    "        print('Reading dataset splits from file...')\n",
    "        with open(datasplit_path) as f:\n",
    "            num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = pkl.load(f)\n",
    "\n",
    "        if verbose:\n",
    "            print('Number of users = %d' % num_users)\n",
    "            print('Number of items = %d' % num_items)\n",
    "            print('Number of links = %d' % ratings.shape[0])\n",
    "            print('Fraction of positive links = %.4f' % (float(ratings.shape[0]) / (num_users * num_items),))\n",
    "\n",
    "    else:\n",
    "        num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = load_data(dataset, seed=seed,\n",
    "                                                                                            verbose=verbose)\n",
    "\n",
    "        with open(datasplit_path, 'w') as f:\n",
    "            pkl.dump([num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features], f)\n",
    "\n",
    "    neutral_rating = -1\n",
    "\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges\n",
    "    num_test = int(np.ceil(ratings.shape[0] * 0.1))\n",
    "    if dataset == 'ml_100k':\n",
    "        num_val = int(np.ceil(ratings.shape[0] * 0.9 * 0.05))\n",
    "    else:\n",
    "        num_val = int(np.ceil(ratings.shape[0] * 0.9 * 0.05))\n",
    "\n",
    "    num_train = ratings.shape[0] - num_val - num_test\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    train_idx = idx_nonzero[0:num_train]\n",
    "    val_idx = idx_nonzero[num_train:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    train_pairs_idx = pairs_nonzero[0:num_train]\n",
    "    val_pairs_idx = pairs_nonzero[num_train:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values\n",
    "\n",
    "\n",
    "def load_data_monti(dataset, testing=False):\n",
    "    \"\"\"\n",
    "    Loads data from Monti et al. paper.\n",
    "    \"\"\"\n",
    "\n",
    "    path_dataset = 'data/' + dataset + '/training_test_dataset.mat'\n",
    "\n",
    "    M = load_matlab_file(path_dataset, 'M')\n",
    "    Otraining = load_matlab_file(path_dataset, 'Otraining')\n",
    "    Otest = load_matlab_file(path_dataset, 'Otest')\n",
    "\n",
    "    num_users = M.shape[0]\n",
    "    num_items = M.shape[1]\n",
    "\n",
    "    if dataset == 'flixster':\n",
    "        Wrow = load_matlab_file(path_dataset, 'W_users')\n",
    "        Wcol = load_matlab_file(path_dataset, 'W_movies')\n",
    "        u_features = Wrow\n",
    "        v_features = Wcol\n",
    "        # print(num_items, v_features.shape)\n",
    "        # v_features = np.eye(num_items)\n",
    "\n",
    "    elif dataset == 'douban':\n",
    "        Wrow = load_matlab_file(path_dataset, 'W_users')\n",
    "        u_features = Wrow\n",
    "        v_features = np.eye(num_items)\n",
    "    elif dataset == 'yahoo_music':\n",
    "        Wcol = load_matlab_file(path_dataset, 'W_tracks')\n",
    "        u_features = np.eye(num_users)\n",
    "        v_features = Wcol\n",
    "\n",
    "    u_nodes_ratings = np.where(M)[0]\n",
    "    v_nodes_ratings = np.where(M)[1]\n",
    "    ratings = M[np.where(M)]\n",
    "\n",
    "    u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "    ratings = ratings.astype(np.float64)\n",
    "\n",
    "    u_nodes = u_nodes_ratings\n",
    "    v_nodes = v_nodes_ratings\n",
    "\n",
    "    print('number of users = ', len(set(u_nodes)))\n",
    "    print('number of item = ', len(set(v_nodes)))\n",
    "\n",
    "    neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "    # assumes that ratings_train contains at least one example of every rating type\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "\n",
    "    for i in range(len(u_nodes)):\n",
    "        assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges\n",
    "\n",
    "    num_train = np.where(Otraining)[0].shape[0]\n",
    "    num_test = np.where(Otest)[0].shape[0]\n",
    "    num_val = int(np.ceil(num_train * 0.2))\n",
    "    num_train = num_train - num_val\n",
    "\n",
    "    pairs_nonzero_train = np.array([[u, v] for u, v in zip(np.where(Otraining)[0], np.where(Otraining)[1])])\n",
    "    idx_nonzero_train = np.array([u * num_items + v for u, v in pairs_nonzero_train])\n",
    "\n",
    "    pairs_nonzero_test = np.array([[u, v] for u, v in zip(np.where(Otest)[0], np.where(Otest)[1])])\n",
    "    idx_nonzero_test = np.array([u * num_items + v for u, v in pairs_nonzero_test])\n",
    "\n",
    "    # Internally shuffle training set (before splitting off validation set)\n",
    "    rand_idx = range(len(idx_nonzero_train))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(rand_idx)\n",
    "    idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "    pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "    idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "    pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "    val_idx = idx_nonzero[0:num_val]\n",
    "    train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    assert(len(test_idx) == num_test)\n",
    "\n",
    "    val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "    train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    if u_features is not None:\n",
    "        u_features = sp.csr_matrix(u_features)\n",
    "        print(\"User features shape: \" + str(u_features.shape))\n",
    "\n",
    "    if v_features is not None:\n",
    "        v_features = sp.csr_matrix(v_features)\n",
    "        print(\"Item features shape: \" + str(v_features.shape))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values\n",
    "\n",
    "\n",
    "def load_official_trainvaltest_split(dataset, testing=False):\n",
    "    \"\"\"\n",
    "    Loads official train/test split and uses 10% of training samples for validaiton\n",
    "    For each split computes 1-of-num_classes labels. Also computes training\n",
    "    adjacency matrix. Assumes flattening happens everywhere in row-major fashion.\n",
    "    \"\"\"\n",
    "\n",
    "    sep = '\\t'\n",
    "\n",
    "    # Check if files exist and download otherwise\n",
    "    files = ['/u1.base', '/u1.test', '/u.item', '/u.user']\n",
    "    fname = dataset\n",
    "    data_dir = 'data/' + fname\n",
    "\n",
    "    download_dataset(fname, files, data_dir)\n",
    "\n",
    "    dtypes = {\n",
    "        'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "        'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "    filename_train = 'data/' + dataset + '/u1.base'\n",
    "    filename_test = 'data/' + dataset + '/u1.test'\n",
    "\n",
    "    data_train = pd.read_csv(\n",
    "        filename_train, sep=sep, header=None,\n",
    "        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "    data_test = pd.read_csv(\n",
    "        filename_test, sep=sep, header=None,\n",
    "        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "    data_array_train = data_train.as_matrix().tolist()\n",
    "    data_array_train = np.array(data_array_train)\n",
    "    data_array_test = data_test.as_matrix().tolist()\n",
    "    data_array_test = np.array(data_array_test)\n",
    "\n",
    "    data_array = np.concatenate([data_array_train, data_array_test], axis=0)\n",
    "\n",
    "    u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "    v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "    ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "    u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "    v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "    u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "    ratings = ratings.astype(np.float64)\n",
    "\n",
    "    u_nodes = u_nodes_ratings\n",
    "    v_nodes = v_nodes_ratings\n",
    "\n",
    "    neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "    # assumes that ratings_train contains at least one example of every rating type\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "\n",
    "    for i in range(len(u_nodes)):\n",
    "        assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges, see cf-nade code\n",
    "\n",
    "    num_train = data_array_train.shape[0]\n",
    "    num_test = data_array_test.shape[0]\n",
    "    num_val = int(np.ceil(num_train * 0.2))\n",
    "    num_train = num_train - num_val\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    for i in range(len(ratings)):\n",
    "        assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    idx_nonzero_train = idx_nonzero[0:num_train+num_val]\n",
    "    idx_nonzero_test = idx_nonzero[num_train+num_val:]\n",
    "\n",
    "    pairs_nonzero_train = pairs_nonzero[0:num_train+num_val]\n",
    "    pairs_nonzero_test = pairs_nonzero[num_train+num_val:]\n",
    "\n",
    "    # Internally shuffle training set (before splitting off validation set)\n",
    "    rand_idx = range(len(idx_nonzero_train))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(rand_idx)\n",
    "    idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "    pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "    idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "    pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "    val_idx = idx_nonzero[0:num_val]\n",
    "    train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    assert(len(test_idx) == num_test)\n",
    "\n",
    "    val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "    train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    if dataset =='ml_100k':\n",
    "\n",
    "        # movie features (genres)\n",
    "        sep = r'|'\n",
    "        movie_file = 'data/' + dataset + '/u.item'\n",
    "        movie_headers = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "                         'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "                         'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                         'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                         'Thriller', 'War', 'Western']\n",
    "        movie_df = pd.read_csv(movie_file, sep=sep, header=None,\n",
    "                               names=movie_headers, engine='python')\n",
    "\n",
    "        genre_headers = movie_df.columns.values[6:]\n",
    "        num_genres = genre_headers.shape[0]\n",
    "\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, g_vec in zip(movie_df['movie id'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "            # check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                v_features[v_dict[movie_id], :] = g_vec\n",
    "\n",
    "        # user features\n",
    "\n",
    "        sep = r'|'\n",
    "        users_file = 'data/' + dataset + '/u.user'\n",
    "        users_headers = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        occupation = set(users_df['occupation'].values.tolist())\n",
    "\n",
    "        age = users_df['age'].values\n",
    "        age_max = age.max()\n",
    "\n",
    "        gender_dict = {'M': 0., 'F': 1.}\n",
    "        occupation_dict = {f: i for i, f in enumerate(occupation, start=2)}\n",
    "\n",
    "        num_feats = 2 + len(occupation_dict)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user id']\n",
    "            if u_id in u_dict.keys():\n",
    "                # age\n",
    "                u_features[u_dict[u_id], 0] = row['age'] / np.float(age_max)\n",
    "                # gender\n",
    "                u_features[u_dict[u_id], 1] = gender_dict[row['gender']]\n",
    "                # occupation\n",
    "                u_features[u_dict[u_id], occupation_dict[row['occupation']]] = 1.\n",
    "\n",
    "    elif dataset == 'ml_1m':\n",
    "\n",
    "        # load movie features\n",
    "        movies_file = 'data/' + dataset + '/movies.dat'\n",
    "\n",
    "        movies_headers = ['movie_id', 'title', 'genre']\n",
    "        movies_df = pd.read_csv(movies_file, sep=sep, header=None,\n",
    "                                names=movies_headers, engine='python')\n",
    "\n",
    "        # extracting all genres\n",
    "        genres = []\n",
    "        for s in movies_df['genre'].values:\n",
    "            genres.extend(s.split('|'))\n",
    "\n",
    "        genres = list(set(genres))\n",
    "        num_genres = len(genres)\n",
    "\n",
    "        genres_dict = {g: idx for idx, g in enumerate(genres)}\n",
    "\n",
    "        # creating 0 or 1 valued features for all genres\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, s in zip(movies_df['movie_id'].values.tolist(), movies_df['genre'].values.tolist()):\n",
    "            # check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                gen = s.split('|')\n",
    "                for g in gen:\n",
    "                    v_features[v_dict[movie_id], genres_dict[g]] = 1.\n",
    "\n",
    "        # load user features\n",
    "        users_file = 'data/' + dataset + '/users.dat'\n",
    "        users_headers = ['user_id', 'gender', 'age', 'occupation', 'zip-code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        # extracting all features\n",
    "        cols = users_df.columns.values[1:]\n",
    "\n",
    "        cntr = 0\n",
    "        feat_dicts = []\n",
    "        for header in cols:\n",
    "            d = dict()\n",
    "            feats = np.unique(users_df[header].values).tolist()\n",
    "            d.update({f: i for i, f in enumerate(feats, start=cntr)})\n",
    "            feat_dicts.append(d)\n",
    "            cntr += len(d)\n",
    "\n",
    "        num_feats = sum(len(d) for d in feat_dicts)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user_id']\n",
    "            if u_id in u_dict.keys():\n",
    "                for k, header in enumerate(cols):\n",
    "                    u_features[u_dict[u_id], feat_dicts[k][row[header]]] = 1.\n",
    "    else:\n",
    "        raise ValueError('Invalid dataset option %s' % dataset)\n",
    "\n",
    "    u_features = sp.csr_matrix(u_features)\n",
    "    v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    print(\"User features shape: \"+str(u_features.shape))\n",
    "    print(\"Item features shape: \"+str(v_features.shape))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:06.355466Z",
     "start_time": "2021-06-28T08:49:06.342451Z"
    },
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from gcmc.preprocessing import create_trainvaltest_split, \\\n",
    "    sparse_to_tuple, preprocess_user_item_features, globally_normalize_bipartite_adjacency, \\\n",
    "    load_data_monti, load_official_trainvaltest_split, normalize_features\n",
    "from gcmc.model import RecommenderGAE, RecommenderSideInfoGAE\n",
    "from gcmc.utils import construct_feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:07.146736Z",
     "start_time": "2021-06-28T08:49:07.130741Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "# seed = 123 # use only for unit testing\n",
    "seed = int(time.time())\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:07.684026Z",
     "start_time": "2021-06-28T08:49:07.667072Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-sdir', '--summaries_dir'], dest='summaries_dir', nargs=None, const=None, default='logs/2021-06-28_16:49:07.673056', type=<class 'str'>, choices=None, help='Directory for saving tensorflow summaries.', metavar=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings\n",
    "ap = argparse.ArgumentParser(description=\"main\")\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, default=\"ml_100k\",\n",
    "                choices=['ml_100k', 'ml_1m', 'ml_10m', 'douban', 'yahoo_music', 'flixster'],\n",
    "                help=\"Dataset string.\")\n",
    "\n",
    "ap.add_argument(\"-lr\", \"--learning_rate\", type=float, default=0.01,\n",
    "                help=\"Learning rate\")\n",
    "\n",
    "ap.add_argument(\"-e\", \"--epochs\", type=int, default=2500,\n",
    "                help=\"Number training epochs\")\n",
    "\n",
    "ap.add_argument(\"-hi\", \"--hidden\", type=int, nargs=2, default=[500, 75],\n",
    "                help=\"Number hidden units in 1st and 2nd layer\")\n",
    "\n",
    "ap.add_argument(\"-fhi\", \"--feat_hidden\", type=int, default=64,\n",
    "                help=\"Number hidden units in the dense layer for features\")\n",
    "\n",
    "ap.add_argument(\"-ac\", \"--accumulation\", type=str, default=\"sum\", choices=['sum', 'stack'],\n",
    "                help=\"Accumulation function: sum or stack.\")\n",
    "\n",
    "ap.add_argument(\"-do\", \"--dropout\", type=float, default=0.7,\n",
    "                help=\"Dropout fraction\")\n",
    "\n",
    "ap.add_argument(\"-nb\", \"--num_basis_functions\", type=int, default=2,\n",
    "                help=\"Number of basis functions for Mixture Model GCN.\")\n",
    "\n",
    "ap.add_argument(\"-ds\", \"--data_seed\", type=int, default=1234,\n",
    "                help=\"\"\"Seed used to shuffle data in data_utils, taken from cf-nade (1234, 2341, 3412, 4123, 1324).\n",
    "                     Only used for ml_1m and ml_10m datasets. \"\"\")\n",
    "\n",
    "ap.add_argument(\"-sdir\", \"--summaries_dir\", type=str, default='logs/' + str(datetime.datetime.now()).replace(' ', '_'),\n",
    "                help=\"Directory for saving tensorflow summaries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:08.544046Z",
     "start_time": "2021-06-28T08:49:08.536054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "{'dataset': 'ml_100k', 'learning_rate': 0.01, 'epochs': 2500, 'hidden': [500, 75], 'feat_hidden': 64, 'accumulation': 'sum', 'dropout': 0.7, 'num_basis_functions': 2, 'data_seed': 1234, 'summaries_dir': 'logs/2021-06-28_16:49:07.673056', 'norm_symmetric': True, 'features': True, 'write_summary': False, 'testing': False} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Boolean flags\n",
    "fp = ap.add_mutually_exclusive_group(required=False)\n",
    "fp.add_argument('-nsym', '--norm_symmetric', dest='norm_symmetric',\n",
    "                help=\"Option to turn on symmetric global normalization\", action='store_true')\n",
    "fp.add_argument('-nleft', '--norm_left', dest='norm_symmetric',\n",
    "                help=\"Option to turn on left global normalization\", action='store_false')\n",
    "ap.set_defaults(norm_symmetric=True)\n",
    "\n",
    "fp = ap.add_mutually_exclusive_group(required=False)\n",
    "fp.add_argument('-f', '--features', dest='features',\n",
    "                help=\"Whether to use features (1) or not (0)\", action='store_true')\n",
    "fp.add_argument('-no_f', '--no_features', dest='features',\n",
    "                help=\"Whether to use features (1) or not (0)\", action='store_false')\n",
    "ap.set_defaults(features=False)\n",
    "\n",
    "fp = ap.add_mutually_exclusive_group(required=False)\n",
    "fp.add_argument('-ws', '--write_summary', dest='write_summary',\n",
    "                help=\"Option to turn on summary writing\", action='store_true')\n",
    "fp.add_argument('-no_ws', '--no_write_summary', dest='write_summary',\n",
    "                help=\"Option to turn off summary writing\", action='store_false')\n",
    "ap.set_defaults(write_summary=False)\n",
    "\n",
    "fp = ap.add_mutually_exclusive_group(required=False)\n",
    "fp.add_argument('-t', '--testing', dest='testing',\n",
    "                help=\"Option to turn on test set evaluation\", action='store_true')\n",
    "fp.add_argument('-v', '--validation', dest='testing',\n",
    "                help=\"Option to only use validation set evaluation\", action='store_false')\n",
    "ap.set_defaults(testing=False)\n",
    "\n",
    "args = vars(ap.parse_known_args()[0])\n",
    "\n",
    "print('Settings:')\n",
    "print(args, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:09.477009Z",
     "start_time": "2021-06-28T08:49:09.470027Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "DATASET = args['dataset']\n",
    "DATASEED = args['data_seed']\n",
    "NB_EPOCH = args['epochs']\n",
    "DO = args['dropout']\n",
    "HIDDEN = args['hidden']\n",
    "FEATHIDDEN = args['feat_hidden']\n",
    "BASES = args['num_basis_functions']\n",
    "LR = args['learning_rate']\n",
    "WRITESUMMARY = args['write_summary']\n",
    "SUMMARIESDIR = args['summaries_dir']\n",
    "FEATURES = args['features']\n",
    "SYM = args['norm_symmetric']\n",
    "TESTING = args['testing']\n",
    "ACCUM = args['accumulation']\n",
    "\n",
    "SELFCONNECTIONS = False\n",
    "SPLITFROMFILE = True\n",
    "VERBOSE = True\n",
    "\n",
    "if DATASET == 'ml_1m' or DATASET == 'ml_100k' or DATASET == 'douban':\n",
    "    NUMCLASSES = 5\n",
    "elif DATASET == 'ml_10m':\n",
    "    NUMCLASSES = 10\n",
    "    print('\\n WARNING: this might run out of RAM, consider using train_minibatch.py for dataset %s' % DATASET)\n",
    "    print('If you want to proceed with this option anyway, uncomment this.\\n')\n",
    "    sys.exit(1)\n",
    "elif DATASET == 'flixster':\n",
    "    NUMCLASSES = 10\n",
    "elif DATASET == 'yahoo_music':\n",
    "    NUMCLASSES = 71\n",
    "    if ACCUM == 'sum':\n",
    "        print('\\n WARNING: combining DATASET=%s with ACCUM=%s can cause memory issues due to large number of classes.')\n",
    "        print('Consider using \"--accum stack\" as an option for this dataset.')\n",
    "        print('If you want to proceed with this option anyway, uncomment this.\\n')\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:11.326155Z",
     "start_time": "2021-06-28T08:49:10.430304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using official MovieLens dataset split u1.base/u1.test with 20% validation set size...\n",
      "User features shape: (943, 23)\n",
      "Item features shape: (1682, 18)\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset in training, validation and test set\n",
    "\n",
    "if DATASET == 'ml_1m' or DATASET == 'ml_10m':\n",
    "    if FEATURES:\n",
    "        datasplit_path = 'data/' + DATASET + '/withfeatures_split_seed' + str(DATASEED) + '.pickle'\n",
    "    else:\n",
    "        datasplit_path = 'data/' + DATASET + '/split_seed' + str(DATASEED) + '.pickle'\n",
    "elif FEATURES:\n",
    "    datasplit_path = 'data/' + DATASET + '/withfeatures.pickle'\n",
    "else:\n",
    "    datasplit_path = 'data/' + DATASET + '/nofeatures.pickle'\n",
    "\n",
    "\n",
    "if DATASET == 'flixster' or DATASET == 'douban' or DATASET == 'yahoo_music':\n",
    "    u_features, v_features, adj_train, train_labels, train_u_indices, train_v_indices, \\\n",
    "        val_labels, val_u_indices, val_v_indices, test_labels, \\\n",
    "        test_u_indices, test_v_indices, class_values = load_data_monti(DATASET, TESTING)\n",
    "\n",
    "elif DATASET == 'ml_100k':\n",
    "    print(\"Using official MovieLens dataset split u1.base/u1.test with 20% validation set size...\")\n",
    "    u_features, v_features, adj_train, train_labels, train_u_indices, train_v_indices, \\\n",
    "        val_labels, val_u_indices, val_v_indices, test_labels, \\\n",
    "        test_u_indices, test_v_indices, class_values = load_official_trainvaltest_split(DATASET, TESTING)\n",
    "else:\n",
    "    print(\"Using random dataset split ...\")\n",
    "    u_features, v_features, adj_train, train_labels, train_u_indices, train_v_indices, \\\n",
    "        val_labels, val_u_indices, val_v_indices, test_labels, \\\n",
    "        test_u_indices, test_v_indices, class_values = create_trainvaltest_split(DATASET, DATASEED, TESTING,\n",
    "                                                                                 datasplit_path, SPLITFROMFILE,\n",
    "                                                                                 VERBOSE)\n",
    "\n",
    "num_users, num_items = adj_train.shape\n",
    "\n",
    "num_side_features = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:50:04.234027Z",
     "start_time": "2021-06-28T08:50:04.193101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing feature vectors...\n"
     ]
    }
   ],
   "source": [
    "# feature loading\n",
    "if not FEATURES:\n",
    "    u_features = sp.identity(num_users, format='csr')\n",
    "    v_features = sp.identity(num_items, format='csr')\n",
    "\n",
    "    u_features, v_features = preprocess_user_item_features(u_features, v_features)\n",
    "\n",
    "elif FEATURES and u_features is not None and v_features is not None:\n",
    "    # use features as side information and node_id's as node input features\n",
    "\n",
    "    print(\"Normalizing feature vectors...\")\n",
    "    u_features_side = normalize_features(u_features)\n",
    "    v_features_side = normalize_features(v_features)\n",
    "\n",
    "    u_features_side, v_features_side = preprocess_user_item_features(u_features_side, v_features_side)\n",
    "\n",
    "    u_features_side = np.array(u_features_side.todense(), dtype=np.float32)\n",
    "    v_features_side = np.array(v_features_side.todense(), dtype=np.float32)\n",
    "\n",
    "    num_side_features = u_features_side.shape[1]\n",
    "\n",
    "    # node id's for node input features\n",
    "    id_csr_v = sp.identity(num_items, format='csr')\n",
    "    id_csr_u = sp.identity(num_users, format='csr')\n",
    "\n",
    "    u_features, v_features = preprocess_user_item_features(id_csr_u, id_csr_v)\n",
    "\n",
    "else:\n",
    "    raise ValueError('Features flag is set to true but no features are loaded from dataset ' + DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:52:43.363388Z",
     "start_time": "2021-06-28T08:52:43.305550Z"
    }
   },
   "outputs": [],
   "source": [
    "# global normalization\n",
    "support = []\n",
    "support_t = []\n",
    "adj_train_int = sp.csr_matrix(adj_train, dtype=np.int32)\n",
    "\n",
    "for i in range(NUMCLASSES):\n",
    "    # build individual binary rating matrices (supports) for each rating\n",
    "    support_unnormalized = sp.csr_matrix(adj_train_int == i + 1, dtype=np.float32)\n",
    "\n",
    "    if support_unnormalized.nnz == 0 and DATASET != 'yahoo_music':\n",
    "        # yahoo music has dataset split with not all ratings types present in training set.\n",
    "        # this produces empty adjacency matrices for these ratings.\n",
    "        sys.exit('ERROR: normalized bipartite adjacency matrix has only zero entries!!!!!')\n",
    "\n",
    "    support_unnormalized_transpose = support_unnormalized.T\n",
    "    support.append(support_unnormalized)\n",
    "    support_t.append(support_unnormalized_transpose)\n",
    "\n",
    "\n",
    "support = globally_normalize_bipartite_adjacency(support, symmetric=SYM)\n",
    "support_t = globally_normalize_bipartite_adjacency(support_t, symmetric=SYM)\n",
    "\n",
    "if SELFCONNECTIONS:\n",
    "    support.append(sp.identity(u_features.shape[0], format='csr'))\n",
    "    support_t.append(sp.identity(v_features.shape[0], format='csr'))\n",
    "\n",
    "num_support = len(support)\n",
    "support = sp.hstack(support, format='csr')\n",
    "support_t = sp.hstack(support_t, format='csr')\n",
    "\n",
    "if ACCUM == 'stack':\n",
    "    div = HIDDEN[0] // num_support\n",
    "    if HIDDEN[0] % num_support != 0:\n",
    "        print(\"\"\"\\nWARNING: HIDDEN[0] (=%d) of stack layer is adjusted to %d such that\n",
    "                  it can be evenly split in %d splits.\\n\"\"\" % (HIDDEN[0], num_support * div, num_support))\n",
    "    HIDDEN[0] = num_support * div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:53:02.391584Z",
     "start_time": "2021-06-28T08:53:02.327758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect all user and item nodes for test set\n",
    "test_u = list(set(test_u_indices))\n",
    "test_v = list(set(test_v_indices))\n",
    "test_u_dict = {n: i for i, n in enumerate(test_u)}\n",
    "test_v_dict = {n: i for i, n in enumerate(test_v)}\n",
    "\n",
    "test_u_indices = np.array([test_u_dict[o] for o in test_u_indices])\n",
    "test_v_indices = np.array([test_v_dict[o] for o in test_v_indices])\n",
    "\n",
    "test_support = support[np.array(test_u)]\n",
    "test_support_t = support_t[np.array(test_v)]\n",
    "\n",
    "# Collect all user and item nodes for validation set\n",
    "val_u = list(set(val_u_indices))\n",
    "val_v = list(set(val_v_indices))\n",
    "val_u_dict = {n: i for i, n in enumerate(val_u)}\n",
    "val_v_dict = {n: i for i, n in enumerate(val_v)}\n",
    "\n",
    "val_u_indices = np.array([val_u_dict[o] for o in val_u_indices])\n",
    "val_v_indices = np.array([val_v_dict[o] for o in val_v_indices])\n",
    "\n",
    "val_support = support[np.array(val_u)]\n",
    "val_support_t = support_t[np.array(val_v)]\n",
    "\n",
    "# Collect all user and item nodes for train set\n",
    "train_u = list(set(train_u_indices))\n",
    "train_v = list(set(train_v_indices))\n",
    "train_u_dict = {n: i for i, n in enumerate(train_u)}\n",
    "train_v_dict = {n: i for i, n in enumerate(train_v)}\n",
    "\n",
    "train_u_indices = np.array([train_u_dict[o] for o in train_u_indices])\n",
    "train_v_indices = np.array([train_v_dict[o] for o in train_v_indices])\n",
    "\n",
    "train_support = support[np.array(train_u)]\n",
    "train_support_t = support_t[np.array(train_v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:53:19.534991Z",
     "start_time": "2021-06-28T08:53:19.458182Z"
    }
   },
   "outputs": [],
   "source": [
    "# features as side info\n",
    "if FEATURES:\n",
    "    test_u_features_side = u_features_side[np.array(test_u)]\n",
    "    test_v_features_side = v_features_side[np.array(test_v)]\n",
    "\n",
    "    val_u_features_side = u_features_side[np.array(val_u)]\n",
    "    val_v_features_side = v_features_side[np.array(val_v)]\n",
    "\n",
    "    train_u_features_side = u_features_side[np.array(train_u)]\n",
    "    train_v_features_side = v_features_side[np.array(train_v)]\n",
    "\n",
    "else:\n",
    "    test_u_features_side = None\n",
    "    test_v_features_side = None\n",
    "\n",
    "    val_u_features_side = None\n",
    "    val_v_features_side = None\n",
    "\n",
    "    train_u_features_side = None\n",
    "    train_v_features_side = None\n",
    "\n",
    "placeholders = {\n",
    "    'u_features': tf.sparse_placeholder(tf.float32, shape=np.array(u_features.shape, dtype=np.int64)),\n",
    "    'v_features': tf.sparse_placeholder(tf.float32, shape=np.array(v_features.shape, dtype=np.int64)),\n",
    "    'u_features_nonzero': tf.placeholder(tf.int32, shape=()),\n",
    "    'v_features_nonzero': tf.placeholder(tf.int32, shape=()),\n",
    "    'labels': tf.placeholder(tf.int32, shape=(None,)),\n",
    "\n",
    "    'u_features_side': tf.placeholder(tf.float32, shape=(None, num_side_features)),\n",
    "    'v_features_side': tf.placeholder(tf.float32, shape=(None, num_side_features)),\n",
    "\n",
    "    'user_indices': tf.placeholder(tf.int32, shape=(None,)),\n",
    "    'item_indices': tf.placeholder(tf.int32, shape=(None,)),\n",
    "\n",
    "    'class_values': tf.placeholder(tf.float32, shape=class_values.shape),\n",
    "\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'weight_decay': tf.placeholder_with_default(0., shape=()),\n",
    "\n",
    "    'support': tf.sparse_placeholder(tf.float32, shape=(None, None)),\n",
    "    'support_t': tf.sparse_placeholder(tf.float32, shape=(None, None)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:53:36.894106Z",
     "start_time": "2021-06-28T08:53:34.819663Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\tensor\\gcmc\\model.py:255: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\tensor\\gcmc\\model.py:347: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\tensor\\gcmc\\initializations.py:20: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\tensor\\gcmc\\layers.py:84: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\tensor\\gcmc\\initializations.py:41: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\tensor\\gcmc\\layers.py:38: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1719: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\tensor\\gcmc\\layers.py:40: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\tensor\\gcmc\\layers.py:14: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\tensor\\gcmc\\layers.py:130: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\tensor\\gcmc\\model.py:273: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\tensor\\gcmc\\metrics.py:11: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\training\\moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "if FEATURES:\n",
    "    model = RecommenderSideInfoGAE(placeholders,\n",
    "                                   input_dim=u_features.shape[1],\n",
    "                                   feat_hidden_dim=FEATHIDDEN,\n",
    "                                   num_classes=NUMCLASSES,\n",
    "                                   num_support=num_support,\n",
    "                                   self_connections=SELFCONNECTIONS,\n",
    "                                   num_basis_functions=BASES,\n",
    "                                   hidden=HIDDEN,\n",
    "                                   num_users=num_users,\n",
    "                                   num_items=num_items,\n",
    "                                   accum=ACCUM,\n",
    "                                   learning_rate=LR,\n",
    "                                   num_side_features=num_side_features,\n",
    "                                   logging=True)\n",
    "else:\n",
    "    model = RecommenderGAE(placeholders,\n",
    "                           input_dim=u_features.shape[1],\n",
    "                           num_classes=NUMCLASSES,\n",
    "                           num_support=num_support,\n",
    "                           self_connections=SELFCONNECTIONS,\n",
    "                           num_basis_functions=BASES,\n",
    "                           hidden=HIDDEN,\n",
    "                           num_users=num_users,\n",
    "                           num_items=num_items,\n",
    "                           accum=ACCUM,\n",
    "                           learning_rate=LR,\n",
    "                           logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:53:51.910475Z",
     "start_time": "2021-06-28T08:53:51.901467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert sparse placeholders to tuples to construct feed_dict\n",
    "test_support = sparse_to_tuple(test_support)\n",
    "test_support_t = sparse_to_tuple(test_support_t)\n",
    "\n",
    "val_support = sparse_to_tuple(val_support)\n",
    "val_support_t = sparse_to_tuple(val_support_t)\n",
    "\n",
    "train_support = sparse_to_tuple(train_support)\n",
    "train_support_t = sparse_to_tuple(train_support_t)\n",
    "\n",
    "u_features = sparse_to_tuple(u_features)\n",
    "v_features = sparse_to_tuple(v_features)\n",
    "assert u_features[2][1] == v_features[2][1], 'Number of features of users and items must be the same!'\n",
    "\n",
    "num_features = u_features[2][1]\n",
    "u_features_nonzero = u_features[1].shape[0]\n",
    "v_features_nonzero = v_features[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:54:04.506026Z",
     "start_time": "2021-06-28T08:54:04.501060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feed_dicts for validation and test set stay constant over different update steps\n",
    "train_feed_dict = construct_feed_dict(placeholders, u_features, v_features, u_features_nonzero,\n",
    "                                      v_features_nonzero, train_support, train_support_t,\n",
    "                                      train_labels, train_u_indices, train_v_indices, class_values, DO,\n",
    "                                      train_u_features_side, train_v_features_side)\n",
    "# No dropout for validation and test runs\n",
    "val_feed_dict = construct_feed_dict(placeholders, u_features, v_features, u_features_nonzero,\n",
    "                                    v_features_nonzero, val_support, val_support_t,\n",
    "                                    val_labels, val_u_indices, val_v_indices, class_values, 0.,\n",
    "                                    val_u_features_side, val_v_features_side)\n",
    "\n",
    "test_feed_dict = construct_feed_dict(placeholders, u_features, v_features, u_features_nonzero,\n",
    "                                     v_features_nonzero, test_support, test_support_t,\n",
    "                                     test_labels, test_u_indices, test_v_indices, class_values, 0.,\n",
    "                                     test_u_features_side, test_v_features_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:55:00.551735Z",
     "start_time": "2021-06-28T08:54:59.947384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect all variables to be logged into summary\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if WRITESUMMARY:\n",
    "    train_summary_writer = tf.summary.FileWriter(SUMMARIESDIR + '/train', sess.graph)\n",
    "    val_summary_writer = tf.summary.FileWriter(SUMMARIESDIR + '/val')\n",
    "else:\n",
    "    train_summary_writer = None\n",
    "    val_summary_writer = None\n",
    "\n",
    "best_val_score = np.inf\n",
    "best_val_loss = np.inf\n",
    "best_epoch = 0\n",
    "wait = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T09:21:08.649753Z",
     "start_time": "2021-06-28T08:55:15.243802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "[*] Epoch: 0001 train_loss= 2.13964 train_rmse= 1.20293 val_loss= 2.58343 val_rmse= 1.25460 \t\ttime= 1.25951\n",
      "[*] Epoch: 0002 train_loss= 2.80828 train_rmse= 1.23295 val_loss= 8.13137 val_rmse= 1.19438 \t\ttime= 0.69214\n",
      "[*] Epoch: 0003 train_loss= 10.50304 train_rmse= 1.16949 val_loss= 1.66683 val_rmse= 1.30030 \t\ttime= 0.63130\n",
      "[*] Epoch: 0004 train_loss= 2.09541 train_rmse= 1.20628 val_loss= 3.12389 val_rmse= 1.26504 \t\ttime= 0.62932\n",
      "[*] Epoch: 0005 train_loss= 3.99205 train_rmse= 1.24871 val_loss= 1.65855 val_rmse= 1.26392 \t\ttime= 0.63131\n",
      "[*] Epoch: 0006 train_loss= 2.03082 train_rmse= 1.20085 val_loss= 2.86684 val_rmse= 1.21077 \t\ttime= 0.59640\n",
      "[*] Epoch: 0007 train_loss= 3.04376 train_rmse= 1.17191 val_loss= 1.95373 val_rmse= 1.29805 \t\ttime= 0.63729\n",
      "[*] Epoch: 0008 train_loss= 2.26731 train_rmse= 1.21053 val_loss= 2.29387 val_rmse= 1.24576 \t\ttime= 0.67220\n",
      "[*] Epoch: 0009 train_loss= 2.49592 train_rmse= 1.20982 val_loss= 2.88226 val_rmse= 1.25773 \t\ttime= 0.60039\n",
      "[*] Epoch: 0010 train_loss= 2.98587 train_rmse= 1.21376 val_loss= 2.31705 val_rmse= 1.24197 \t\ttime= 0.67819\n",
      "[*] Epoch: 0011 train_loss= 2.44782 train_rmse= 1.19700 val_loss= 1.65064 val_rmse= 1.27170 \t\ttime= 0.61535\n",
      "[*] Epoch: 0012 train_loss= 1.76383 train_rmse= 1.21710 val_loss= 1.71536 val_rmse= 1.30314 \t\ttime= 0.61536\n",
      "[*] Epoch: 0013 train_loss= 1.81435 train_rmse= 1.23304 val_loss= 1.84690 val_rmse= 1.29189 \t\ttime= 0.63729\n",
      "[*] Epoch: 0014 train_loss= 1.94682 train_rmse= 1.22216 val_loss= 1.73542 val_rmse= 1.29492 \t\ttime= 0.64527\n",
      "[*] Epoch: 0015 train_loss= 1.82067 train_rmse= 1.22619 val_loss= 1.61066 val_rmse= 1.28565 \t\ttime= 0.66521\n",
      "[*] Epoch: 0016 train_loss= 1.68167 train_rmse= 1.22456 val_loss= 1.65722 val_rmse= 1.25883 \t\ttime= 0.66522\n",
      "[*] Epoch: 0017 train_loss= 1.72270 train_rmse= 1.20370 val_loss= 1.77712 val_rmse= 1.24307 \t\ttime= 0.72008\n",
      "[*] Epoch: 0018 train_loss= 1.83821 train_rmse= 1.19284 val_loss= 1.81428 val_rmse= 1.24119 \t\ttime= 0.68617\n",
      "[*] Epoch: 0019 train_loss= 1.85425 train_rmse= 1.19200 val_loss= 1.75126 val_rmse= 1.24853 \t\ttime= 0.71011\n",
      "[*] Epoch: 0020 train_loss= 1.79042 train_rmse= 1.19749 val_loss= 1.65841 val_rmse= 1.26355 \t\ttime= 0.74602\n",
      "[*] Epoch: 0021 train_loss= 1.68575 train_rmse= 1.20884 val_loss= 1.60779 val_rmse= 1.28063 \t\ttime= 0.61635\n",
      "[*] Epoch: 0022 train_loss= 1.64174 train_rmse= 1.22428 val_loss= 1.61504 val_rmse= 1.29139 \t\ttime= 0.62832\n",
      "[*] Epoch: 0023 train_loss= 1.64915 train_rmse= 1.22954 val_loss= 1.64653 val_rmse= 1.29441 \t\ttime= 0.68517\n",
      "[*] Epoch: 0024 train_loss= 1.68530 train_rmse= 1.23167 val_loss= 1.66189 val_rmse= 1.29485 \t\ttime= 0.58044\n",
      "[*] Epoch: 0025 train_loss= 1.70239 train_rmse= 1.23128 val_loss= 1.65056 val_rmse= 1.29598 \t\ttime= 0.64628\n",
      "[*] Epoch: 0026 train_loss= 1.68942 train_rmse= 1.23289 val_loss= 1.62659 val_rmse= 1.29715 \t\ttime= 0.68119\n",
      "[*] Epoch: 0027 train_loss= 1.65601 train_rmse= 1.23703 val_loss= 1.60861 val_rmse= 1.29693 \t\ttime= 0.63231\n",
      "[*] Epoch: 0028 train_loss= 1.62799 train_rmse= 1.23737 val_loss= 1.60332 val_rmse= 1.29541 \t\ttime= 0.61933\n",
      "[*] Epoch: 0029 train_loss= 1.61984 train_rmse= 1.23716 val_loss= 1.60705 val_rmse= 1.29357 \t\ttime= 0.62433\n",
      "[*] Epoch: 0030 train_loss= 1.62265 train_rmse= 1.23675 val_loss= 1.61427 val_rmse= 1.29201 \t\ttime= 0.58942\n",
      "[*] Epoch: 0031 train_loss= 1.62667 train_rmse= 1.23575 val_loss= 1.62073 val_rmse= 1.29107 \t\ttime= 0.56051\n",
      "[*] Epoch: 0032 train_loss= 1.63408 train_rmse= 1.23388 val_loss= 1.62383 val_rmse= 1.29089 \t\ttime= 0.58843\n",
      "[*] Epoch: 0033 train_loss= 1.63864 train_rmse= 1.23347 val_loss= 1.62307 val_rmse= 1.29134 \t\ttime= 0.66821\n",
      "[*] Epoch: 0034 train_loss= 1.63496 train_rmse= 1.23405 val_loss= 1.61963 val_rmse= 1.29217 \t\ttime= 0.57746\n",
      "[*] Epoch: 0035 train_loss= 1.62831 train_rmse= 1.23470 val_loss= 1.61492 val_rmse= 1.29315 \t\ttime= 0.61635\n",
      "[*] Epoch: 0036 train_loss= 1.62256 train_rmse= 1.23513 val_loss= 1.61013 val_rmse= 1.29410 \t\ttime= 0.63730\n",
      "[*] Epoch: 0037 train_loss= 1.62134 train_rmse= 1.23703 val_loss= 1.60605 val_rmse= 1.29486 \t\ttime= 0.51661\n",
      "[*] Epoch: 0038 train_loss= 1.61438 train_rmse= 1.23772 val_loss= 1.60310 val_rmse= 1.29537 \t\ttime= 0.62234\n",
      "[*] Epoch: 0039 train_loss= 1.61054 train_rmse= 1.23799 val_loss= 1.60131 val_rmse= 1.29560 \t\ttime= 0.62433\n",
      "[*] Epoch: 0040 train_loss= 1.60796 train_rmse= 1.23794 val_loss= 1.60051 val_rmse= 1.29556 \t\ttime= 0.63231\n",
      "[*] Epoch: 0041 train_loss= 1.60467 train_rmse= 1.23752 val_loss= 1.60035 val_rmse= 1.29531 \t\ttime= 0.61635\n",
      "[*] Epoch: 0042 train_loss= 1.60788 train_rmse= 1.23685 val_loss= 1.60045 val_rmse= 1.29490 \t\ttime= 0.62631\n",
      "[*] Epoch: 0043 train_loss= 1.60732 train_rmse= 1.23661 val_loss= 1.60050 val_rmse= 1.29438 \t\ttime= 0.59441\n",
      "[*] Epoch: 0044 train_loss= 1.60553 train_rmse= 1.23621 val_loss= 1.60025 val_rmse= 1.29382 \t\ttime= 0.60040\n",
      "[*] Epoch: 0045 train_loss= 1.60607 train_rmse= 1.23533 val_loss= 1.59961 val_rmse= 1.29325 \t\ttime= 0.62034\n",
      "[*] Epoch: 0046 train_loss= 1.60594 train_rmse= 1.23513 val_loss= 1.59855 val_rmse= 1.29270 \t\ttime= 0.70412\n",
      "[*] Epoch: 0047 train_loss= 1.60409 train_rmse= 1.23454 val_loss= 1.59724 val_rmse= 1.29217 \t\ttime= 0.57047\n",
      "[*] Epoch: 0048 train_loss= 1.60137 train_rmse= 1.23389 val_loss= 1.59583 val_rmse= 1.29165 \t\ttime= 0.62732\n",
      "[*] Epoch: 0049 train_loss= 1.59909 train_rmse= 1.23339 val_loss= 1.59443 val_rmse= 1.29113 \t\ttime= 0.62234\n",
      "[*] Epoch: 0050 train_loss= 1.59642 train_rmse= 1.23299 val_loss= 1.59313 val_rmse= 1.29058 \t\ttime= 0.56948\n",
      "[*] Epoch: 0051 train_loss= 1.59460 train_rmse= 1.23288 val_loss= 1.59193 val_rmse= 1.28999 \t\ttime= 0.63929\n",
      "[*] Epoch: 0052 train_loss= 1.59314 train_rmse= 1.23208 val_loss= 1.59078 val_rmse= 1.28933 \t\ttime= 0.67820\n",
      "[*] Epoch: 0053 train_loss= 1.59230 train_rmse= 1.23134 val_loss= 1.58961 val_rmse= 1.28859 \t\ttime= 0.58743\n",
      "[*] Epoch: 0054 train_loss= 1.59131 train_rmse= 1.23099 val_loss= 1.58834 val_rmse= 1.28773 \t\ttime= 0.63631\n",
      "[*] Epoch: 0055 train_loss= 1.58820 train_rmse= 1.23018 val_loss= 1.58687 val_rmse= 1.28671 \t\ttime= 0.57147\n",
      "[*] Epoch: 0056 train_loss= 1.58826 train_rmse= 1.22920 val_loss= 1.58514 val_rmse= 1.28550 \t\ttime= 0.59341\n",
      "[*] Epoch: 0057 train_loss= 1.58325 train_rmse= 1.22777 val_loss= 1.58309 val_rmse= 1.28404 \t\ttime= 0.68118\n",
      "[*] Epoch: 0058 train_loss= 1.58238 train_rmse= 1.22654 val_loss= 1.58072 val_rmse= 1.28230 \t\ttime= 0.76298\n",
      "[*] Epoch: 0059 train_loss= 1.57880 train_rmse= 1.22476 val_loss= 1.57798 val_rmse= 1.28017 \t\ttime= 0.79886\n",
      "[*] Epoch: 0060 train_loss= 1.57457 train_rmse= 1.22267 val_loss= 1.57487 val_rmse= 1.27758 \t\ttime= 0.67719\n",
      "[*] Epoch: 0061 train_loss= 1.56878 train_rmse= 1.22005 val_loss= 1.57144 val_rmse= 1.27444 \t\ttime= 0.65924\n",
      "[*] Epoch: 0062 train_loss= 1.56817 train_rmse= 1.21668 val_loss= 1.56776 val_rmse= 1.27070 \t\ttime= 0.63829\n",
      "[*] Epoch: 0063 train_loss= 1.56133 train_rmse= 1.21270 val_loss= 1.56385 val_rmse= 1.26634 \t\ttime= 0.67619\n",
      "[*] Epoch: 0064 train_loss= 1.55746 train_rmse= 1.20745 val_loss= 1.55976 val_rmse= 1.26139 \t\ttime= 0.72806\n",
      "[*] Epoch: 0065 train_loss= 1.54978 train_rmse= 1.20259 val_loss= 1.55538 val_rmse= 1.25592 \t\ttime= 0.67220\n",
      "[*] Epoch: 0066 train_loss= 1.54702 train_rmse= 1.19820 val_loss= 1.55087 val_rmse= 1.24993 \t\ttime= 0.70112\n",
      "[*] Epoch: 0067 train_loss= 1.54499 train_rmse= 1.19228 val_loss= 1.54629 val_rmse= 1.24359 \t\ttime= 0.67719\n",
      "[*] Epoch: 0068 train_loss= 1.53867 train_rmse= 1.18516 val_loss= 1.54184 val_rmse= 1.23713 \t\ttime= 0.62633\n",
      "[*] Epoch: 0069 train_loss= 1.53869 train_rmse= 1.18029 val_loss= 1.53779 val_rmse= 1.23086 \t\ttime= 0.63630\n",
      "[*] Epoch: 0070 train_loss= 1.52665 train_rmse= 1.17437 val_loss= 1.53428 val_rmse= 1.22469 \t\ttime= 0.71509\n",
      "[*] Epoch: 0071 train_loss= 1.53129 train_rmse= 1.16784 val_loss= 1.53119 val_rmse= 1.21894 \t\ttime= 0.58245\n",
      "[*] Epoch: 0072 train_loss= 1.52173 train_rmse= 1.16412 val_loss= 1.52843 val_rmse= 1.21366 \t\ttime= 0.64926\n",
      "[*] Epoch: 0073 train_loss= 1.52046 train_rmse= 1.16092 val_loss= 1.52582 val_rmse= 1.20903 \t\ttime= 0.64527\n",
      "[*] Epoch: 0074 train_loss= 1.51424 train_rmse= 1.15435 val_loss= 1.52327 val_rmse= 1.20485 \t\ttime= 0.65126\n",
      "[*] Epoch: 0075 train_loss= 1.51126 train_rmse= 1.14897 val_loss= 1.52069 val_rmse= 1.20111 \t\ttime= 0.65923\n",
      "[*] Epoch: 0076 train_loss= 1.50451 train_rmse= 1.15012 val_loss= 1.51810 val_rmse= 1.19774 \t\ttime= 0.65126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0077 train_loss= 1.50574 train_rmse= 1.14724 val_loss= 1.51548 val_rmse= 1.19475 \t\ttime= 0.62933\n",
      "[*] Epoch: 0078 train_loss= 1.49902 train_rmse= 1.14347 val_loss= 1.51286 val_rmse= 1.19218 \t\ttime= 0.74800\n",
      "[*] Epoch: 0079 train_loss= 1.50097 train_rmse= 1.14519 val_loss= 1.51030 val_rmse= 1.18986 \t\ttime= 0.69913\n",
      "[*] Epoch: 0080 train_loss= 1.49195 train_rmse= 1.13464 val_loss= 1.50779 val_rmse= 1.18754 \t\ttime= 0.57047\n",
      "[*] Epoch: 0081 train_loss= 1.49493 train_rmse= 1.13809 val_loss= 1.50551 val_rmse= 1.18548 \t\ttime= 0.59541\n",
      "[*] Epoch: 0082 train_loss= 1.48325 train_rmse= 1.13245 val_loss= 1.50318 val_rmse= 1.18314 \t\ttime= 0.65924\n",
      "[*] Epoch: 0083 train_loss= 1.48758 train_rmse= 1.13134 val_loss= 1.50077 val_rmse= 1.18058 \t\ttime= 0.61835\n",
      "[*] Epoch: 0084 train_loss= 1.47885 train_rmse= 1.12616 val_loss= 1.49817 val_rmse= 1.17761 \t\ttime= 0.59840\n",
      "[*] Epoch: 0085 train_loss= 1.47663 train_rmse= 1.12258 val_loss= 1.49561 val_rmse= 1.17453 \t\ttime= 0.60140\n",
      "[*] Epoch: 0086 train_loss= 1.47687 train_rmse= 1.12624 val_loss= 1.49312 val_rmse= 1.17126 \t\ttime= 0.59940\n",
      "[*] Epoch: 0087 train_loss= 1.47114 train_rmse= 1.11367 val_loss= 1.49090 val_rmse= 1.16839 \t\ttime= 0.55352\n",
      "[*] Epoch: 0088 train_loss= 1.47183 train_rmse= 1.11158 val_loss= 1.48953 val_rmse= 1.16657 \t\ttime= 0.58842\n",
      "[*] Epoch: 0089 train_loss= 1.45950 train_rmse= 1.10281 val_loss= 1.48794 val_rmse= 1.16444 \t\ttime= 0.70213\n",
      "[*] Epoch: 0090 train_loss= 1.46772 train_rmse= 1.11017 val_loss= 1.48622 val_rmse= 1.16232 \t\ttime= 0.66522\n",
      "[*] Epoch: 0091 train_loss= 1.45806 train_rmse= 1.10368 val_loss= 1.48307 val_rmse= 1.15895 \t\ttime= 0.68018\n",
      "[*] Epoch: 0092 train_loss= 1.46525 train_rmse= 1.10992 val_loss= 1.47973 val_rmse= 1.15531 \t\ttime= 0.70112\n",
      "[*] Epoch: 0093 train_loss= 1.46549 train_rmse= 1.10203 val_loss= 1.47792 val_rmse= 1.15357 \t\ttime= 0.58543\n",
      "[*] Epoch: 0094 train_loss= 1.46393 train_rmse= 1.10420 val_loss= 1.47700 val_rmse= 1.15299 \t\ttime= 0.64129\n",
      "[*] Epoch: 0095 train_loss= 1.45004 train_rmse= 1.09507 val_loss= 1.47587 val_rmse= 1.15194 \t\ttime= 0.72306\n",
      "[*] Epoch: 0096 train_loss= 1.44644 train_rmse= 1.09504 val_loss= 1.47400 val_rmse= 1.15003 \t\ttime= 0.54554\n",
      "[*] Epoch: 0097 train_loss= 1.45571 train_rmse= 1.09921 val_loss= 1.47205 val_rmse= 1.14777 \t\ttime= 0.64926\n",
      "[*] Epoch: 0098 train_loss= 1.45559 train_rmse= 1.09962 val_loss= 1.47087 val_rmse= 1.14614 \t\ttime= 0.64727\n",
      "[*] Epoch: 0099 train_loss= 1.44806 train_rmse= 1.09070 val_loss= 1.47020 val_rmse= 1.14475 \t\ttime= 0.53657\n",
      "[*] Epoch: 0100 train_loss= 1.45018 train_rmse= 1.09396 val_loss= 1.46894 val_rmse= 1.14286 \t\ttime= 0.61037\n",
      "[*] Epoch: 0101 train_loss= 1.44906 train_rmse= 1.08217 val_loss= 1.46817 val_rmse= 1.14150 \t\ttime= 0.62533\n",
      "[*] Epoch: 0102 train_loss= 1.44250 train_rmse= 1.09097 val_loss= 1.46824 val_rmse= 1.14064 \t\ttime= 0.60538\n",
      "[*] Epoch: 0103 train_loss= 1.43739 train_rmse= 1.08324 val_loss= 1.46841 val_rmse= 1.13953 \t\ttime= 0.61137\n",
      "[*] Epoch: 0104 train_loss= 1.44542 train_rmse= 1.08341 val_loss= 1.46484 val_rmse= 1.13604 \t\ttime= 0.65724\n",
      "[*] Epoch: 0105 train_loss= 1.44146 train_rmse= 1.08095 val_loss= 1.46219 val_rmse= 1.13286 \t\ttime= 0.61735\n",
      "[*] Epoch: 0106 train_loss= 1.43431 train_rmse= 1.08333 val_loss= 1.46200 val_rmse= 1.13131 \t\ttime= 0.60438\n",
      "[*] Epoch: 0107 train_loss= 1.43442 train_rmse= 1.07433 val_loss= 1.46210 val_rmse= 1.12937 \t\ttime= 0.69714\n",
      "[*] Epoch: 0108 train_loss= 1.43790 train_rmse= 1.07265 val_loss= 1.45945 val_rmse= 1.12594 \t\ttime= 0.66821\n",
      "[*] Epoch: 0109 train_loss= 1.43703 train_rmse= 1.07629 val_loss= 1.45483 val_rmse= 1.12131 \t\ttime= 0.62832\n",
      "[*] Epoch: 0110 train_loss= 1.43128 train_rmse= 1.07195 val_loss= 1.45223 val_rmse= 1.11768 \t\ttime= 0.62333\n",
      "[*] Epoch: 0111 train_loss= 1.43061 train_rmse= 1.06911 val_loss= 1.45077 val_rmse= 1.11469 \t\ttime= 0.62234\n",
      "[*] Epoch: 0112 train_loss= 1.43029 train_rmse= 1.07028 val_loss= 1.44975 val_rmse= 1.11156 \t\ttime= 0.57147\n",
      "[*] Epoch: 0113 train_loss= 1.43329 train_rmse= 1.06372 val_loss= 1.45269 val_rmse= 1.11085 \t\ttime= 0.59740\n",
      "[*] Epoch: 0114 train_loss= 1.42753 train_rmse= 1.06352 val_loss= 1.45172 val_rmse= 1.10881 \t\ttime= 0.70612\n",
      "[*] Epoch: 0115 train_loss= 1.42690 train_rmse= 1.05682 val_loss= 1.44738 val_rmse= 1.10491 \t\ttime= 0.58244\n",
      "[*] Epoch: 0116 train_loss= 1.42315 train_rmse= 1.05362 val_loss= 1.44735 val_rmse= 1.10437 \t\ttime= 0.63430\n",
      "[*] Epoch: 0117 train_loss= 1.43066 train_rmse= 1.05380 val_loss= 1.45175 val_rmse= 1.10740 \t\ttime= 0.63430\n",
      "[*] Epoch: 0118 train_loss= 1.42101 train_rmse= 1.04226 val_loss= 1.45517 val_rmse= 1.10967 \t\ttime= 0.54754\n",
      "[*] Epoch: 0119 train_loss= 1.42411 train_rmse= 1.05037 val_loss= 1.45149 val_rmse= 1.10819 \t\ttime= 0.62932\n",
      "[*] Epoch: 0120 train_loss= 1.42118 train_rmse= 1.05536 val_loss= 1.44591 val_rmse= 1.10464 \t\ttime= 0.67919\n",
      "[*] Epoch: 0121 train_loss= 1.41885 train_rmse= 1.04580 val_loss= 1.44441 val_rmse= 1.10332 \t\ttime= 0.62732\n",
      "[*] Epoch: 0122 train_loss= 1.42116 train_rmse= 1.04840 val_loss= 1.44365 val_rmse= 1.10292 \t\ttime= 0.75399\n",
      "[*] Epoch: 0123 train_loss= 1.42446 train_rmse= 1.04590 val_loss= 1.44820 val_rmse= 1.10511 \t\ttime= 0.75998\n",
      "[*] Epoch: 0124 train_loss= 1.41588 train_rmse= 1.05276 val_loss= 1.45023 val_rmse= 1.10591 \t\ttime= 0.62233\n",
      "[*] Epoch: 0125 train_loss= 1.41719 train_rmse= 1.05156 val_loss= 1.44799 val_rmse= 1.10489 \t\ttime= 0.63629\n",
      "[*] Epoch: 0126 train_loss= 1.41346 train_rmse= 1.05064 val_loss= 1.44997 val_rmse= 1.10577 \t\ttime= 0.73106\n",
      "[*] Epoch: 0127 train_loss= 1.41164 train_rmse= 1.04590 val_loss= 1.45404 val_rmse= 1.10812 \t\ttime= 0.59940\n",
      "[*] Epoch: 0128 train_loss= 1.40780 train_rmse= 1.04070 val_loss= 1.45720 val_rmse= 1.11033 \t\ttime= 0.65126\n",
      "[*] Epoch: 0129 train_loss= 1.41116 train_rmse= 1.04746 val_loss= 1.45806 val_rmse= 1.11085 \t\ttime= 0.66323\n",
      "[*] Epoch: 0130 train_loss= 1.40659 train_rmse= 1.03833 val_loss= 1.45738 val_rmse= 1.10997 \t\ttime= 0.59142\n",
      "[*] Epoch: 0131 train_loss= 1.41163 train_rmse= 1.03947 val_loss= 1.45466 val_rmse= 1.10686 \t\ttime= 0.63729\n",
      "[*] Epoch: 0132 train_loss= 1.40632 train_rmse= 1.03245 val_loss= 1.45069 val_rmse= 1.10267 \t\ttime= 0.64926\n",
      "[*] Epoch: 0133 train_loss= 1.40663 train_rmse= 1.03998 val_loss= 1.44993 val_rmse= 1.10040 \t\ttime= 0.63331\n",
      "[*] Epoch: 0134 train_loss= 1.41003 train_rmse= 1.03157 val_loss= 1.44613 val_rmse= 1.09679 \t\ttime= 0.61735\n",
      "[*] Epoch: 0135 train_loss= 1.41315 train_rmse= 1.04508 val_loss= 1.44578 val_rmse= 1.09614 \t\ttime= 0.62732\n",
      "[*] Epoch: 0136 train_loss= 1.41159 train_rmse= 1.04028 val_loss= 1.44478 val_rmse= 1.09544 \t\ttime= 0.59740\n",
      "[*] Epoch: 0137 train_loss= 1.40565 train_rmse= 1.03635 val_loss= 1.44537 val_rmse= 1.09539 \t\ttime= 0.62433\n",
      "[*] Epoch: 0138 train_loss= 1.40257 train_rmse= 1.02875 val_loss= 1.44901 val_rmse= 1.09737 \t\ttime= 0.61236\n",
      "[*] Epoch: 0139 train_loss= 1.40337 train_rmse= 1.02990 val_loss= 1.44851 val_rmse= 1.09663 \t\ttime= 0.68616\n",
      "[*] Epoch: 0140 train_loss= 1.40200 train_rmse= 1.03995 val_loss= 1.44757 val_rmse= 1.09550 \t\ttime= 0.53657\n",
      "[*] Epoch: 0141 train_loss= 1.40136 train_rmse= 1.03628 val_loss= 1.44944 val_rmse= 1.09602 \t\ttime= 0.59440\n",
      "[*] Epoch: 0142 train_loss= 1.40383 train_rmse= 1.03753 val_loss= 1.45328 val_rmse= 1.09753 \t\ttime= 0.61336\n",
      "[*] Epoch: 0143 train_loss= 1.40542 train_rmse= 1.02624 val_loss= 1.45006 val_rmse= 1.09455 \t\ttime= 0.55352\n",
      "[*] Epoch: 0144 train_loss= 1.40377 train_rmse= 1.02652 val_loss= 1.44431 val_rmse= 1.08988 \t\ttime= 0.61635\n",
      "[*] Epoch: 0145 train_loss= 1.39843 train_rmse= 1.02018 val_loss= 1.44024 val_rmse= 1.08642 \t\ttime= 0.64428\n",
      "[*] Epoch: 0146 train_loss= 1.40563 train_rmse= 1.03527 val_loss= 1.44147 val_rmse= 1.08653 \t\ttime= 0.61037\n",
      "[*] Epoch: 0147 train_loss= 1.39710 train_rmse= 1.01704 val_loss= 1.44077 val_rmse= 1.08584 \t\ttime= 0.61436\n",
      "[*] Epoch: 0148 train_loss= 1.40018 train_rmse= 1.02050 val_loss= 1.43648 val_rmse= 1.08319 \t\ttime= 0.61934\n",
      "[*] Epoch: 0149 train_loss= 1.40449 train_rmse= 1.02744 val_loss= 1.43766 val_rmse= 1.08413 \t\ttime= 0.58444\n",
      "[*] Epoch: 0150 train_loss= 1.41103 train_rmse= 1.03040 val_loss= 1.44561 val_rmse= 1.08913 \t\ttime= 0.56449\n",
      "[*] Epoch: 0151 train_loss= 1.39195 train_rmse= 1.01687 val_loss= 1.44819 val_rmse= 1.09206 \t\ttime= 0.59242\n",
      "[*] Epoch: 0152 train_loss= 1.38980 train_rmse= 1.01961 val_loss= 1.45144 val_rmse= 1.09554 \t\ttime= 0.67519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0153 train_loss= 1.39677 train_rmse= 1.01989 val_loss= 1.45737 val_rmse= 1.10069 \t\ttime= 0.58443\n",
      "[*] Epoch: 0154 train_loss= 1.40259 train_rmse= 1.02241 val_loss= 1.46120 val_rmse= 1.10382 \t\ttime= 0.62932\n",
      "[*] Epoch: 0155 train_loss= 1.39586 train_rmse= 1.00875 val_loss= 1.45987 val_rmse= 1.10268 \t\ttime= 0.64627\n",
      "[*] Epoch: 0156 train_loss= 1.39393 train_rmse= 1.01419 val_loss= 1.45598 val_rmse= 1.09921 \t\ttime= 0.54355\n",
      "[*] Epoch: 0157 train_loss= 1.39105 train_rmse= 1.01300 val_loss= 1.45403 val_rmse= 1.09709 \t\ttime= 0.62233\n",
      "[*] Epoch: 0158 train_loss= 1.39429 train_rmse= 1.01875 val_loss= 1.44907 val_rmse= 1.09250 \t\ttime= 0.62533\n",
      "[*] Epoch: 0159 train_loss= 1.39910 train_rmse= 1.02253 val_loss= 1.44643 val_rmse= 1.09010 \t\ttime= 0.63131\n",
      "[*] Epoch: 0160 train_loss= 1.38768 train_rmse= 1.00973 val_loss= 1.44440 val_rmse= 1.08816 \t\ttime= 0.59641\n",
      "[*] Epoch: 0161 train_loss= 1.38974 train_rmse= 1.02157 val_loss= 1.44486 val_rmse= 1.08804 \t\ttime= 0.62034\n",
      "[*] Epoch: 0162 train_loss= 1.39463 train_rmse= 1.02222 val_loss= 1.45025 val_rmse= 1.09153 \t\ttime= 0.60239\n",
      "[*] Epoch: 0163 train_loss= 1.39782 train_rmse= 1.01925 val_loss= 1.45087 val_rmse= 1.09185 \t\ttime= 0.60837\n",
      "[*] Epoch: 0164 train_loss= 1.39729 train_rmse= 1.02702 val_loss= 1.44924 val_rmse= 1.09008 \t\ttime= 0.61236\n",
      "[*] Epoch: 0165 train_loss= 1.39236 train_rmse= 1.00722 val_loss= 1.45060 val_rmse= 1.09035 \t\ttime= 0.66023\n",
      "[*] Epoch: 0166 train_loss= 1.38762 train_rmse= 1.00312 val_loss= 1.45215 val_rmse= 1.09057 \t\ttime= 0.55452\n",
      "[*] Epoch: 0167 train_loss= 1.38745 train_rmse= 1.00755 val_loss= 1.44964 val_rmse= 1.08793 \t\ttime= 0.60339\n",
      "[*] Epoch: 0168 train_loss= 1.38882 train_rmse= 1.00573 val_loss= 1.44534 val_rmse= 1.08406 \t\ttime= 0.62633\n",
      "[*] Epoch: 0169 train_loss= 1.39038 train_rmse= 1.01037 val_loss= 1.44370 val_rmse= 1.08223 \t\ttime= 0.56748\n",
      "[*] Epoch: 0170 train_loss= 1.38714 train_rmse= 1.00950 val_loss= 1.44718 val_rmse= 1.08464 \t\ttime= 0.60638\n",
      "[*] Epoch: 0171 train_loss= 1.38715 train_rmse= 1.01181 val_loss= 1.45165 val_rmse= 1.08781 \t\ttime= 0.66323\n",
      "[*] Epoch: 0172 train_loss= 1.38727 train_rmse= 1.00816 val_loss= 1.45248 val_rmse= 1.08927 \t\ttime= 0.66422\n",
      "[*] Epoch: 0173 train_loss= 1.39063 train_rmse= 1.00416 val_loss= 1.46020 val_rmse= 1.09520 \t\ttime= 0.60539\n",
      "[*] Epoch: 0174 train_loss= 1.38486 train_rmse= 0.99777 val_loss= 1.46207 val_rmse= 1.09703 \t\ttime= 0.60339\n",
      "[*] Epoch: 0175 train_loss= 1.40146 train_rmse= 1.00011 val_loss= 1.45480 val_rmse= 1.09211 \t\ttime= 0.60139\n",
      "[*] Epoch: 0176 train_loss= 1.38191 train_rmse= 1.00499 val_loss= 1.44876 val_rmse= 1.08663 \t\ttime= 0.58843\n",
      "[*] Epoch: 0177 train_loss= 1.39222 train_rmse= 1.00522 val_loss= 1.45217 val_rmse= 1.08980 \t\ttime= 0.58444\n",
      "[*] Epoch: 0178 train_loss= 1.38701 train_rmse= 1.00866 val_loss= 1.45321 val_rmse= 1.09065 \t\ttime= 0.67719\n",
      "[*] Epoch: 0179 train_loss= 1.38585 train_rmse= 1.00378 val_loss= 1.45162 val_rmse= 1.08986 \t\ttime= 0.56848\n",
      "[*] Epoch: 0180 train_loss= 1.38611 train_rmse= 0.99966 val_loss= 1.45206 val_rmse= 1.09018 \t\ttime= 0.62633\n",
      "[*] Epoch: 0181 train_loss= 1.38720 train_rmse= 1.00800 val_loss= 1.46283 val_rmse= 1.09797 \t\ttime= 0.61536\n",
      "[*] Epoch: 0182 train_loss= 1.38787 train_rmse= 0.99367 val_loss= 1.46534 val_rmse= 1.10044 \t\ttime= 0.51163\n",
      "[*] Epoch: 0183 train_loss= 1.39434 train_rmse= 1.00230 val_loss= 1.45840 val_rmse= 1.09601 \t\ttime= 0.62333\n",
      "[*] Epoch: 0184 train_loss= 1.37739 train_rmse= 0.99621 val_loss= 1.45407 val_rmse= 1.09227 \t\ttime= 0.58843\n",
      "[*] Epoch: 0185 train_loss= 1.38536 train_rmse= 1.00084 val_loss= 1.44910 val_rmse= 1.08700 \t\ttime= 0.63430\n",
      "[*] Epoch: 0186 train_loss= 1.38555 train_rmse= 0.99553 val_loss= 1.44799 val_rmse= 1.08631 \t\ttime= 0.58544\n",
      "[*] Epoch: 0187 train_loss= 1.37940 train_rmse= 1.00925 val_loss= 1.44985 val_rmse= 1.08669 \t\ttime= 0.57945\n",
      "[*] Epoch: 0188 train_loss= 1.38338 train_rmse= 1.00179 val_loss= 1.44664 val_rmse= 1.08415 \t\ttime= 0.60438\n",
      "[*] Epoch: 0189 train_loss= 1.37646 train_rmse= 0.99803 val_loss= 1.44174 val_rmse= 1.07990 \t\ttime= 0.53656\n",
      "[*] Epoch: 0190 train_loss= 1.38594 train_rmse= 1.01191 val_loss= 1.44443 val_rmse= 1.08229 \t\ttime= 0.59641\n",
      "[*] Epoch: 0191 train_loss= 1.37889 train_rmse= 0.99756 val_loss= 1.45240 val_rmse= 1.08784 \t\ttime= 0.66325\n",
      "[*] Epoch: 0192 train_loss= 1.37503 train_rmse= 0.98976 val_loss= 1.45552 val_rmse= 1.09017 \t\ttime= 0.60638\n",
      "[*] Epoch: 0193 train_loss= 1.37481 train_rmse= 0.98977 val_loss= 1.45270 val_rmse= 1.08891 \t\ttime= 0.60039\n",
      "[*] Epoch: 0194 train_loss= 1.37237 train_rmse= 0.98808 val_loss= 1.45099 val_rmse= 1.08753 \t\ttime= 0.60339\n",
      "[*] Epoch: 0195 train_loss= 1.37686 train_rmse= 0.98899 val_loss= 1.45067 val_rmse= 1.08697 \t\ttime= 0.57047\n",
      "[*] Epoch: 0196 train_loss= 1.37813 train_rmse= 0.99706 val_loss= 1.45616 val_rmse= 1.09002 \t\ttime= 0.59740\n",
      "[*] Epoch: 0197 train_loss= 1.37826 train_rmse= 0.99397 val_loss= 1.45494 val_rmse= 1.08872 \t\ttime= 0.59641\n",
      "[*] Epoch: 0198 train_loss= 1.37945 train_rmse= 1.00022 val_loss= 1.44988 val_rmse= 1.08548 \t\ttime= 0.65924\n",
      "[*] Epoch: 0199 train_loss= 1.37906 train_rmse= 1.00222 val_loss= 1.45451 val_rmse= 1.08897 \t\ttime= 0.57047\n",
      "[*] Epoch: 0200 train_loss= 1.36671 train_rmse= 0.98389 val_loss= 1.45883 val_rmse= 1.09193 \t\ttime= 0.60837\n",
      "[*] Epoch: 0201 train_loss= 1.37531 train_rmse= 0.99171 val_loss= 1.45818 val_rmse= 1.09181 \t\ttime= 0.60538\n",
      "[*] Epoch: 0202 train_loss= 1.37405 train_rmse= 0.98779 val_loss= 1.45345 val_rmse= 1.08800 \t\ttime= 0.53058\n",
      "[*] Epoch: 0203 train_loss= 1.37244 train_rmse= 0.98577 val_loss= 1.44819 val_rmse= 1.08299 \t\ttime= 0.60239\n",
      "[*] Epoch: 0204 train_loss= 1.37465 train_rmse= 0.99040 val_loss= 1.44633 val_rmse= 1.08123 \t\ttime= 0.63231\n",
      "[*] Epoch: 0205 train_loss= 1.37222 train_rmse= 0.99860 val_loss= 1.44826 val_rmse= 1.08233 \t\ttime= 0.63031\n",
      "[*] Epoch: 0206 train_loss= 1.37184 train_rmse= 0.98343 val_loss= 1.44603 val_rmse= 1.08081 \t\ttime= 0.59641\n",
      "[*] Epoch: 0207 train_loss= 1.37288 train_rmse= 0.99220 val_loss= 1.44326 val_rmse= 1.07891 \t\ttime= 0.62332\n",
      "[*] Epoch: 0208 train_loss= 1.36884 train_rmse= 0.98704 val_loss= 1.44354 val_rmse= 1.07950 \t\ttime= 0.58045\n",
      "[*] Epoch: 0209 train_loss= 1.37393 train_rmse= 0.99333 val_loss= 1.44710 val_rmse= 1.08270 \t\ttime= 0.57148\n",
      "[*] Epoch: 0210 train_loss= 1.36578 train_rmse= 0.98313 val_loss= 1.44983 val_rmse= 1.08436 \t\ttime= 0.59142\n",
      "[*] Epoch: 0211 train_loss= 1.37080 train_rmse= 0.98577 val_loss= 1.44820 val_rmse= 1.08339 \t\ttime= 0.65624\n",
      "[*] Epoch: 0212 train_loss= 1.37705 train_rmse= 0.99287 val_loss= 1.44354 val_rmse= 1.07974 \t\ttime= 0.56150\n",
      "[*] Epoch: 0213 train_loss= 1.36764 train_rmse= 0.99221 val_loss= 1.44376 val_rmse= 1.07970 \t\ttime= 0.61236\n",
      "[*] Epoch: 0214 train_loss= 1.37002 train_rmse= 0.99705 val_loss= 1.44569 val_rmse= 1.08070 \t\ttime= 0.62932\n",
      "[*] Epoch: 0215 train_loss= 1.36833 train_rmse= 0.99666 val_loss= 1.45129 val_rmse= 1.08450 \t\ttime= 0.54754\n",
      "[*] Epoch: 0216 train_loss= 1.37155 train_rmse= 0.99024 val_loss= 1.45704 val_rmse= 1.08882 \t\ttime= 0.62732\n",
      "[*] Epoch: 0217 train_loss= 1.37022 train_rmse= 0.98946 val_loss= 1.45787 val_rmse= 1.08970 \t\ttime= 0.62732\n",
      "[*] Epoch: 0218 train_loss= 1.37158 train_rmse= 0.97950 val_loss= 1.45498 val_rmse= 1.08734 \t\ttime= 0.63929\n",
      "[*] Epoch: 0219 train_loss= 1.36786 train_rmse= 0.98548 val_loss= 1.45511 val_rmse= 1.08730 \t\ttime= 0.58743\n",
      "[*] Epoch: 0220 train_loss= 1.36901 train_rmse= 0.98488 val_loss= 1.45883 val_rmse= 1.09048 \t\ttime= 0.59641\n",
      "[*] Epoch: 0221 train_loss= 1.37464 train_rmse= 0.98229 val_loss= 1.45548 val_rmse= 1.08844 \t\ttime= 0.61436\n",
      "[*] Epoch: 0222 train_loss= 1.36641 train_rmse= 0.98742 val_loss= 1.45080 val_rmse= 1.08517 \t\ttime= 0.55252\n",
      "[*] Epoch: 0223 train_loss= 1.36463 train_rmse= 0.98480 val_loss= 1.44733 val_rmse= 1.08236 \t\ttime= 0.63829\n",
      "[*] Epoch: 0224 train_loss= 1.36201 train_rmse= 0.98052 val_loss= 1.44725 val_rmse= 1.08266 \t\ttime= 0.68118\n",
      "[*] Epoch: 0225 train_loss= 1.36768 train_rmse= 0.99717 val_loss= 1.45151 val_rmse= 1.08578 \t\ttime= 0.57147\n",
      "[*] Epoch: 0226 train_loss= 1.37323 train_rmse= 1.00811 val_loss= 1.45955 val_rmse= 1.09194 \t\ttime= 0.62034\n",
      "[*] Epoch: 0227 train_loss= 1.36782 train_rmse= 0.98154 val_loss= 1.45966 val_rmse= 1.09311 \t\ttime= 0.60638\n",
      "[*] Epoch: 0228 train_loss= 1.36404 train_rmse= 0.98018 val_loss= 1.45781 val_rmse= 1.09194 \t\ttime= 0.55950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0229 train_loss= 1.36928 train_rmse= 0.98469 val_loss= 1.45727 val_rmse= 1.09105 \t\ttime= 0.60139\n",
      "[*] Epoch: 0230 train_loss= 1.37148 train_rmse= 0.99093 val_loss= 1.45939 val_rmse= 1.09226 \t\ttime= 0.60438\n",
      "[*] Epoch: 0231 train_loss= 1.36799 train_rmse= 0.98381 val_loss= 1.46129 val_rmse= 1.09340 \t\ttime= 0.67420\n",
      "[*] Epoch: 0232 train_loss= 1.37062 train_rmse= 0.97874 val_loss= 1.45896 val_rmse= 1.09173 \t\ttime= 0.56250\n",
      "[*] Epoch: 0233 train_loss= 1.35987 train_rmse= 0.97185 val_loss= 1.45697 val_rmse= 1.09057 \t\ttime= 0.58743\n",
      "[*] Epoch: 0234 train_loss= 1.36201 train_rmse= 0.97627 val_loss= 1.45672 val_rmse= 1.09081 \t\ttime= 0.59641\n",
      "[*] Epoch: 0235 train_loss= 1.35436 train_rmse= 0.97010 val_loss= 1.45833 val_rmse= 1.09251 \t\ttime= 0.55452\n",
      "[*] Epoch: 0236 train_loss= 1.36813 train_rmse= 0.98860 val_loss= 1.45997 val_rmse= 1.09384 \t\ttime= 0.59740\n",
      "[*] Epoch: 0237 train_loss= 1.36294 train_rmse= 0.98317 val_loss= 1.45690 val_rmse= 1.09147 \t\ttime= 0.65825\n",
      "[*] Epoch: 0238 train_loss= 1.35664 train_rmse= 0.98203 val_loss= 1.45299 val_rmse= 1.08836 \t\ttime= 0.57845\n",
      "[*] Epoch: 0239 train_loss= 1.36394 train_rmse= 0.99524 val_loss= 1.45390 val_rmse= 1.08882 \t\ttime= 0.60837\n",
      "[*] Epoch: 0240 train_loss= 1.35829 train_rmse= 0.98198 val_loss= 1.45669 val_rmse= 1.09109 \t\ttime= 0.60239\n",
      "[*] Epoch: 0241 train_loss= 1.35487 train_rmse= 0.97815 val_loss= 1.45759 val_rmse= 1.09177 \t\ttime= 0.59940\n",
      "[*] Epoch: 0242 train_loss= 1.35536 train_rmse= 0.98062 val_loss= 1.45815 val_rmse= 1.09210 \t\ttime= 0.57746\n",
      "[*] Epoch: 0243 train_loss= 1.36261 train_rmse= 0.99139 val_loss= 1.45948 val_rmse= 1.09298 \t\ttime= 0.58544\n",
      "[*] Epoch: 0244 train_loss= 1.35421 train_rmse= 0.97402 val_loss= 1.46245 val_rmse= 1.09542 \t\ttime= 0.67619\n",
      "[*] Epoch: 0245 train_loss= 1.36125 train_rmse= 0.97139 val_loss= 1.46050 val_rmse= 1.09343 \t\ttime= 0.56749\n",
      "[*] Epoch: 0246 train_loss= 1.35366 train_rmse= 0.97232 val_loss= 1.45914 val_rmse= 1.09249 \t\ttime= 0.60139\n",
      "[*] Epoch: 0247 train_loss= 1.35595 train_rmse= 0.97465 val_loss= 1.45658 val_rmse= 1.09054 \t\ttime= 0.61236\n",
      "[*] Epoch: 0248 train_loss= 1.35330 train_rmse= 0.97022 val_loss= 1.45444 val_rmse= 1.08867 \t\ttime= 0.55651\n",
      "[*] Epoch: 0249 train_loss= 1.35539 train_rmse= 0.97884 val_loss= 1.45094 val_rmse= 1.08577 \t\ttime= 0.61336\n",
      "[*] Epoch: 0250 train_loss= 1.35386 train_rmse= 0.98198 val_loss= 1.45294 val_rmse= 1.08749 \t\ttime= 0.60039\n",
      "[*] Epoch: 0251 train_loss= 1.35818 train_rmse= 0.97943 val_loss= 1.45429 val_rmse= 1.08891 \t\ttime= 0.64228\n",
      "[*] Epoch: 0252 train_loss= 1.35545 train_rmse= 0.98058 val_loss= 1.45641 val_rmse= 1.09130 \t\ttime= 0.61535\n",
      "[*] Epoch: 0253 train_loss= 1.35846 train_rmse= 0.99362 val_loss= 1.46257 val_rmse= 1.09674 \t\ttime= 0.61336\n",
      "[*] Epoch: 0254 train_loss= 1.35351 train_rmse= 0.97534 val_loss= 1.46693 val_rmse= 1.10050 \t\ttime= 0.58444\n",
      "[*] Epoch: 0255 train_loss= 1.35301 train_rmse= 0.97411 val_loss= 1.47007 val_rmse= 1.10347 \t\ttime= 0.55053\n",
      "[*] Epoch: 0256 train_loss= 1.35335 train_rmse= 0.96983 val_loss= 1.46933 val_rmse= 1.10281 \t\ttime= 0.63729\n",
      "[*] Epoch: 0257 train_loss= 1.35384 train_rmse= 0.95887 val_loss= 1.46517 val_rmse= 1.09899 \t\ttime= 0.68118\n",
      "[*] Epoch: 0258 train_loss= 1.36352 train_rmse= 0.98005 val_loss= 1.45811 val_rmse= 1.09200 \t\ttime= 0.57446\n",
      "[*] Epoch: 0259 train_loss= 1.35546 train_rmse= 0.97604 val_loss= 1.45411 val_rmse= 1.08823 \t\ttime= 0.63231\n",
      "[*] Epoch: 0260 train_loss= 1.35532 train_rmse= 0.98127 val_loss= 1.45021 val_rmse= 1.08452 \t\ttime= 0.62034\n",
      "[*] Epoch: 0261 train_loss= 1.35373 train_rmse= 0.99002 val_loss= 1.45100 val_rmse= 1.08486 \t\ttime= 0.58843\n",
      "[*] Epoch: 0262 train_loss= 1.34922 train_rmse= 0.97511 val_loss= 1.45313 val_rmse= 1.08641 \t\ttime= 0.63331\n",
      "[*] Epoch: 0263 train_loss= 1.36072 train_rmse= 0.98837 val_loss= 1.46118 val_rmse= 1.09343 \t\ttime= 0.60039\n",
      "[*] Epoch: 0264 train_loss= 1.34912 train_rmse= 0.97753 val_loss= 1.46701 val_rmse= 1.09933 \t\ttime= 0.66123\n",
      "[*] Epoch: 0265 train_loss= 1.35118 train_rmse= 0.96119 val_loss= 1.46800 val_rmse= 1.10111 \t\ttime= 0.63830\n",
      "[*] Epoch: 0266 train_loss= 1.34503 train_rmse= 0.96346 val_loss= 1.47230 val_rmse= 1.10557 \t\ttime= 0.59641\n",
      "[*] Epoch: 0267 train_loss= 1.35785 train_rmse= 0.97116 val_loss= 1.47119 val_rmse= 1.10479 \t\ttime= 0.59641\n",
      "[*] Epoch: 0268 train_loss= 1.35619 train_rmse= 0.96723 val_loss= 1.46602 val_rmse= 1.10032 \t\ttime= 0.55252\n",
      "[*] Epoch: 0269 train_loss= 1.35112 train_rmse= 0.96771 val_loss= 1.45811 val_rmse= 1.09299 \t\ttime= 0.63231\n",
      "[*] Epoch: 0270 train_loss= 1.35941 train_rmse= 0.98574 val_loss= 1.46409 val_rmse= 1.09764 \t\ttime= 0.69813\n",
      "[*] Epoch: 0271 train_loss= 1.35183 train_rmse= 0.97645 val_loss= 1.46489 val_rmse= 1.09789 \t\ttime= 0.58543\n",
      "[*] Epoch: 0272 train_loss= 1.35651 train_rmse= 0.96763 val_loss= 1.45778 val_rmse= 1.09216 \t\ttime= 0.61535\n",
      "[*] Epoch: 0273 train_loss= 1.34739 train_rmse= 0.96639 val_loss= 1.45206 val_rmse= 1.08677 \t\ttime= 0.62233\n",
      "[*] Epoch: 0274 train_loss= 1.35113 train_rmse= 0.97050 val_loss= 1.45023 val_rmse= 1.08485 \t\ttime= 0.55252\n",
      "[*] Epoch: 0275 train_loss= 1.35559 train_rmse= 0.98566 val_loss= 1.45599 val_rmse= 1.08886 \t\ttime= 0.57546\n",
      "[*] Epoch: 0276 train_loss= 1.35251 train_rmse= 0.97080 val_loss= 1.45724 val_rmse= 1.08949 \t\ttime= 0.59540\n",
      "[*] Epoch: 0277 train_loss= 1.36247 train_rmse= 0.97198 val_loss= 1.45393 val_rmse= 1.08782 \t\ttime= 0.62233\n",
      "[*] Epoch: 0278 train_loss= 1.34960 train_rmse= 0.97063 val_loss= 1.46108 val_rmse= 1.09489 \t\ttime= 0.59640\n",
      "[*] Epoch: 0279 train_loss= 1.35266 train_rmse= 0.96990 val_loss= 1.46948 val_rmse= 1.10344 \t\ttime= 0.57147\n",
      "[*] Epoch: 0280 train_loss= 1.34870 train_rmse= 0.96543 val_loss= 1.47465 val_rmse= 1.10862 \t\ttime= 0.62134\n",
      "[*] Epoch: 0281 train_loss= 1.34898 train_rmse= 0.96753 val_loss= 1.47185 val_rmse= 1.10695 \t\ttime= 0.54155\n",
      "[*] Epoch: 0282 train_loss= 1.34541 train_rmse= 0.96941 val_loss= 1.46565 val_rmse= 1.10157 \t\ttime= 0.62433\n",
      "[*] Epoch: 0283 train_loss= 1.34111 train_rmse= 0.96432 val_loss= 1.46155 val_rmse= 1.09785 \t\ttime= 0.70811\n",
      "[*] Epoch: 0284 train_loss= 1.35051 train_rmse= 0.97862 val_loss= 1.45899 val_rmse= 1.09507 \t\ttime= 0.62433\n",
      "[*] Epoch: 0285 train_loss= 1.34546 train_rmse= 0.97798 val_loss= 1.46032 val_rmse= 1.09514 \t\ttime= 0.62832\n",
      "[*] Epoch: 0286 train_loss= 1.34582 train_rmse= 0.96720 val_loss= 1.45727 val_rmse= 1.09205 \t\ttime= 0.63031\n",
      "[*] Epoch: 0287 train_loss= 1.34030 train_rmse= 0.95924 val_loss= 1.45016 val_rmse= 1.08569 \t\ttime= 0.55651\n",
      "[*] Epoch: 0288 train_loss= 1.34290 train_rmse= 0.96491 val_loss= 1.44612 val_rmse= 1.08146 \t\ttime= 0.62732\n",
      "[*] Epoch: 0289 train_loss= 1.35619 train_rmse= 0.98387 val_loss= 1.45013 val_rmse= 1.08373 \t\ttime= 0.61137\n",
      "[*] Epoch: 0290 train_loss= 1.34003 train_rmse= 0.96439 val_loss= 1.45263 val_rmse= 1.08465 \t\ttime= 0.63331\n",
      "[*] Epoch: 0291 train_loss= 1.35368 train_rmse= 0.97584 val_loss= 1.45452 val_rmse= 1.08637 \t\ttime= 0.60139\n",
      "[*] Epoch: 0292 train_loss= 1.34305 train_rmse= 0.96247 val_loss= 1.45598 val_rmse= 1.08840 \t\ttime= 0.61036\n",
      "[*] Epoch: 0293 train_loss= 1.34440 train_rmse= 0.95492 val_loss= 1.45579 val_rmse= 1.08916 \t\ttime= 0.59740\n",
      "[*] Epoch: 0294 train_loss= 1.34855 train_rmse= 0.96640 val_loss= 1.45866 val_rmse= 1.09242 \t\ttime= 0.54454\n",
      "[*] Epoch: 0295 train_loss= 1.34103 train_rmse= 0.97558 val_loss= 1.46426 val_rmse= 1.09713 \t\ttime= 0.60438\n",
      "[*] Epoch: 0296 train_loss= 1.34780 train_rmse= 0.96764 val_loss= 1.46220 val_rmse= 1.09570 \t\ttime= 0.66422\n",
      "[*] Epoch: 0297 train_loss= 1.34795 train_rmse= 0.97402 val_loss= 1.45494 val_rmse= 1.09009 \t\ttime= 0.56848\n",
      "[*] Epoch: 0298 train_loss= 1.34027 train_rmse= 0.97007 val_loss= 1.44984 val_rmse= 1.08539 \t\ttime= 0.61935\n",
      "[*] Epoch: 0299 train_loss= 1.34233 train_rmse= 0.96440 val_loss= 1.44535 val_rmse= 1.08057 \t\ttime= 0.60139\n",
      "[*] Epoch: 0300 train_loss= 1.34996 train_rmse= 0.97468 val_loss= 1.45052 val_rmse= 1.08392 \t\ttime= 0.55651\n",
      "[*] Epoch: 0301 train_loss= 1.34076 train_rmse= 0.96345 val_loss= 1.45842 val_rmse= 1.08851 \t\ttime= 0.59541\n",
      "[*] Epoch: 0302 train_loss= 1.35933 train_rmse= 0.96140 val_loss= 1.45611 val_rmse= 1.08728 \t\ttime= 0.58244\n",
      "[*] Epoch: 0303 train_loss= 1.33981 train_rmse= 0.96160 val_loss= 1.45439 val_rmse= 1.08664 \t\ttime= 0.69813\n",
      "[*] Epoch: 0304 train_loss= 1.34165 train_rmse= 0.96130 val_loss= 1.45290 val_rmse= 1.08558 \t\ttime= 0.56349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0305 train_loss= 1.35005 train_rmse= 0.96041 val_loss= 1.45939 val_rmse= 1.09244 \t\ttime= 0.59020\n",
      "[*] Epoch: 0306 train_loss= 1.33723 train_rmse= 0.96202 val_loss= 1.46741 val_rmse= 1.09949 \t\ttime= 0.61236\n",
      "[*] Epoch: 0307 train_loss= 1.34527 train_rmse= 0.96396 val_loss= 1.46732 val_rmse= 1.10051 \t\ttime= 0.56349\n",
      "[*] Epoch: 0308 train_loss= 1.34164 train_rmse= 0.95939 val_loss= 1.46156 val_rmse= 1.09667 \t\ttime= 0.62932\n",
      "[*] Epoch: 0309 train_loss= 1.34797 train_rmse= 0.97026 val_loss= 1.45480 val_rmse= 1.09028 \t\ttime= 0.65824\n",
      "[*] Epoch: 0310 train_loss= 1.35078 train_rmse= 0.96871 val_loss= 1.45583 val_rmse= 1.09159 \t\ttime= 0.57945\n",
      "[*] Epoch: 0311 train_loss= 1.35100 train_rmse= 0.97659 val_loss= 1.46291 val_rmse= 1.09764 \t\ttime= 0.61535\n",
      "[*] Epoch: 0312 train_loss= 1.34812 train_rmse= 0.96149 val_loss= 1.46409 val_rmse= 1.09749 \t\ttime= 0.62134\n",
      "[*] Epoch: 0313 train_loss= 1.34383 train_rmse= 0.96457 val_loss= 1.45931 val_rmse= 1.09277 \t\ttime= 0.57147\n",
      "[*] Epoch: 0314 train_loss= 1.34214 train_rmse= 0.96275 val_loss= 1.45209 val_rmse= 1.08586 \t\ttime= 0.57845\n",
      "[*] Epoch: 0315 train_loss= 1.33543 train_rmse= 0.95976 val_loss= 1.44635 val_rmse= 1.08023 \t\ttime= 0.61236\n",
      "[*] Epoch: 0316 train_loss= 1.34052 train_rmse= 0.96197 val_loss= 1.44405 val_rmse= 1.07864 \t\ttime= 0.66622\n",
      "[*] Epoch: 0317 train_loss= 1.34273 train_rmse= 0.96192 val_loss= 1.44739 val_rmse= 1.08146 \t\ttime= 0.57446\n",
      "[*] Epoch: 0318 train_loss= 1.33834 train_rmse= 0.96370 val_loss= 1.45112 val_rmse= 1.08424 \t\ttime= 0.60538\n",
      "[*] Epoch: 0319 train_loss= 1.34503 train_rmse= 0.96739 val_loss= 1.45108 val_rmse= 1.08520 \t\ttime= 0.64428\n",
      "[*] Epoch: 0320 train_loss= 1.34377 train_rmse= 0.96544 val_loss= 1.44776 val_rmse= 1.08346 \t\ttime= 0.53456\n",
      "[*] Epoch: 0321 train_loss= 1.33194 train_rmse= 0.96138 val_loss= 1.44826 val_rmse= 1.08437 \t\ttime= 0.59740\n",
      "[*] Epoch: 0322 train_loss= 1.33557 train_rmse= 0.96654 val_loss= 1.45307 val_rmse= 1.08873 \t\ttime= 0.65624\n",
      "[*] Epoch: 0323 train_loss= 1.33178 train_rmse= 0.95446 val_loss= 1.45885 val_rmse= 1.09390 \t\ttime= 0.61635\n",
      "[*] Epoch: 0324 train_loss= 1.33561 train_rmse= 0.96195 val_loss= 1.46152 val_rmse= 1.09630 \t\ttime= 0.61137\n",
      "[*] Epoch: 0325 train_loss= 1.34297 train_rmse= 0.95736 val_loss= 1.45758 val_rmse= 1.09281 \t\ttime= 0.61635\n",
      "[*] Epoch: 0326 train_loss= 1.33530 train_rmse= 0.95295 val_loss= 1.45207 val_rmse= 1.08780 \t\ttime= 0.58145\n",
      "[*] Epoch: 0327 train_loss= 1.34280 train_rmse= 0.96569 val_loss= 1.44526 val_rmse= 1.08165 \t\ttime= 0.59142\n",
      "[*] Epoch: 0328 train_loss= 1.33905 train_rmse= 0.96942 val_loss= 1.44409 val_rmse= 1.08113 \t\ttime= 0.60139\n",
      "[*] Epoch: 0329 train_loss= 1.33831 train_rmse= 0.97146 val_loss= 1.44899 val_rmse= 1.08427 \t\ttime= 0.65924\n",
      "[*] Epoch: 0330 train_loss= 1.33872 train_rmse= 0.97086 val_loss= 1.45138 val_rmse= 1.08498 \t\ttime= 0.55552\n",
      "[*] Epoch: 0331 train_loss= 1.33859 train_rmse= 0.96024 val_loss= 1.45139 val_rmse= 1.08487 \t\ttime= 0.59441\n",
      "[*] Epoch: 0332 train_loss= 1.33307 train_rmse= 0.95638 val_loss= 1.44873 val_rmse= 1.08285 \t\ttime= 0.63630\n",
      "[*] Epoch: 0333 train_loss= 1.33261 train_rmse= 0.95607 val_loss= 1.44955 val_rmse= 1.08351 \t\ttime= 0.53856\n",
      "[*] Epoch: 0334 train_loss= 1.34330 train_rmse= 0.95068 val_loss= 1.45562 val_rmse= 1.08957 \t\ttime= 0.60040\n",
      "[*] Epoch: 0335 train_loss= 1.33053 train_rmse= 0.95614 val_loss= 1.46397 val_rmse= 1.09736 \t\ttime= 0.63032\n",
      "[*] Epoch: 0336 train_loss= 1.33339 train_rmse= 0.95741 val_loss= 1.46720 val_rmse= 1.10062 \t\ttime= 0.64428\n",
      "[*] Epoch: 0337 train_loss= 1.34103 train_rmse= 0.96031 val_loss= 1.46382 val_rmse= 1.09844 \t\ttime= 0.61137\n",
      "[*] Epoch: 0338 train_loss= 1.33435 train_rmse= 0.97013 val_loss= 1.45959 val_rmse= 1.09479 \t\ttime= 0.59840\n",
      "[*] Epoch: 0339 train_loss= 1.33652 train_rmse= 0.96586 val_loss= 1.45603 val_rmse= 1.09103 \t\ttime= 0.61635\n",
      "[*] Epoch: 0340 train_loss= 1.32430 train_rmse= 0.95348 val_loss= 1.45821 val_rmse= 1.09220 \t\ttime= 0.60039\n",
      "[*] Epoch: 0341 train_loss= 1.33035 train_rmse= 0.96069 val_loss= 1.46195 val_rmse= 1.09440 \t\ttime= 0.59342\n",
      "[*] Epoch: 0342 train_loss= 1.33941 train_rmse= 0.95997 val_loss= 1.46081 val_rmse= 1.09283 \t\ttime= 0.68218\n",
      "[*] Epoch: 0343 train_loss= 1.32721 train_rmse= 0.95546 val_loss= 1.45808 val_rmse= 1.09078 \t\ttime= 0.55950\n",
      "[*] Epoch: 0344 train_loss= 1.32736 train_rmse= 0.95819 val_loss= 1.45764 val_rmse= 1.09088 \t\ttime= 0.63730\n",
      "[*] Epoch: 0345 train_loss= 1.33647 train_rmse= 0.95559 val_loss= 1.46226 val_rmse= 1.09576 \t\ttime= 0.60738\n",
      "[*] Epoch: 0346 train_loss= 1.32844 train_rmse= 0.94438 val_loss= 1.46459 val_rmse= 1.09861 \t\ttime= 0.53158\n",
      "[*] Epoch: 0347 train_loss= 1.33723 train_rmse= 0.96295 val_loss= 1.46435 val_rmse= 1.09865 \t\ttime= 0.56349\n",
      "[*] Epoch: 0348 train_loss= 1.33118 train_rmse= 0.95515 val_loss= 1.46187 val_rmse= 1.09632 \t\ttime= 0.63630\n",
      "[*] Epoch: 0349 train_loss= 1.32776 train_rmse= 0.95502 val_loss= 1.45967 val_rmse= 1.09424 \t\ttime= 0.65924\n",
      "[*] Epoch: 0350 train_loss= 1.32848 train_rmse= 0.94857 val_loss= 1.45470 val_rmse= 1.08967 \t\ttime= 0.56848\n",
      "[*] Epoch: 0351 train_loss= 1.32954 train_rmse= 0.95995 val_loss= 1.45065 val_rmse= 1.08578 \t\ttime= 0.60139\n",
      "[*] Epoch: 0352 train_loss= 1.32913 train_rmse= 0.96302 val_loss= 1.45493 val_rmse= 1.08873 \t\ttime= 0.60538\n",
      "[*] Epoch: 0353 train_loss= 1.32047 train_rmse= 0.95568 val_loss= 1.46059 val_rmse= 1.09334 \t\ttime= 0.54554\n",
      "[*] Epoch: 0354 train_loss= 1.32482 train_rmse= 0.94871 val_loss= 1.46125 val_rmse= 1.09411 \t\ttime= 0.58045\n",
      "[*] Epoch: 0355 train_loss= 1.32579 train_rmse= 0.95145 val_loss= 1.46021 val_rmse= 1.09391 \t\ttime= 0.64029\n",
      "[*] Epoch: 0356 train_loss= 1.32638 train_rmse= 0.95458 val_loss= 1.46049 val_rmse= 1.09511 \t\ttime= 0.60438\n",
      "[*] Epoch: 0357 train_loss= 1.32600 train_rmse= 0.95640 val_loss= 1.46644 val_rmse= 1.10135 \t\ttime= 0.65026\n",
      "[*] Epoch: 0358 train_loss= 1.32852 train_rmse= 0.94577 val_loss= 1.46787 val_rmse= 1.10320 \t\ttime= 0.63033\n",
      "[*] Epoch: 0359 train_loss= 1.32947 train_rmse= 0.94271 val_loss= 1.46485 val_rmse= 1.10107 \t\ttime= 0.60239\n",
      "[*] Epoch: 0360 train_loss= 1.34048 train_rmse= 0.96031 val_loss= 1.45637 val_rmse= 1.09368 \t\ttime= 0.64328\n",
      "[*] Epoch: 0361 train_loss= 1.32052 train_rmse= 0.96309 val_loss= 1.45271 val_rmse= 1.08994 \t\ttime= 0.73104\n",
      "[*] Epoch: 0362 train_loss= 1.32211 train_rmse= 0.95908 val_loss= 1.45289 val_rmse= 1.08941 \t\ttime= 0.69016\n",
      "[*] Epoch: 0363 train_loss= 1.33083 train_rmse= 0.96994 val_loss= 1.45957 val_rmse= 1.09459 \t\ttime= 0.68118\n",
      "[*] Epoch: 0364 train_loss= 1.32789 train_rmse= 0.95867 val_loss= 1.46254 val_rmse= 1.09681 \t\ttime= 0.73404\n",
      "[*] Epoch: 0365 train_loss= 1.32478 train_rmse= 0.94786 val_loss= 1.46183 val_rmse= 1.09640 \t\ttime= 0.71509\n",
      "[*] Epoch: 0366 train_loss= 1.32815 train_rmse= 0.94669 val_loss= 1.45918 val_rmse= 1.09461 \t\ttime= 0.80684\n",
      "[*] Epoch: 0367 train_loss= 1.32702 train_rmse= 0.95259 val_loss= 1.46124 val_rmse= 1.09715 \t\ttime= 0.81183\n",
      "[*] Epoch: 0368 train_loss= 1.32344 train_rmse= 0.95463 val_loss= 1.46505 val_rmse= 1.10088 \t\ttime= 0.83777\n",
      "[*] Epoch: 0369 train_loss= 1.31428 train_rmse= 0.94814 val_loss= 1.46504 val_rmse= 1.10068 \t\ttime= 0.68517\n",
      "[*] Epoch: 0370 train_loss= 1.32680 train_rmse= 0.96371 val_loss= 1.46174 val_rmse= 1.09720 \t\ttime= 0.60039\n",
      "[*] Epoch: 0371 train_loss= 1.32527 train_rmse= 0.96079 val_loss= 1.45825 val_rmse= 1.09347 \t\ttime= 0.62134\n",
      "[*] Epoch: 0372 train_loss= 1.32631 train_rmse= 0.95461 val_loss= 1.45358 val_rmse= 1.08826 \t\ttime= 0.63830\n",
      "[*] Epoch: 0373 train_loss= 1.32549 train_rmse= 0.96248 val_loss= 1.45666 val_rmse= 1.09056 \t\ttime= 0.64527\n",
      "[*] Epoch: 0374 train_loss= 1.32171 train_rmse= 0.94146 val_loss= 1.46064 val_rmse= 1.09421 \t\ttime= 0.58643\n",
      "[*] Epoch: 0375 train_loss= 1.31781 train_rmse= 0.94489 val_loss= 1.46316 val_rmse= 1.09745 \t\ttime= 0.58444\n",
      "[*] Epoch: 0376 train_loss= 1.33024 train_rmse= 0.95159 val_loss= 1.46038 val_rmse= 1.09579 \t\ttime= 0.61236\n",
      "[*] Epoch: 0377 train_loss= 1.31630 train_rmse= 0.95186 val_loss= 1.45831 val_rmse= 1.09434 \t\ttime= 0.53856\n",
      "[*] Epoch: 0378 train_loss= 1.32048 train_rmse= 0.94889 val_loss= 1.45455 val_rmse= 1.09107 \t\ttime= 0.58842\n",
      "[*] Epoch: 0379 train_loss= 1.32376 train_rmse= 0.96366 val_loss= 1.45750 val_rmse= 1.09328 \t\ttime= 0.64727\n",
      "[*] Epoch: 0380 train_loss= 1.32334 train_rmse= 0.96129 val_loss= 1.46571 val_rmse= 1.10005 \t\ttime= 0.58643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0381 train_loss= 1.33059 train_rmse= 0.94719 val_loss= 1.46922 val_rmse= 1.10356 \t\ttime= 0.64128\n",
      "[*] Epoch: 0382 train_loss= 1.32238 train_rmse= 0.94794 val_loss= 1.46696 val_rmse= 1.10190 \t\ttime= 0.60139\n",
      "[*] Epoch: 0383 train_loss= 1.32299 train_rmse= 0.94434 val_loss= 1.46103 val_rmse= 1.09637 \t\ttime= 0.58244\n",
      "[*] Epoch: 0384 train_loss= 1.31685 train_rmse= 0.93988 val_loss= 1.45942 val_rmse= 1.09522 \t\ttime= 0.58344\n",
      "[*] Epoch: 0385 train_loss= 1.31565 train_rmse= 0.94925 val_loss= 1.46172 val_rmse= 1.09722 \t\ttime= 0.65128\n",
      "[*] Epoch: 0386 train_loss= 1.31813 train_rmse= 0.94268 val_loss= 1.46136 val_rmse= 1.09659 \t\ttime= 0.65524\n",
      "[*] Epoch: 0387 train_loss= 1.31994 train_rmse= 0.94589 val_loss= 1.45898 val_rmse= 1.09453 \t\ttime= 0.55651\n",
      "[*] Epoch: 0388 train_loss= 1.32124 train_rmse= 0.95486 val_loss= 1.45993 val_rmse= 1.09563 \t\ttime= 0.60139\n",
      "[*] Epoch: 0389 train_loss= 1.30659 train_rmse= 0.94307 val_loss= 1.46110 val_rmse= 1.09657 \t\ttime= 0.62832\n",
      "[*] Epoch: 0390 train_loss= 1.31786 train_rmse= 0.94617 val_loss= 1.46074 val_rmse= 1.09589 \t\ttime= 0.57162\n",
      "[*] Epoch: 0391 train_loss= 1.32228 train_rmse= 0.95248 val_loss= 1.46423 val_rmse= 1.09922 \t\ttime= 0.61635\n",
      "[*] Epoch: 0392 train_loss= 1.31469 train_rmse= 0.94130 val_loss= 1.46519 val_rmse= 1.10011 \t\ttime= 0.63231\n",
      "[*] Epoch: 0393 train_loss= 1.31197 train_rmse= 0.94736 val_loss= 1.46627 val_rmse= 1.10109 \t\ttime= 0.59341\n",
      "[*] Epoch: 0394 train_loss= 1.31743 train_rmse= 0.94221 val_loss= 1.46219 val_rmse= 1.09752 \t\ttime= 0.59342\n",
      "[*] Epoch: 0395 train_loss= 1.31341 train_rmse= 0.94572 val_loss= 1.45757 val_rmse= 1.09342 \t\ttime= 0.60239\n",
      "[*] Epoch: 0396 train_loss= 1.30776 train_rmse= 0.94477 val_loss= 1.45791 val_rmse= 1.09388 \t\ttime= 0.60339\n",
      "[*] Epoch: 0397 train_loss= 1.31488 train_rmse= 0.94473 val_loss= 1.46090 val_rmse= 1.09698 \t\ttime= 0.60139\n",
      "[*] Epoch: 0398 train_loss= 1.31344 train_rmse= 0.94329 val_loss= 1.46033 val_rmse= 1.09643 \t\ttime= 0.60139\n",
      "[*] Epoch: 0399 train_loss= 1.31528 train_rmse= 0.95563 val_loss= 1.46241 val_rmse= 1.09837 \t\ttime= 0.66123\n",
      "[*] Epoch: 0400 train_loss= 1.31162 train_rmse= 0.93815 val_loss= 1.46331 val_rmse= 1.09953 \t\ttime= 0.56150\n",
      "[*] Epoch: 0401 train_loss= 1.30643 train_rmse= 0.93984 val_loss= 1.46460 val_rmse= 1.10092 \t\ttime= 0.62633\n",
      "[*] Epoch: 0402 train_loss= 1.31261 train_rmse= 0.94806 val_loss= 1.46741 val_rmse= 1.10333 \t\ttime= 0.61835\n",
      "[*] Epoch: 0403 train_loss= 1.31366 train_rmse= 0.94442 val_loss= 1.46729 val_rmse= 1.10313 \t\ttime= 0.52759\n",
      "[*] Epoch: 0404 train_loss= 1.31122 train_rmse= 0.94212 val_loss= 1.46501 val_rmse= 1.10054 \t\ttime= 0.61835\n",
      "[*] Epoch: 0405 train_loss= 1.30499 train_rmse= 0.94021 val_loss= 1.46253 val_rmse= 1.09760 \t\ttime= 0.65924\n",
      "[*] Epoch: 0406 train_loss= 1.31578 train_rmse= 0.94066 val_loss= 1.45827 val_rmse= 1.09335 \t\ttime= 0.64428\n",
      "[*] Epoch: 0407 train_loss= 1.30848 train_rmse= 0.93911 val_loss= 1.45521 val_rmse= 1.09047 \t\ttime= 0.60638\n",
      "[*] Epoch: 0408 train_loss= 1.31163 train_rmse= 0.94878 val_loss= 1.45514 val_rmse= 1.09033 \t\ttime= 0.59641\n",
      "[*] Epoch: 0409 train_loss= 1.31518 train_rmse= 0.95216 val_loss= 1.45678 val_rmse= 1.09151 \t\ttime= 0.62832\n",
      "[*] Epoch: 0410 train_loss= 1.31404 train_rmse= 0.95226 val_loss= 1.45670 val_rmse= 1.09116 \t\ttime= 0.61635\n",
      "[*] Epoch: 0411 train_loss= 1.31616 train_rmse= 0.95860 val_loss= 1.45895 val_rmse= 1.09293 \t\ttime= 0.60936\n",
      "[*] Epoch: 0412 train_loss= 1.31479 train_rmse= 0.94706 val_loss= 1.46797 val_rmse= 1.10173 \t\ttime= 0.69514\n",
      "[*] Epoch: 0413 train_loss= 1.32366 train_rmse= 0.94261 val_loss= 1.46996 val_rmse= 1.10405 \t\ttime= 0.61934\n",
      "[*] Epoch: 0414 train_loss= 1.30892 train_rmse= 0.92778 val_loss= 1.46900 val_rmse= 1.10384 \t\ttime= 0.63331\n",
      "[*] Epoch: 0415 train_loss= 1.31148 train_rmse= 0.93330 val_loss= 1.46708 val_rmse= 1.10253 \t\ttime= 0.61535\n",
      "[*] Epoch: 0416 train_loss= 1.30649 train_rmse= 0.93722 val_loss= 1.46501 val_rmse= 1.10085 \t\ttime= 0.55850\n",
      "[*] Epoch: 0417 train_loss= 1.31024 train_rmse= 0.94716 val_loss= 1.46331 val_rmse= 1.09900 \t\ttime= 0.61436\n",
      "[*] Epoch: 0418 train_loss= 1.30931 train_rmse= 0.94027 val_loss= 1.46148 val_rmse= 1.09725 \t\ttime= 0.68118\n",
      "[*] Epoch: 0419 train_loss= 1.30694 train_rmse= 0.93907 val_loss= 1.46001 val_rmse= 1.09582 \t\ttime= 0.60537\n",
      "[*] Epoch: 0420 train_loss= 1.30652 train_rmse= 0.94797 val_loss= 1.46064 val_rmse= 1.09613 \t\ttime= 0.61934\n",
      "[*] Epoch: 0421 train_loss= 1.30204 train_rmse= 0.94422 val_loss= 1.46548 val_rmse= 1.10044 \t\ttime= 0.64328\n",
      "[*] Epoch: 0422 train_loss= 1.31351 train_rmse= 0.95029 val_loss= 1.47041 val_rmse= 1.10514 \t\ttime= 0.62234\n",
      "[*] Epoch: 0423 train_loss= 1.30882 train_rmse= 0.93345 val_loss= 1.46919 val_rmse= 1.10443 \t\ttime= 0.63929\n",
      "[*] Epoch: 0424 train_loss= 1.30300 train_rmse= 0.92827 val_loss= 1.46554 val_rmse= 1.10186 \t\ttime= 0.65425\n",
      "[*] Epoch: 0425 train_loss= 1.31219 train_rmse= 0.94538 val_loss= 1.46595 val_rmse= 1.10291 \t\ttime= 0.64526\n",
      "[*] Epoch: 0426 train_loss= 1.30493 train_rmse= 0.93984 val_loss= 1.46560 val_rmse= 1.10319 \t\ttime= 0.59840\n",
      "[*] Epoch: 0427 train_loss= 1.30924 train_rmse= 0.94083 val_loss= 1.46305 val_rmse= 1.10089 \t\ttime= 0.59142\n",
      "[*] Epoch: 0428 train_loss= 1.30506 train_rmse= 0.94297 val_loss= 1.46100 val_rmse= 1.09848 \t\ttime= 0.60538\n",
      "[*] Epoch: 0429 train_loss= 1.30882 train_rmse= 0.94101 val_loss= 1.45891 val_rmse= 1.09595 \t\ttime= 0.55950\n",
      "[*] Epoch: 0430 train_loss= 1.30374 train_rmse= 0.93904 val_loss= 1.46213 val_rmse= 1.09847 \t\ttime= 0.61038\n",
      "[*] Epoch: 0431 train_loss= 1.30070 train_rmse= 0.93061 val_loss= 1.46404 val_rmse= 1.09971 \t\ttime= 0.67021\n",
      "[*] Epoch: 0432 train_loss= 1.30504 train_rmse= 0.94220 val_loss= 1.46649 val_rmse= 1.10168 \t\ttime= 0.57446\n",
      "[*] Epoch: 0433 train_loss= 1.30708 train_rmse= 0.94526 val_loss= 1.47129 val_rmse= 1.10637 \t\ttime= 0.59641\n",
      "[*] Epoch: 0434 train_loss= 1.29725 train_rmse= 0.93265 val_loss= 1.47806 val_rmse= 1.11409 \t\ttime= 0.63131\n",
      "[*] Epoch: 0435 train_loss= 1.30307 train_rmse= 0.93244 val_loss= 1.47852 val_rmse= 1.11559 \t\ttime= 0.53158\n",
      "[*] Epoch: 0436 train_loss= 1.31225 train_rmse= 0.93766 val_loss= 1.47292 val_rmse= 1.11086 \t\ttime= 0.60638\n",
      "[*] Epoch: 0437 train_loss= 1.30418 train_rmse= 0.93939 val_loss= 1.46978 val_rmse= 1.10827 \t\ttime= 0.62034\n",
      "[*] Epoch: 0438 train_loss= 1.30258 train_rmse= 0.94448 val_loss= 1.46931 val_rmse= 1.10779 \t\ttime= 0.69514\n",
      "[*] Epoch: 0439 train_loss= 1.30629 train_rmse= 0.94015 val_loss= 1.46625 val_rmse= 1.10422 \t\ttime= 0.63331\n",
      "[*] Epoch: 0440 train_loss= 1.29821 train_rmse= 0.93113 val_loss= 1.46254 val_rmse= 1.09979 \t\ttime= 0.59441\n",
      "[*] Epoch: 0441 train_loss= 1.29543 train_rmse= 0.93873 val_loss= 1.46096 val_rmse= 1.09688 \t\ttime= 0.63131\n",
      "[*] Epoch: 0442 train_loss= 1.30237 train_rmse= 0.94014 val_loss= 1.46562 val_rmse= 1.10026 \t\ttime= 0.58144\n",
      "[*] Epoch: 0443 train_loss= 1.29803 train_rmse= 0.93814 val_loss= 1.47078 val_rmse= 1.10472 \t\ttime= 0.59740\n",
      "[*] Epoch: 0444 train_loss= 1.30421 train_rmse= 0.93284 val_loss= 1.47293 val_rmse= 1.10748 \t\ttime= 0.69614\n",
      "[*] Epoch: 0445 train_loss= 1.29952 train_rmse= 0.93048 val_loss= 1.47126 val_rmse= 1.10679 \t\ttime= 0.57347\n",
      "[*] Epoch: 0446 train_loss= 1.30151 train_rmse= 0.92571 val_loss= 1.46830 val_rmse= 1.10477 \t\ttime= 0.61037\n",
      "[*] Epoch: 0447 train_loss= 1.30045 train_rmse= 0.92904 val_loss= 1.46649 val_rmse= 1.10340 \t\ttime= 0.61735\n",
      "[*] Epoch: 0448 train_loss= 1.31119 train_rmse= 0.95192 val_loss= 1.46710 val_rmse= 1.10384 \t\ttime= 0.53657\n",
      "[*] Epoch: 0449 train_loss= 1.29896 train_rmse= 0.93734 val_loss= 1.47046 val_rmse= 1.10707 \t\ttime= 0.61236\n",
      "[*] Epoch: 0450 train_loss= 1.29781 train_rmse= 0.93080 val_loss= 1.46703 val_rmse= 1.10359 \t\ttime= 0.62931\n",
      "[*] Epoch: 0451 train_loss= 1.30667 train_rmse= 0.93530 val_loss= 1.46017 val_rmse= 1.09625 \t\ttime= 0.64626\n",
      "[*] Epoch: 0452 train_loss= 1.29807 train_rmse= 0.94015 val_loss= 1.45625 val_rmse= 1.09170 \t\ttime= 0.62533\n",
      "[*] Epoch: 0453 train_loss= 1.29976 train_rmse= 0.93662 val_loss= 1.46454 val_rmse= 1.09940 \t\ttime= 0.63430\n",
      "[*] Epoch: 0454 train_loss= 1.29495 train_rmse= 0.92888 val_loss= 1.46928 val_rmse= 1.10387 \t\ttime= 0.62632\n",
      "[*] Epoch: 0455 train_loss= 1.29293 train_rmse= 0.92582 val_loss= 1.46858 val_rmse= 1.10350 \t\ttime= 0.61536\n",
      "[*] Epoch: 0456 train_loss= 1.29925 train_rmse= 0.92628 val_loss= 1.46468 val_rmse= 1.10023 \t\ttime= 0.61834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0457 train_loss= 1.30334 train_rmse= 0.94018 val_loss= 1.46126 val_rmse= 1.09730 \t\ttime= 0.70910\n",
      "[*] Epoch: 0458 train_loss= 1.30265 train_rmse= 0.93608 val_loss= 1.46430 val_rmse= 1.10110 \t\ttime= 0.63131\n",
      "[*] Epoch: 0459 train_loss= 1.29574 train_rmse= 0.94034 val_loss= 1.47428 val_rmse= 1.11127 \t\ttime= 0.59142\n",
      "[*] Epoch: 0460 train_loss= 1.30231 train_rmse= 0.93126 val_loss= 1.47642 val_rmse= 1.11373 \t\ttime= 0.61436\n",
      "[*] Epoch: 0461 train_loss= 1.30500 train_rmse= 0.92952 val_loss= 1.47096 val_rmse= 1.10863 \t\ttime= 0.54355\n",
      "[*] Epoch: 0462 train_loss= 1.29393 train_rmse= 0.92382 val_loss= 1.46306 val_rmse= 1.10053 \t\ttime= 0.61336\n",
      "[*] Epoch: 0463 train_loss= 1.29975 train_rmse= 0.93627 val_loss= 1.45777 val_rmse= 1.09481 \t\ttime= 0.64029\n",
      "[*] Epoch: 0464 train_loss= 1.31246 train_rmse= 0.94323 val_loss= 1.46136 val_rmse= 1.09783 \t\ttime= 0.59242\n",
      "[*] Epoch: 0465 train_loss= 1.29272 train_rmse= 0.93077 val_loss= 1.46551 val_rmse= 1.10156 \t\ttime= 0.60538\n",
      "[*] Epoch: 0466 train_loss= 1.29370 train_rmse= 0.93252 val_loss= 1.46970 val_rmse= 1.10539 \t\ttime= 0.63430\n",
      "[*] Epoch: 0467 train_loss= 1.30913 train_rmse= 0.93217 val_loss= 1.46713 val_rmse= 1.10328 \t\ttime= 0.60338\n",
      "[*] Epoch: 0468 train_loss= 1.30062 train_rmse= 0.93646 val_loss= 1.46490 val_rmse= 1.10186 \t\ttime= 0.59042\n",
      "[*] Epoch: 0469 train_loss= 1.29421 train_rmse= 0.93289 val_loss= 1.46399 val_rmse= 1.10172 \t\ttime= 0.58344\n",
      "[*] Epoch: 0470 train_loss= 1.30403 train_rmse= 0.93384 val_loss= 1.46285 val_rmse= 1.10118 \t\ttime= 0.65724\n",
      "[*] Epoch: 0471 train_loss= 1.29428 train_rmse= 0.93583 val_loss= 1.46610 val_rmse= 1.10448 \t\ttime= 0.57247\n",
      "[*] Epoch: 0472 train_loss= 1.29296 train_rmse= 0.93382 val_loss= 1.46786 val_rmse= 1.10599 \t\ttime= 0.58942\n",
      "[*] Epoch: 0473 train_loss= 1.29503 train_rmse= 0.93819 val_loss= 1.46535 val_rmse= 1.10310 \t\ttime= 0.60937\n",
      "[*] Epoch: 0474 train_loss= 1.29619 train_rmse= 0.93222 val_loss= 1.45883 val_rmse= 1.09620 \t\ttime= 0.52958\n",
      "[*] Epoch: 0475 train_loss= 1.29340 train_rmse= 0.93467 val_loss= 1.45570 val_rmse= 1.09270 \t\ttime= 0.62633\n",
      "[*] Epoch: 0476 train_loss= 1.29067 train_rmse= 0.93112 val_loss= 1.45778 val_rmse= 1.09445 \t\ttime= 0.63331\n",
      "[*] Epoch: 0477 train_loss= 1.29524 train_rmse= 0.92957 val_loss= 1.46054 val_rmse= 1.09671 \t\ttime= 0.63829\n",
      "[*] Epoch: 0478 train_loss= 1.28973 train_rmse= 0.92657 val_loss= 1.45988 val_rmse= 1.09563 \t\ttime= 0.61236\n",
      "[*] Epoch: 0479 train_loss= 1.29536 train_rmse= 0.93080 val_loss= 1.45815 val_rmse= 1.09361 \t\ttime= 0.60038\n",
      "[*] Epoch: 0480 train_loss= 1.28460 train_rmse= 0.92465 val_loss= 1.45709 val_rmse= 1.09240 \t\ttime= 0.59142\n",
      "[*] Epoch: 0481 train_loss= 1.28570 train_rmse= 0.92901 val_loss= 1.45923 val_rmse= 1.09469 \t\ttime= 0.56050\n",
      "[*] Epoch: 0482 train_loss= 1.28979 train_rmse= 0.92720 val_loss= 1.46030 val_rmse= 1.09619 \t\ttime= 0.59441\n",
      "[*] Epoch: 0483 train_loss= 1.28967 train_rmse= 0.92211 val_loss= 1.45939 val_rmse= 1.09604 \t\ttime= 0.71209\n",
      "[*] Epoch: 0484 train_loss= 1.28310 train_rmse= 0.92611 val_loss= 1.45982 val_rmse= 1.09703 \t\ttime= 0.55252\n",
      "[*] Epoch: 0485 train_loss= 1.29110 train_rmse= 0.93682 val_loss= 1.46152 val_rmse= 1.09897 \t\ttime= 0.61237\n",
      "[*] Epoch: 0486 train_loss= 1.28966 train_rmse= 0.93467 val_loss= 1.46646 val_rmse= 1.10397 \t\ttime= 0.60139\n",
      "[*] Epoch: 0487 train_loss= 1.28023 train_rmse= 0.92350 val_loss= 1.47236 val_rmse= 1.10992 \t\ttime= 0.55950\n",
      "[*] Epoch: 0488 train_loss= 1.29620 train_rmse= 0.93209 val_loss= 1.47453 val_rmse= 1.11215 \t\ttime= 0.59142\n",
      "[*] Epoch: 0489 train_loss= 1.28503 train_rmse= 0.91993 val_loss= 1.47143 val_rmse= 1.10889 \t\ttime= 0.61835\n",
      "[*] Epoch: 0490 train_loss= 1.28808 train_rmse= 0.92173 val_loss= 1.46526 val_rmse= 1.10271 \t\ttime= 0.62932\n",
      "[*] Epoch: 0491 train_loss= 1.29019 train_rmse= 0.92497 val_loss= 1.46181 val_rmse= 1.09944 \t\ttime= 0.60239\n",
      "[*] Epoch: 0492 train_loss= 1.29263 train_rmse= 0.92981 val_loss= 1.46450 val_rmse= 1.10252 \t\ttime= 0.59341\n",
      "[*] Epoch: 0493 train_loss= 1.28991 train_rmse= 0.93038 val_loss= 1.46404 val_rmse= 1.10216 \t\ttime= 0.57247\n",
      "[*] Epoch: 0494 train_loss= 1.29180 train_rmse= 0.93786 val_loss= 1.46200 val_rmse= 1.09994 \t\ttime= 0.51961\n",
      "[*] Epoch: 0495 train_loss= 1.29120 train_rmse= 0.93859 val_loss= 1.46034 val_rmse= 1.09794 \t\ttime= 0.62134\n",
      "[*] Epoch: 0496 train_loss= 1.27945 train_rmse= 0.92111 val_loss= 1.46195 val_rmse= 1.09928 \t\ttime= 0.63231\n",
      "[*] Epoch: 0497 train_loss= 1.28805 train_rmse= 0.92470 val_loss= 1.46378 val_rmse= 1.10093 \t\ttime= 0.60438\n",
      "[*] Epoch: 0498 train_loss= 1.28751 train_rmse= 0.92816 val_loss= 1.46954 val_rmse= 1.10638 \t\ttime= 0.58244\n",
      "[*] Epoch: 0499 train_loss= 1.29154 train_rmse= 0.92915 val_loss= 1.47124 val_rmse= 1.10795 \t\ttime= 0.58843\n",
      "[*] Epoch: 0500 train_loss= 1.28573 train_rmse= 0.92533 val_loss= 1.46928 val_rmse= 1.10621 \t\ttime= 0.60738\n",
      "[*] Epoch: 0501 train_loss= 1.29696 train_rmse= 0.93103 val_loss= 1.46502 val_rmse= 1.10209 \t\ttime= 0.54155\n",
      "[*] Epoch: 0502 train_loss= 1.29079 train_rmse= 0.93702 val_loss= 1.47054 val_rmse= 1.10809 \t\ttime= 0.58344\n",
      "[*] Epoch: 0503 train_loss= 1.28792 train_rmse= 0.92940 val_loss= 1.47599 val_rmse= 1.11402 \t\ttime= 0.64029\n",
      "[*] Epoch: 0504 train_loss= 1.29299 train_rmse= 0.92565 val_loss= 1.47443 val_rmse= 1.11295 \t\ttime= 0.57645\n",
      "[*] Epoch: 0505 train_loss= 1.28401 train_rmse= 0.91491 val_loss= 1.46871 val_rmse= 1.10733 \t\ttime= 0.58643\n",
      "[*] Epoch: 0506 train_loss= 1.27195 train_rmse= 0.91181 val_loss= 1.46520 val_rmse= 1.10379 \t\ttime= 0.58643\n",
      "[*] Epoch: 0507 train_loss= 1.28268 train_rmse= 0.92045 val_loss= 1.46256 val_rmse= 1.10124 \t\ttime= 0.55053\n",
      "[*] Epoch: 0508 train_loss= 1.29012 train_rmse= 0.93609 val_loss= 1.46583 val_rmse= 1.10507 \t\ttime= 0.60638\n",
      "[*] Epoch: 0509 train_loss= 1.28037 train_rmse= 0.93333 val_loss= 1.47186 val_rmse= 1.11114 \t\ttime= 0.58643\n",
      "[*] Epoch: 0510 train_loss= 1.28286 train_rmse= 0.92530 val_loss= 1.47423 val_rmse= 1.11316 \t\ttime= 0.65625\n",
      "[*] Epoch: 0511 train_loss= 1.27587 train_rmse= 0.92052 val_loss= 1.47408 val_rmse= 1.11265 \t\ttime= 0.56550\n",
      "[*] Epoch: 0512 train_loss= 1.28520 train_rmse= 0.92192 val_loss= 1.46901 val_rmse= 1.10725 \t\ttime= 0.60339\n",
      "[*] Epoch: 0513 train_loss= 1.27889 train_rmse= 0.91866 val_loss= 1.46331 val_rmse= 1.10141 \t\ttime= 0.58045\n",
      "[*] Epoch: 0514 train_loss= 1.28818 train_rmse= 0.93136 val_loss= 1.46000 val_rmse= 1.09818 \t\ttime= 0.51960\n",
      "[*] Epoch: 0515 train_loss= 1.28050 train_rmse= 0.92269 val_loss= 1.45919 val_rmse= 1.09733 \t\ttime= 0.64627\n",
      "[*] Epoch: 0516 train_loss= 1.29313 train_rmse= 0.93810 val_loss= 1.46039 val_rmse= 1.09800 \t\ttime= 0.63730\n",
      "[*] Epoch: 0517 train_loss= 1.27514 train_rmse= 0.92518 val_loss= 1.46507 val_rmse= 1.10230 \t\ttime= 0.60637\n",
      "[*] Epoch: 0518 train_loss= 1.28266 train_rmse= 0.92888 val_loss= 1.46766 val_rmse= 1.10482 \t\ttime= 0.60239\n",
      "[*] Epoch: 0519 train_loss= 1.28391 train_rmse= 0.92132 val_loss= 1.46677 val_rmse= 1.10408 \t\ttime= 0.60438\n",
      "[*] Epoch: 0520 train_loss= 1.28192 train_rmse= 0.92116 val_loss= 1.46379 val_rmse= 1.10124 \t\ttime= 0.59441\n",
      "[*] Epoch: 0521 train_loss= 1.28715 train_rmse= 0.92181 val_loss= 1.46290 val_rmse= 1.10100 \t\ttime= 0.53956\n",
      "[*] Epoch: 0522 train_loss= 1.28055 train_rmse= 0.91995 val_loss= 1.46597 val_rmse= 1.10456 \t\ttime= 0.59641\n",
      "[*] Epoch: 0523 train_loss= 1.28183 train_rmse= 0.92155 val_loss= 1.47093 val_rmse= 1.10974 \t\ttime= 0.67819\n",
      "[*] Epoch: 0524 train_loss= 1.27194 train_rmse= 0.91302 val_loss= 1.47441 val_rmse= 1.11317 \t\ttime= 0.56748\n",
      "[*] Epoch: 0525 train_loss= 1.27968 train_rmse= 0.92214 val_loss= 1.47470 val_rmse= 1.11370 \t\ttime= 0.60838\n",
      "[*] Epoch: 0526 train_loss= 1.27555 train_rmse= 0.92257 val_loss= 1.47161 val_rmse= 1.11072 \t\ttime= 0.60538\n",
      "[*] Epoch: 0527 train_loss= 1.27325 train_rmse= 0.92565 val_loss= 1.46807 val_rmse= 1.10711 \t\ttime= 0.54554\n",
      "[*] Epoch: 0528 train_loss= 1.27630 train_rmse= 0.92083 val_loss= 1.46458 val_rmse= 1.10327 \t\ttime= 0.59242\n",
      "[*] Epoch: 0529 train_loss= 1.28194 train_rmse= 0.91923 val_loss= 1.45897 val_rmse= 1.09756 \t\ttime= 0.61935\n",
      "[*] Epoch: 0530 train_loss= 1.28724 train_rmse= 0.92868 val_loss= 1.46134 val_rmse= 1.10036 \t\ttime= 0.65226\n",
      "[*] Epoch: 0531 train_loss= 1.26639 train_rmse= 0.90885 val_loss= 1.46317 val_rmse= 1.10214 \t\ttime= 0.58743\n",
      "[*] Epoch: 0532 train_loss= 1.27267 train_rmse= 0.91702 val_loss= 1.46125 val_rmse= 1.10016 \t\ttime= 0.59840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0533 train_loss= 1.28157 train_rmse= 0.93163 val_loss= 1.46238 val_rmse= 1.10092 \t\ttime= 0.60937\n",
      "[*] Epoch: 0534 train_loss= 1.28083 train_rmse= 0.92549 val_loss= 1.47156 val_rmse= 1.11046 \t\ttime= 0.54853\n",
      "[*] Epoch: 0535 train_loss= 1.28232 train_rmse= 0.92046 val_loss= 1.47823 val_rmse= 1.11789 \t\ttime= 0.59142\n",
      "[*] Epoch: 0536 train_loss= 1.27981 train_rmse= 0.91572 val_loss= 1.47838 val_rmse= 1.11872 \t\ttime= 0.64827\n",
      "[*] Epoch: 0537 train_loss= 1.28310 train_rmse= 0.91488 val_loss= 1.47153 val_rmse= 1.11205 \t\ttime= 0.59142\n",
      "[*] Epoch: 0538 train_loss= 1.27641 train_rmse= 0.91659 val_loss= 1.46524 val_rmse= 1.10590 \t\ttime= 0.61037\n",
      "[*] Epoch: 0539 train_loss= 1.27669 train_rmse= 0.92496 val_loss= 1.46334 val_rmse= 1.10419 \t\ttime= 0.60438\n",
      "[*] Epoch: 0540 train_loss= 1.27344 train_rmse= 0.92094 val_loss= 1.46649 val_rmse= 1.10748 \t\ttime= 0.58543\n",
      "[*] Epoch: 0541 train_loss= 1.27531 train_rmse= 0.92425 val_loss= 1.47043 val_rmse= 1.11125 \t\ttime= 0.58246\n",
      "[*] Epoch: 0542 train_loss= 1.27276 train_rmse= 0.91902 val_loss= 1.47293 val_rmse= 1.11304 \t\ttime= 0.59541\n",
      "[*] Epoch: 0543 train_loss= 1.27402 train_rmse= 0.92718 val_loss= 1.47331 val_rmse= 1.11265 \t\ttime= 0.65824\n",
      "[*] Epoch: 0544 train_loss= 1.27203 train_rmse= 0.91543 val_loss= 1.47071 val_rmse= 1.10968 \t\ttime= 0.53956\n",
      "[*] Epoch: 0545 train_loss= 1.27874 train_rmse= 0.92142 val_loss= 1.46622 val_rmse= 1.10509 \t\ttime= 0.63530\n",
      "[*] Epoch: 0546 train_loss= 1.28154 train_rmse= 0.92073 val_loss= 1.46723 val_rmse= 1.10682 \t\ttime= 0.59740\n",
      "[*] Epoch: 0547 train_loss= 1.27598 train_rmse= 0.91332 val_loss= 1.46726 val_rmse= 1.10730 \t\ttime= 0.54953\n",
      "[*] Epoch: 0548 train_loss= 1.27398 train_rmse= 0.91731 val_loss= 1.46744 val_rmse= 1.10764 \t\ttime= 0.59740\n",
      "[*] Epoch: 0549 train_loss= 1.27637 train_rmse= 0.92049 val_loss= 1.46992 val_rmse= 1.10991 \t\ttime= 0.63730\n",
      "[*] Epoch: 0550 train_loss= 1.27499 train_rmse= 0.92159 val_loss= 1.46889 val_rmse= 1.10857 \t\ttime= 0.65125\n",
      "[*] Epoch: 0551 train_loss= 1.27631 train_rmse= 0.93310 val_loss= 1.47223 val_rmse= 1.11211 \t\ttime= 0.61137\n",
      "[*] Epoch: 0552 train_loss= 1.27279 train_rmse= 0.91945 val_loss= 1.47059 val_rmse= 1.11053 \t\ttime= 0.59341\n",
      "[*] Epoch: 0553 train_loss= 1.27204 train_rmse= 0.91637 val_loss= 1.46996 val_rmse= 1.11068 \t\ttime= 0.59441\n",
      "[*] Epoch: 0554 train_loss= 1.26799 train_rmse= 0.91912 val_loss= 1.47089 val_rmse= 1.11247 \t\ttime= 0.56248\n",
      "[*] Epoch: 0555 train_loss= 1.27080 train_rmse= 0.90886 val_loss= 1.47148 val_rmse= 1.11341 \t\ttime= 0.62234\n",
      "[*] Epoch: 0556 train_loss= 1.26503 train_rmse= 0.91354 val_loss= 1.47068 val_rmse= 1.11260 \t\ttime= 0.65824\n",
      "[*] Epoch: 0557 train_loss= 1.26857 train_rmse= 0.91171 val_loss= 1.46872 val_rmse= 1.11032 \t\ttime= 0.57547\n",
      "[*] Epoch: 0558 train_loss= 1.27098 train_rmse= 0.92379 val_loss= 1.46994 val_rmse= 1.11163 \t\ttime= 0.61735\n",
      "[*] Epoch: 0559 train_loss= 1.27191 train_rmse= 0.91613 val_loss= 1.46894 val_rmse= 1.11060 \t\ttime= 0.62932\n",
      "[*] Epoch: 0560 train_loss= 1.26694 train_rmse= 0.91654 val_loss= 1.46943 val_rmse= 1.11094 \t\ttime= 0.53557\n",
      "[*] Epoch: 0561 train_loss= 1.26562 train_rmse= 0.91380 val_loss= 1.46642 val_rmse= 1.10750 \t\ttime= 0.58643\n",
      "[*] Epoch: 0562 train_loss= 1.26428 train_rmse= 0.91548 val_loss= 1.46655 val_rmse= 1.10710 \t\ttime= 0.62932\n",
      "[*] Epoch: 0563 train_loss= 1.26409 train_rmse= 0.91408 val_loss= 1.46794 val_rmse= 1.10802 \t\ttime= 0.65924\n",
      "[*] Epoch: 0564 train_loss= 1.26824 train_rmse= 0.91650 val_loss= 1.46725 val_rmse= 1.10696 \t\ttime= 0.59940\n",
      "[*] Epoch: 0565 train_loss= 1.26749 train_rmse= 0.91721 val_loss= 1.46619 val_rmse= 1.10639 \t\ttime= 0.59640\n",
      "[*] Epoch: 0566 train_loss= 1.26411 train_rmse= 0.91916 val_loss= 1.46616 val_rmse= 1.10726 \t\ttime= 0.59940\n",
      "[*] Epoch: 0567 train_loss= 1.26735 train_rmse= 0.91713 val_loss= 1.46594 val_rmse= 1.10793 \t\ttime= 0.57148\n",
      "[*] Epoch: 0568 train_loss= 1.26276 train_rmse= 0.90968 val_loss= 1.46666 val_rmse= 1.10885 \t\ttime= 0.61436\n",
      "[*] Epoch: 0569 train_loss= 1.26481 train_rmse= 0.91541 val_loss= 1.46646 val_rmse= 1.10831 \t\ttime= 0.64926\n",
      "[*] Epoch: 0570 train_loss= 1.26816 train_rmse= 0.91579 val_loss= 1.46627 val_rmse= 1.10727 \t\ttime= 0.59242\n",
      "[*] Epoch: 0571 train_loss= 1.26130 train_rmse= 0.91001 val_loss= 1.46647 val_rmse= 1.10650 \t\ttime= 0.62633\n",
      "[*] Epoch: 0572 train_loss= 1.26288 train_rmse= 0.90589 val_loss= 1.46962 val_rmse= 1.10926 \t\ttime= 0.61436\n",
      "[*] Epoch: 0573 train_loss= 1.26301 train_rmse= 0.91060 val_loss= 1.47272 val_rmse= 1.11241 \t\ttime= 0.55851\n",
      "[*] Epoch: 0574 train_loss= 1.27172 train_rmse= 0.91133 val_loss= 1.46990 val_rmse= 1.10963 \t\ttime= 0.58942\n",
      "[*] Epoch: 0575 train_loss= 1.26907 train_rmse= 0.90912 val_loss= 1.46299 val_rmse= 1.10320 \t\ttime= 0.60239\n",
      "[*] Epoch: 0576 train_loss= 1.27061 train_rmse= 0.92551 val_loss= 1.46031 val_rmse= 1.10154 \t\ttime= 0.64727\n",
      "[*] Epoch: 0577 train_loss= 1.27154 train_rmse= 0.92397 val_loss= 1.45837 val_rmse= 1.10040 \t\ttime= 0.57746\n",
      "[*] Epoch: 0578 train_loss= 1.26282 train_rmse= 0.91472 val_loss= 1.46118 val_rmse= 1.10388 \t\ttime= 0.59740\n",
      "[*] Epoch: 0579 train_loss= 1.26035 train_rmse= 0.91198 val_loss= 1.46499 val_rmse= 1.10753 \t\ttime= 0.61037\n",
      "[*] Epoch: 0580 train_loss= 1.25753 train_rmse= 0.91061 val_loss= 1.46668 val_rmse= 1.10861 \t\ttime= 0.56349\n",
      "[*] Epoch: 0581 train_loss= 1.26528 train_rmse= 0.91520 val_loss= 1.46871 val_rmse= 1.11009 \t\ttime= 0.58843\n",
      "[*] Epoch: 0582 train_loss= 1.25318 train_rmse= 0.90476 val_loss= 1.46893 val_rmse= 1.11022 \t\ttime= 0.66223\n",
      "[*] Epoch: 0583 train_loss= 1.26148 train_rmse= 0.90826 val_loss= 1.47064 val_rmse= 1.11249 \t\ttime= 0.60039\n",
      "[*] Epoch: 0584 train_loss= 1.26234 train_rmse= 0.90900 val_loss= 1.46924 val_rmse= 1.11164 \t\ttime= 0.59940\n",
      "[*] Epoch: 0585 train_loss= 1.26142 train_rmse= 0.91075 val_loss= 1.46454 val_rmse= 1.10713 \t\ttime= 0.60738\n",
      "[*] Epoch: 0586 train_loss= 1.25849 train_rmse= 0.90802 val_loss= 1.46055 val_rmse= 1.10317 \t\ttime= 0.58543\n",
      "[*] Epoch: 0587 train_loss= 1.25647 train_rmse= 0.91075 val_loss= 1.45968 val_rmse= 1.10226 \t\ttime= 0.59640\n",
      "[*] Epoch: 0588 train_loss= 1.26590 train_rmse= 0.92632 val_loss= 1.46307 val_rmse= 1.10569 \t\ttime= 0.61236\n",
      "[*] Epoch: 0589 train_loss= 1.25896 train_rmse= 0.91085 val_loss= 1.46970 val_rmse= 1.11252 \t\ttime= 0.66223\n",
      "[*] Epoch: 0590 train_loss= 1.25983 train_rmse= 0.91079 val_loss= 1.47513 val_rmse= 1.11795 \t\ttime= 0.55153\n",
      "[*] Epoch: 0591 train_loss= 1.26432 train_rmse= 0.91113 val_loss= 1.47376 val_rmse= 1.11627 \t\ttime= 0.60239\n",
      "[*] Epoch: 0592 train_loss= 1.26279 train_rmse= 0.91503 val_loss= 1.47020 val_rmse= 1.11232 \t\ttime= 0.61535\n",
      "[*] Epoch: 0593 train_loss= 1.25881 train_rmse= 0.90949 val_loss= 1.46728 val_rmse= 1.10910 \t\ttime= 0.53058\n",
      "[*] Epoch: 0594 train_loss= 1.27865 train_rmse= 0.91954 val_loss= 1.46099 val_rmse= 1.10279 \t\ttime= 0.61236\n",
      "[*] Epoch: 0595 train_loss= 1.26585 train_rmse= 0.91656 val_loss= 1.45728 val_rmse= 1.09978 \t\ttime= 0.62534\n",
      "[*] Epoch: 0596 train_loss= 1.26178 train_rmse= 0.90862 val_loss= 1.46022 val_rmse= 1.10355 \t\ttime= 0.65126\n",
      "[*] Epoch: 0597 train_loss= 1.26068 train_rmse= 0.90663 val_loss= 1.46377 val_rmse= 1.10722 \t\ttime= 0.59740\n",
      "[*] Epoch: 0598 train_loss= 1.25399 train_rmse= 0.90951 val_loss= 1.46614 val_rmse= 1.10927 \t\ttime= 0.58143\n",
      "[*] Epoch: 0599 train_loss= 1.25624 train_rmse= 0.90897 val_loss= 1.46708 val_rmse= 1.10949 \t\ttime= 0.61834\n",
      "[*] Epoch: 0600 train_loss= 1.27444 train_rmse= 0.92485 val_loss= 1.46530 val_rmse= 1.10734 \t\ttime= 0.53457\n",
      "[*] Epoch: 0601 train_loss= 1.24585 train_rmse= 0.90424 val_loss= 1.46563 val_rmse= 1.10805 \t\ttime= 0.59242\n",
      "[*] Epoch: 0602 train_loss= 1.25995 train_rmse= 0.91186 val_loss= 1.46553 val_rmse= 1.10847 \t\ttime= 0.65325\n",
      "[*] Epoch: 0603 train_loss= 1.25816 train_rmse= 0.90957 val_loss= 1.46723 val_rmse= 1.11061 \t\ttime= 0.57945\n",
      "[*] Epoch: 0604 train_loss= 1.25313 train_rmse= 0.89845 val_loss= 1.47014 val_rmse= 1.11375 \t\ttime= 0.64926\n",
      "[*] Epoch: 0605 train_loss= 1.25752 train_rmse= 0.90665 val_loss= 1.47019 val_rmse= 1.11400 \t\ttime= 0.62732\n",
      "[*] Epoch: 0606 train_loss= 1.26320 train_rmse= 0.90791 val_loss= 1.46794 val_rmse= 1.11151 \t\ttime= 0.56050\n",
      "[*] Epoch: 0607 train_loss= 1.26660 train_rmse= 0.92299 val_loss= 1.46483 val_rmse= 1.10838 \t\ttime= 0.60039\n",
      "[*] Epoch: 0608 train_loss= 1.25418 train_rmse= 0.91273 val_loss= 1.46785 val_rmse= 1.11191 \t\ttime= 0.62333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0609 train_loss= 1.25326 train_rmse= 0.91244 val_loss= 1.47210 val_rmse= 1.11693 \t\ttime= 0.65226\n",
      "[*] Epoch: 0610 train_loss= 1.24464 train_rmse= 0.89689 val_loss= 1.47255 val_rmse= 1.11749 \t\ttime= 0.59441\n",
      "[*] Epoch: 0611 train_loss= 1.25756 train_rmse= 0.90777 val_loss= 1.46702 val_rmse= 1.11182 \t\ttime= 0.57745\n",
      "[*] Epoch: 0612 train_loss= 1.24826 train_rmse= 0.89821 val_loss= 1.46339 val_rmse= 1.10844 \t\ttime= 0.62732\n",
      "[*] Epoch: 0613 train_loss= 1.25383 train_rmse= 0.90919 val_loss= 1.46083 val_rmse= 1.10638 \t\ttime= 0.54354\n",
      "[*] Epoch: 0614 train_loss= 1.25531 train_rmse= 0.91413 val_loss= 1.45899 val_rmse= 1.10528 \t\ttime= 0.59541\n",
      "[*] Epoch: 0615 train_loss= 1.25028 train_rmse= 0.90863 val_loss= 1.46057 val_rmse= 1.10741 \t\ttime= 0.62633\n",
      "[*] Epoch: 0616 train_loss= 1.25827 train_rmse= 0.91826 val_loss= 1.46342 val_rmse= 1.11049 \t\ttime= 0.60638\n",
      "[*] Epoch: 0617 train_loss= 1.25212 train_rmse= 0.90871 val_loss= 1.46395 val_rmse= 1.11051 \t\ttime= 0.62333\n",
      "[*] Epoch: 0618 train_loss= 1.25301 train_rmse= 0.90614 val_loss= 1.46505 val_rmse= 1.11128 \t\ttime= 0.60439\n",
      "[*] Epoch: 0619 train_loss= 1.26092 train_rmse= 0.91180 val_loss= 1.45932 val_rmse= 1.10483 \t\ttime= 0.58643\n",
      "[*] Epoch: 0620 train_loss= 1.25001 train_rmse= 0.90383 val_loss= 1.45557 val_rmse= 1.10131 \t\ttime= 0.57546\n",
      "[*] Epoch: 0621 train_loss= 1.24890 train_rmse= 0.90625 val_loss= 1.45973 val_rmse= 1.10717 \t\ttime= 0.58344\n",
      "[*] Epoch: 0622 train_loss= 1.24613 train_rmse= 0.90922 val_loss= 1.46251 val_rmse= 1.11129 \t\ttime= 0.67719\n",
      "[*] Epoch: 0623 train_loss= 1.24828 train_rmse= 0.90520 val_loss= 1.46195 val_rmse= 1.11121 \t\ttime= 0.54953\n",
      "[*] Epoch: 0624 train_loss= 1.25024 train_rmse= 0.90707 val_loss= 1.45808 val_rmse= 1.10672 \t\ttime= 0.59740\n",
      "[*] Epoch: 0625 train_loss= 1.24703 train_rmse= 0.90813 val_loss= 1.45568 val_rmse= 1.10337 \t\ttime= 0.60638\n",
      "[*] Epoch: 0626 train_loss= 1.25319 train_rmse= 0.91424 val_loss= 1.46163 val_rmse= 1.10941 \t\ttime= 0.53556\n",
      "[*] Epoch: 0627 train_loss= 1.24323 train_rmse= 0.90546 val_loss= 1.46866 val_rmse= 1.11666 \t\ttime= 0.62134\n",
      "[*] Epoch: 0628 train_loss= 1.24893 train_rmse= 0.90278 val_loss= 1.46913 val_rmse= 1.11735 \t\ttime= 0.61336\n",
      "[*] Epoch: 0629 train_loss= 1.25140 train_rmse= 0.90761 val_loss= 1.46309 val_rmse= 1.11133 \t\ttime= 0.63928\n",
      "[*] Epoch: 0630 train_loss= 1.24362 train_rmse= 0.90047 val_loss= 1.45505 val_rmse= 1.10340 \t\ttime= 0.57746\n",
      "[*] Epoch: 0631 train_loss= 1.24558 train_rmse= 0.90450 val_loss= 1.45061 val_rmse= 1.09922 \t\ttime= 0.57247\n",
      "[*] Epoch: 0632 train_loss= 1.24348 train_rmse= 0.90838 val_loss= 1.45114 val_rmse= 1.10064 \t\ttime= 0.62233\n",
      "[*] Epoch: 0633 train_loss= 1.25300 train_rmse= 0.91273 val_loss= 1.46156 val_rmse= 1.11337 \t\ttime= 0.57247\n",
      "[*] Epoch: 0634 train_loss= 1.23743 train_rmse= 0.90172 val_loss= 1.47391 val_rmse= 1.12759 \t\ttime= 0.61535\n",
      "[*] Epoch: 0635 train_loss= 1.25702 train_rmse= 0.91329 val_loss= 1.47891 val_rmse= 1.13285 \t\ttime= 0.67220\n",
      "[*] Epoch: 0636 train_loss= 1.26411 train_rmse= 0.91604 val_loss= 1.47548 val_rmse= 1.12734 \t\ttime= 0.56349\n",
      "[*] Epoch: 0637 train_loss= 1.24239 train_rmse= 0.90535 val_loss= 1.46949 val_rmse= 1.11915 \t\ttime= 0.60338\n",
      "[*] Epoch: 0638 train_loss= 1.24267 train_rmse= 0.90367 val_loss= 1.46496 val_rmse= 1.11444 \t\ttime= 0.59840\n",
      "[*] Epoch: 0639 train_loss= 1.25090 train_rmse= 0.91236 val_loss= 1.46011 val_rmse= 1.11121 \t\ttime= 0.56748\n",
      "[*] Epoch: 0640 train_loss= 1.24281 train_rmse= 0.90044 val_loss= 1.46339 val_rmse= 1.11711 \t\ttime= 0.58244\n",
      "[*] Epoch: 0641 train_loss= 1.24517 train_rmse= 0.90528 val_loss= 1.46415 val_rmse= 1.11896 \t\ttime= 0.58843\n",
      "[*] Epoch: 0642 train_loss= 1.25166 train_rmse= 0.90766 val_loss= 1.46040 val_rmse= 1.11429 \t\ttime= 0.67220\n",
      "[*] Epoch: 0643 train_loss= 1.26118 train_rmse= 0.92568 val_loss= 1.45724 val_rmse= 1.10960 \t\ttime= 0.55751\n",
      "[*] Epoch: 0644 train_loss= 1.24301 train_rmse= 0.91063 val_loss= 1.45728 val_rmse= 1.10890 \t\ttime= 0.63331\n",
      "[*] Epoch: 0645 train_loss= 1.23403 train_rmse= 0.90534 val_loss= 1.45812 val_rmse= 1.11005 \t\ttime= 0.61336\n",
      "[*] Epoch: 0646 train_loss= 1.24242 train_rmse= 0.90821 val_loss= 1.46432 val_rmse= 1.11826 \t\ttime= 0.54754\n",
      "[*] Epoch: 0647 train_loss= 1.24044 train_rmse= 0.90522 val_loss= 1.46578 val_rmse= 1.12141 \t\ttime= 0.59940\n",
      "[*] Epoch: 0648 train_loss= 1.24141 train_rmse= 0.89965 val_loss= 1.46351 val_rmse= 1.11944 \t\ttime= 0.61137\n",
      "[*] Epoch: 0649 train_loss= 1.23989 train_rmse= 0.90161 val_loss= 1.45773 val_rmse= 1.11334 \t\ttime= 0.61236\n",
      "[*] Epoch: 0650 train_loss= 1.23809 train_rmse= 0.90628 val_loss= 1.45469 val_rmse= 1.11086 \t\ttime= 0.61835\n",
      "[*] Epoch: 0651 train_loss= 1.23935 train_rmse= 0.90575 val_loss= 1.45183 val_rmse= 1.10882 \t\ttime= 0.59341\n",
      "[*] Epoch: 0652 train_loss= 1.25720 train_rmse= 0.92465 val_loss= 1.45606 val_rmse= 1.11358 \t\ttime= 0.59640\n",
      "[*] Epoch: 0653 train_loss= 1.23996 train_rmse= 0.90337 val_loss= 1.46228 val_rmse= 1.11921 \t\ttime= 0.55253\n",
      "[*] Epoch: 0654 train_loss= 1.23138 train_rmse= 0.90101 val_loss= 1.46224 val_rmse= 1.11760 \t\ttime= 0.63530\n",
      "[*] Epoch: 0655 train_loss= 1.23719 train_rmse= 0.90762 val_loss= 1.45724 val_rmse= 1.11069 \t\ttime= 0.67918\n",
      "[*] Epoch: 0656 train_loss= 1.24804 train_rmse= 0.91035 val_loss= 1.44997 val_rmse= 1.10280 \t\ttime= 0.54056\n",
      "[*] Epoch: 0657 train_loss= 1.24144 train_rmse= 0.90939 val_loss= 1.44875 val_rmse= 1.10347 \t\ttime= 0.60538\n",
      "[*] Epoch: 0658 train_loss= 1.22608 train_rmse= 0.89913 val_loss= 1.44655 val_rmse= 1.10287 \t\ttime= 0.61635\n",
      "[*] Epoch: 0659 train_loss= 1.23953 train_rmse= 0.90421 val_loss= 1.44767 val_rmse= 1.10581 \t\ttime= 0.54953\n",
      "[*] Epoch: 0660 train_loss= 1.23166 train_rmse= 0.90133 val_loss= 1.44692 val_rmse= 1.10673 \t\ttime= 0.60937\n",
      "[*] Epoch: 0661 train_loss= 1.23397 train_rmse= 0.90461 val_loss= 1.44734 val_rmse= 1.10787 \t\ttime= 0.61236\n",
      "[*] Epoch: 0662 train_loss= 1.22393 train_rmse= 0.89736 val_loss= 1.45121 val_rmse= 1.11198 \t\ttime= 0.64029\n",
      "[*] Epoch: 0663 train_loss= 1.22898 train_rmse= 0.90359 val_loss= 1.45470 val_rmse= 1.11489 \t\ttime= 0.56649\n",
      "[*] Epoch: 0664 train_loss= 1.22530 train_rmse= 0.89712 val_loss= 1.45646 val_rmse= 1.11633 \t\ttime= 0.57746\n",
      "[*] Epoch: 0665 train_loss= 1.23608 train_rmse= 0.90862 val_loss= 1.45177 val_rmse= 1.11100 \t\ttime= 0.60937\n",
      "[*] Epoch: 0666 train_loss= 1.23543 train_rmse= 0.90775 val_loss= 1.45021 val_rmse= 1.10979 \t\ttime= 0.55252\n",
      "[*] Epoch: 0667 train_loss= 1.22534 train_rmse= 0.89393 val_loss= 1.45120 val_rmse= 1.11143 \t\ttime= 0.63430\n",
      "[*] Epoch: 0668 train_loss= 1.21857 train_rmse= 0.89159 val_loss= 1.45135 val_rmse= 1.11204 \t\ttime= 0.71010\n",
      "[*] Epoch: 0669 train_loss= 1.21964 train_rmse= 0.89798 val_loss= 1.44724 val_rmse= 1.10764 \t\ttime= 0.60539\n",
      "[*] Epoch: 0670 train_loss= 1.22758 train_rmse= 0.90058 val_loss= 1.44874 val_rmse= 1.10903 \t\ttime= 0.59940\n",
      "[*] Epoch: 0671 train_loss= 1.22257 train_rmse= 0.89856 val_loss= 1.45354 val_rmse= 1.11416 \t\ttime= 0.62433\n",
      "[*] Epoch: 0672 train_loss= 1.21695 train_rmse= 0.89098 val_loss= 1.45742 val_rmse= 1.11817 \t\ttime= 0.55452\n",
      "[*] Epoch: 0673 train_loss= 1.23914 train_rmse= 0.91515 val_loss= 1.45352 val_rmse= 1.11354 \t\ttime= 0.60837\n",
      "[*] Epoch: 0674 train_loss= 1.21648 train_rmse= 0.89429 val_loss= 1.44753 val_rmse= 1.10695 \t\ttime= 0.58643\n",
      "[*] Epoch: 0675 train_loss= 1.21550 train_rmse= 0.89612 val_loss= 1.44193 val_rmse= 1.10138 \t\ttime= 0.67419\n",
      "[*] Epoch: 0676 train_loss= 1.21505 train_rmse= 0.88978 val_loss= 1.44276 val_rmse= 1.10254 \t\ttime= 0.56848\n",
      "[*] Epoch: 0677 train_loss= 1.21938 train_rmse= 0.89456 val_loss= 1.44767 val_rmse= 1.10807 \t\ttime= 0.57645\n",
      "[*] Epoch: 0678 train_loss= 1.21647 train_rmse= 0.89097 val_loss= 1.45740 val_rmse= 1.11837 \t\ttime= 0.61136\n",
      "[*] Epoch: 0679 train_loss= 1.21793 train_rmse= 0.89673 val_loss= 1.46043 val_rmse= 1.12177 \t\ttime= 0.54754\n",
      "[*] Epoch: 0680 train_loss= 1.21114 train_rmse= 0.89260 val_loss= 1.45892 val_rmse= 1.11996 \t\ttime= 0.60638\n",
      "[*] Epoch: 0681 train_loss= 1.22731 train_rmse= 0.90454 val_loss= 1.45281 val_rmse= 1.11334 \t\ttime= 0.64428\n",
      "[*] Epoch: 0682 train_loss= 1.22165 train_rmse= 0.89643 val_loss= 1.44558 val_rmse= 1.10601 \t\ttime= 0.59242\n",
      "[*] Epoch: 0683 train_loss= 1.21936 train_rmse= 0.89411 val_loss= 1.44395 val_rmse= 1.10478 \t\ttime= 0.59940\n",
      "[*] Epoch: 0684 train_loss= 1.22095 train_rmse= 0.89984 val_loss= 1.44876 val_rmse= 1.11141 \t\ttime= 0.63430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0685 train_loss= 1.21440 train_rmse= 0.89421 val_loss= 1.45030 val_rmse= 1.11371 \t\ttime= 0.59441\n",
      "[*] Epoch: 0686 train_loss= 1.21510 train_rmse= 0.89701 val_loss= 1.44951 val_rmse= 1.11254 \t\ttime= 0.56748\n",
      "[*] Epoch: 0687 train_loss= 1.22018 train_rmse= 0.90571 val_loss= 1.44613 val_rmse= 1.10875 \t\ttime= 0.58743\n",
      "[*] Epoch: 0688 train_loss= 1.20903 train_rmse= 0.88827 val_loss= 1.44810 val_rmse= 1.11102 \t\ttime= 0.67021\n",
      "[*] Epoch: 0689 train_loss= 1.21886 train_rmse= 0.89741 val_loss= 1.44567 val_rmse= 1.10819 \t\ttime= 0.56846\n",
      "[*] Epoch: 0690 train_loss= 1.20149 train_rmse= 0.88618 val_loss= 1.44613 val_rmse= 1.10872 \t\ttime= 0.59740\n",
      "[*] Epoch: 0691 train_loss= 1.21830 train_rmse= 0.89788 val_loss= 1.45045 val_rmse= 1.11447 \t\ttime= 0.60438\n",
      "[*] Epoch: 0692 train_loss= 1.20742 train_rmse= 0.89176 val_loss= 1.45221 val_rmse= 1.11674 \t\ttime= 0.55751\n",
      "[*] Epoch: 0693 train_loss= 1.21031 train_rmse= 0.89341 val_loss= 1.44833 val_rmse= 1.11241 \t\ttime= 0.61037\n",
      "[*] Epoch: 0694 train_loss= 1.20480 train_rmse= 0.88894 val_loss= 1.44872 val_rmse= 1.11321 \t\ttime= 0.62533\n",
      "[*] Epoch: 0695 train_loss= 1.21429 train_rmse= 0.89812 val_loss= 1.44768 val_rmse= 1.11165 \t\ttime= 0.63131\n",
      "[*] Epoch: 0696 train_loss= 1.20048 train_rmse= 0.88676 val_loss= 1.44847 val_rmse= 1.11264 \t\ttime= 0.59242\n",
      "[*] Epoch: 0697 train_loss= 1.20919 train_rmse= 0.89325 val_loss= 1.44668 val_rmse= 1.11050 \t\ttime= 0.57247\n",
      "[*] Epoch: 0698 train_loss= 1.21718 train_rmse= 0.89427 val_loss= 1.44656 val_rmse= 1.10890 \t\ttime= 0.60239\n",
      "[*] Epoch: 0699 train_loss= 1.21113 train_rmse= 0.89403 val_loss= 1.44703 val_rmse= 1.10731 \t\ttime= 0.54654\n",
      "[*] Epoch: 0700 train_loss= 1.20729 train_rmse= 0.89078 val_loss= 1.44975 val_rmse= 1.10949 \t\ttime= 0.62832\n",
      "[*] Epoch: 0701 train_loss= 1.21715 train_rmse= 0.89502 val_loss= 1.45183 val_rmse= 1.11315 \t\ttime= 0.67021\n",
      "[*] Epoch: 0702 train_loss= 1.21236 train_rmse= 0.89414 val_loss= 1.44929 val_rmse= 1.11255 \t\ttime= 0.57745\n",
      "[*] Epoch: 0703 train_loss= 1.21558 train_rmse= 0.89716 val_loss= 1.44264 val_rmse= 1.10690 \t\ttime= 0.60039\n",
      "[*] Epoch: 0704 train_loss= 1.20538 train_rmse= 0.89207 val_loss= 1.43470 val_rmse= 1.09877 \t\ttime= 0.61835\n",
      "[*] Epoch: 0705 train_loss= 1.21480 train_rmse= 0.89724 val_loss= 1.43514 val_rmse= 1.09965 \t\ttime= 0.55950\n",
      "[*] Epoch: 0706 train_loss= 1.20873 train_rmse= 0.89457 val_loss= 1.43773 val_rmse= 1.10285 \t\ttime= 0.58843\n",
      "[*] Epoch: 0707 train_loss= 1.20831 train_rmse= 0.88524 val_loss= 1.44664 val_rmse= 1.11175 \t\ttime= 0.57247\n",
      "[*] Epoch: 0708 train_loss= 1.19549 train_rmse= 0.87923 val_loss= 1.45442 val_rmse= 1.11795 \t\ttime= 0.69913\n",
      "[*] Epoch: 0709 train_loss= 1.19689 train_rmse= 0.88243 val_loss= 1.45974 val_rmse= 1.12216 \t\ttime= 0.55552\n",
      "[*] Epoch: 0710 train_loss= 1.21380 train_rmse= 0.89477 val_loss= 1.45821 val_rmse= 1.12032 \t\ttime= 0.59541\n",
      "[*] Epoch: 0711 train_loss= 1.19173 train_rmse= 0.88625 val_loss= 1.45411 val_rmse= 1.11683 \t\ttime= 0.59342\n",
      "[*] Epoch: 0712 train_loss= 1.19674 train_rmse= 0.88589 val_loss= 1.44885 val_rmse= 1.11277 \t\ttime= 0.55053\n",
      "[*] Epoch: 0713 train_loss= 1.19542 train_rmse= 0.88600 val_loss= 1.44964 val_rmse= 1.11592 \t\ttime= 0.60339\n",
      "[*] Epoch: 0714 train_loss= 1.20555 train_rmse= 0.89686 val_loss= 1.44653 val_rmse= 1.11370 \t\ttime= 0.64727\n",
      "[*] Epoch: 0715 train_loss= 1.20778 train_rmse= 0.89728 val_loss= 1.44161 val_rmse= 1.10753 \t\ttime= 0.61535\n",
      "[*] Epoch: 0716 train_loss= 1.19959 train_rmse= 0.88200 val_loss= 1.44000 val_rmse= 1.10200 \t\ttime= 0.62533\n",
      "[*] Epoch: 0717 train_loss= 1.20107 train_rmse= 0.88826 val_loss= 1.44491 val_rmse= 1.10505 \t\ttime= 0.63430\n",
      "[*] Epoch: 0718 train_loss= 1.20290 train_rmse= 0.88760 val_loss= 1.45430 val_rmse= 1.11521 \t\ttime= 0.60338\n",
      "[*] Epoch: 0719 train_loss= 1.19787 train_rmse= 0.88517 val_loss= 1.45912 val_rmse= 1.12190 \t\ttime= 0.58744\n",
      "[*] Epoch: 0720 train_loss= 1.20362 train_rmse= 0.88604 val_loss= 1.45762 val_rmse= 1.12199 \t\ttime= 0.58643\n",
      "[*] Epoch: 0721 train_loss= 1.20278 train_rmse= 0.88979 val_loss= 1.45164 val_rmse= 1.11681 \t\ttime= 0.67718\n",
      "[*] Epoch: 0722 train_loss= 1.18891 train_rmse= 0.87916 val_loss= 1.44209 val_rmse= 1.10604 \t\ttime= 0.55950\n",
      "[*] Epoch: 0723 train_loss= 1.19488 train_rmse= 0.88429 val_loss= 1.43572 val_rmse= 1.09916 \t\ttime= 0.61236\n",
      "[*] Epoch: 0724 train_loss= 1.20232 train_rmse= 0.88499 val_loss= 1.43400 val_rmse= 1.09831 \t\ttime= 0.63530\n",
      "[*] Epoch: 0725 train_loss= 1.21040 train_rmse= 0.88810 val_loss= 1.44311 val_rmse= 1.10945 \t\ttime= 0.56050\n",
      "[*] Epoch: 0726 train_loss= 1.20061 train_rmse= 0.88628 val_loss= 1.45623 val_rmse= 1.12431 \t\ttime= 0.60438\n",
      "[*] Epoch: 0727 train_loss= 1.20018 train_rmse= 0.88520 val_loss= 1.46332 val_rmse= 1.13082 \t\ttime= 0.62833\n",
      "[*] Epoch: 0728 train_loss= 1.19123 train_rmse= 0.88334 val_loss= 1.46362 val_rmse= 1.12951 \t\ttime= 0.60438\n",
      "[*] Epoch: 0729 train_loss= 1.21092 train_rmse= 0.89949 val_loss= 1.45674 val_rmse= 1.12041 \t\ttime= 0.60338\n",
      "[*] Epoch: 0730 train_loss= 1.20873 train_rmse= 0.89066 val_loss= 1.44903 val_rmse= 1.11212 \t\ttime= 0.60239\n",
      "[*] Epoch: 0731 train_loss= 1.19407 train_rmse= 0.88384 val_loss= 1.44211 val_rmse= 1.10668 \t\ttime= 0.59142\n",
      "[*] Epoch: 0732 train_loss= 1.19935 train_rmse= 0.87996 val_loss= 1.44654 val_rmse= 1.11493 \t\ttime= 0.56051\n",
      "[*] Epoch: 0733 train_loss= 1.19980 train_rmse= 0.88540 val_loss= 1.45176 val_rmse= 1.12240 \t\ttime= 0.61436\n",
      "[*] Epoch: 0734 train_loss= 1.19324 train_rmse= 0.88151 val_loss= 1.45388 val_rmse= 1.12415 \t\ttime= 0.63231\n",
      "[*] Epoch: 0735 train_loss= 1.20368 train_rmse= 0.89099 val_loss= 1.45188 val_rmse= 1.11984 \t\ttime= 0.59640\n",
      "[*] Epoch: 0736 train_loss= 1.19571 train_rmse= 0.88808 val_loss= 1.45002 val_rmse= 1.11468 \t\ttime= 0.58743\n",
      "[*] Epoch: 0737 train_loss= 1.19511 train_rmse= 0.88087 val_loss= 1.45199 val_rmse= 1.11437 \t\ttime= 0.59741\n",
      "[*] Epoch: 0738 train_loss= 1.19811 train_rmse= 0.88705 val_loss= 1.45559 val_rmse= 1.11838 \t\ttime= 0.56848\n",
      "[*] Epoch: 0739 train_loss= 1.20244 train_rmse= 0.88942 val_loss= 1.45490 val_rmse= 1.11881 \t\ttime= 0.58244\n",
      "[*] Epoch: 0740 train_loss= 1.20297 train_rmse= 0.89141 val_loss= 1.45039 val_rmse= 1.11608 \t\ttime= 0.58942\n",
      "[*] Epoch: 0741 train_loss= 1.19055 train_rmse= 0.88585 val_loss= 1.44527 val_rmse= 1.11253 \t\ttime= 0.67121\n",
      "[*] Epoch: 0742 train_loss= 1.19087 train_rmse= 0.87848 val_loss= 1.44123 val_rmse= 1.10862 \t\ttime= 0.58344\n",
      "[*] Epoch: 0743 train_loss= 1.18895 train_rmse= 0.87793 val_loss= 1.44244 val_rmse= 1.10935 \t\ttime= 0.58244\n",
      "[*] Epoch: 0744 train_loss= 1.19506 train_rmse= 0.88452 val_loss= 1.44667 val_rmse= 1.11237 \t\ttime= 0.59940\n",
      "[*] Epoch: 0745 train_loss= 1.17922 train_rmse= 0.87521 val_loss= 1.45104 val_rmse= 1.11573 \t\ttime= 0.52460\n",
      "[*] Epoch: 0746 train_loss= 1.19049 train_rmse= 0.87965 val_loss= 1.44968 val_rmse= 1.11415 \t\ttime= 0.62533\n",
      "[*] Epoch: 0747 train_loss= 1.18004 train_rmse= 0.87598 val_loss= 1.44688 val_rmse= 1.11090 \t\ttime= 0.61735\n",
      "[*] Epoch: 0748 train_loss= 1.18934 train_rmse= 0.88073 val_loss= 1.44341 val_rmse= 1.10800 \t\ttime= 0.60538\n",
      "[*] Epoch: 0749 train_loss= 1.18764 train_rmse= 0.87556 val_loss= 1.44573 val_rmse= 1.11168 \t\ttime= 0.61536\n",
      "[*] Epoch: 0750 train_loss= 1.19670 train_rmse= 0.88822 val_loss= 1.45310 val_rmse= 1.12156 \t\ttime= 0.62134\n",
      "[*] Epoch: 0751 train_loss= 1.20132 train_rmse= 0.89080 val_loss= 1.45489 val_rmse= 1.12312 \t\ttime= 0.60239\n",
      "[*] Epoch: 0752 train_loss= 1.20047 train_rmse= 0.89116 val_loss= 1.45164 val_rmse= 1.11727 \t\ttime= 0.57147\n",
      "[*] Epoch: 0753 train_loss= 1.18728 train_rmse= 0.87741 val_loss= 1.44661 val_rmse= 1.10943 \t\ttime= 0.58842\n",
      "[*] Epoch: 0754 train_loss= 1.19464 train_rmse= 0.88540 val_loss= 1.44456 val_rmse= 1.10692 \t\ttime= 0.66024\n",
      "[*] Epoch: 0755 train_loss= 1.19381 train_rmse= 0.87890 val_loss= 1.44593 val_rmse= 1.11002 \t\ttime= 0.57446\n",
      "[*] Epoch: 0756 train_loss= 1.18710 train_rmse= 0.87718 val_loss= 1.44727 val_rmse= 1.11279 \t\ttime= 0.60638\n",
      "[*] Epoch: 0757 train_loss= 1.19659 train_rmse= 0.88620 val_loss= 1.44897 val_rmse= 1.11476 \t\ttime= 0.62532\n",
      "[*] Epoch: 0758 train_loss= 1.18848 train_rmse= 0.88091 val_loss= 1.44812 val_rmse= 1.11339 \t\ttime= 0.54654\n",
      "[*] Epoch: 0759 train_loss= 1.19099 train_rmse= 0.88165 val_loss= 1.44526 val_rmse= 1.10898 \t\ttime= 0.62533\n",
      "[*] Epoch: 0760 train_loss= 1.18364 train_rmse= 0.87639 val_loss= 1.44247 val_rmse= 1.10537 \t\ttime= 0.61735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0761 train_loss= 1.18980 train_rmse= 0.88039 val_loss= 1.44468 val_rmse= 1.10934 \t\ttime= 0.63031\n",
      "[*] Epoch: 0762 train_loss= 1.18113 train_rmse= 0.87595 val_loss= 1.44615 val_rmse= 1.11180 \t\ttime= 0.58045\n",
      "[*] Epoch: 0763 train_loss= 1.18363 train_rmse= 0.87201 val_loss= 1.44660 val_rmse= 1.11332 \t\ttime= 0.58045\n",
      "[*] Epoch: 0764 train_loss= 1.18798 train_rmse= 0.87793 val_loss= 1.44499 val_rmse= 1.11044 \t\ttime= 0.59740\n",
      "[*] Epoch: 0765 train_loss= 1.19057 train_rmse= 0.87964 val_loss= 1.44447 val_rmse= 1.10809 \t\ttime= 0.57047\n",
      "[*] Epoch: 0766 train_loss= 1.19732 train_rmse= 0.88996 val_loss= 1.44766 val_rmse= 1.11087 \t\ttime= 0.59042\n",
      "[*] Epoch: 0767 train_loss= 1.18237 train_rmse= 0.87823 val_loss= 1.44944 val_rmse= 1.11343 \t\ttime= 0.66821\n",
      "[*] Epoch: 0768 train_loss= 1.18123 train_rmse= 0.87788 val_loss= 1.45005 val_rmse= 1.11467 \t\ttime= 0.59241\n",
      "[*] Epoch: 0769 train_loss= 1.17657 train_rmse= 0.87013 val_loss= 1.45120 val_rmse= 1.11618 \t\ttime= 0.66323\n",
      "[*] Epoch: 0770 train_loss= 1.18398 train_rmse= 0.87926 val_loss= 1.44943 val_rmse= 1.11313 \t\ttime= 0.59142\n",
      "[*] Epoch: 0771 train_loss= 1.17674 train_rmse= 0.87108 val_loss= 1.44866 val_rmse= 1.11055 \t\ttime= 0.55751\n",
      "[*] Epoch: 0772 train_loss= 1.18019 train_rmse= 0.87295 val_loss= 1.44930 val_rmse= 1.11101 \t\ttime= 0.59443\n",
      "[*] Epoch: 0773 train_loss= 1.19114 train_rmse= 0.87940 val_loss= 1.45139 val_rmse= 1.11526 \t\ttime= 0.60239\n",
      "[*] Epoch: 0774 train_loss= 1.17609 train_rmse= 0.87423 val_loss= 1.45377 val_rmse= 1.12007 \t\ttime= 0.64727\n",
      "[*] Epoch: 0775 train_loss= 1.18285 train_rmse= 0.87775 val_loss= 1.45187 val_rmse= 1.11918 \t\ttime= 0.58244\n",
      "[*] Epoch: 0776 train_loss= 1.17435 train_rmse= 0.87307 val_loss= 1.44818 val_rmse= 1.11589 \t\ttime= 0.59341\n",
      "[*] Epoch: 0777 train_loss= 1.19493 train_rmse= 0.88815 val_loss= 1.44096 val_rmse= 1.10714 \t\ttime= 0.60139\n",
      "[*] Epoch: 0778 train_loss= 1.18828 train_rmse= 0.88418 val_loss= 1.44253 val_rmse= 1.10744 \t\ttime= 0.55053\n",
      "[*] Epoch: 0779 train_loss= 1.19373 train_rmse= 0.88407 val_loss= 1.45174 val_rmse= 1.11732 \t\ttime= 0.61735\n",
      "[*] Epoch: 0780 train_loss= 1.19078 train_rmse= 0.88430 val_loss= 1.45628 val_rmse= 1.12232 \t\ttime= 0.62932\n",
      "[*] Epoch: 0781 train_loss= 1.17552 train_rmse= 0.87120 val_loss= 1.45971 val_rmse= 1.12614 \t\ttime= 0.59939\n",
      "[*] Epoch: 0782 train_loss= 1.17682 train_rmse= 0.87489 val_loss= 1.45907 val_rmse= 1.12514 \t\ttime= 0.60538\n",
      "[*] Epoch: 0783 train_loss= 1.18666 train_rmse= 0.88226 val_loss= 1.45482 val_rmse= 1.12026 \t\ttime= 0.60141\n",
      "[*] Epoch: 0784 train_loss= 1.17215 train_rmse= 0.87187 val_loss= 1.45002 val_rmse= 1.11575 \t\ttime= 0.56549\n",
      "[*] Epoch: 0785 train_loss= 1.17496 train_rmse= 0.87120 val_loss= 1.44383 val_rmse= 1.10981 \t\ttime= 0.55252\n",
      "[*] Epoch: 0786 train_loss= 1.16821 train_rmse= 0.86834 val_loss= 1.43948 val_rmse= 1.10526 \t\ttime= 0.61035\n",
      "[*] Epoch: 0787 train_loss= 1.16884 train_rmse= 0.86822 val_loss= 1.44124 val_rmse= 1.10777 \t\ttime= 0.67518\n",
      "[*] Epoch: 0788 train_loss= 1.17837 train_rmse= 0.86833 val_loss= 1.44513 val_rmse= 1.11196 \t\ttime= 0.56848\n",
      "[*] Epoch: 0789 train_loss= 1.17073 train_rmse= 0.86945 val_loss= 1.44836 val_rmse= 1.11427 \t\ttime= 0.60239\n",
      "[*] Epoch: 0790 train_loss= 1.17339 train_rmse= 0.87036 val_loss= 1.45178 val_rmse= 1.11695 \t\ttime= 0.61934\n",
      "[*] Epoch: 0791 train_loss= 1.17553 train_rmse= 0.87293 val_loss= 1.45534 val_rmse= 1.12049 \t\ttime= 0.53557\n",
      "[*] Epoch: 0792 train_loss= 1.17148 train_rmse= 0.86955 val_loss= 1.45799 val_rmse= 1.12319 \t\ttime= 0.62932\n",
      "[*] Epoch: 0793 train_loss= 1.17981 train_rmse= 0.87629 val_loss= 1.45517 val_rmse= 1.11946 \t\ttime= 0.62134\n",
      "[*] Epoch: 0794 train_loss= 1.17920 train_rmse= 0.87248 val_loss= 1.44775 val_rmse= 1.11144 \t\ttime= 0.63530\n",
      "[*] Epoch: 0795 train_loss= 1.17408 train_rmse= 0.87018 val_loss= 1.44192 val_rmse= 1.10537 \t\ttime= 0.58045\n",
      "[*] Epoch: 0796 train_loss= 1.17968 train_rmse= 0.87404 val_loss= 1.43351 val_rmse= 1.09666 \t\ttime= 0.60339\n",
      "[*] Epoch: 0797 train_loss= 1.16734 train_rmse= 0.86958 val_loss= 1.43189 val_rmse= 1.09641 \t\ttime= 0.59940\n",
      "[*] Epoch: 0798 train_loss= 1.18779 train_rmse= 0.87026 val_loss= 1.44220 val_rmse= 1.10786 \t\ttime= 0.55252\n",
      "[*] Epoch: 0799 train_loss= 1.18322 train_rmse= 0.87664 val_loss= 1.45421 val_rmse= 1.12009 \t\ttime= 0.63031\n",
      "[*] Epoch: 0800 train_loss= 1.17397 train_rmse= 0.86998 val_loss= 1.45995 val_rmse= 1.12463 \t\ttime= 0.66821\n",
      "[*] Epoch: 0801 train_loss= 1.19119 train_rmse= 0.88514 val_loss= 1.45761 val_rmse= 1.12003 \t\ttime= 0.56648\n",
      "[*] Epoch: 0802 train_loss= 1.18531 train_rmse= 0.87530 val_loss= 1.45420 val_rmse= 1.11635 \t\ttime= 0.58344\n",
      "[*] Epoch: 0803 train_loss= 1.18042 train_rmse= 0.87042 val_loss= 1.44879 val_rmse= 1.11233 \t\ttime= 0.62533\n",
      "[*] Epoch: 0804 train_loss= 1.17369 train_rmse= 0.87259 val_loss= 1.44255 val_rmse= 1.10743 \t\ttime= 0.58244\n",
      "[*] Epoch: 0805 train_loss= 1.16954 train_rmse= 0.86635 val_loss= 1.44046 val_rmse= 1.10716 \t\ttime= 0.62035\n",
      "[*] Epoch: 0806 train_loss= 1.17232 train_rmse= 0.86861 val_loss= 1.44262 val_rmse= 1.10969 \t\ttime= 0.64329\n",
      "[*] Epoch: 0807 train_loss= 1.17889 train_rmse= 0.87499 val_loss= 1.44343 val_rmse= 1.10895 \t\ttime= 0.65226\n",
      "[*] Epoch: 0808 train_loss= 1.17485 train_rmse= 0.87396 val_loss= 1.44333 val_rmse= 1.10628 \t\ttime= 0.58244\n",
      "[*] Epoch: 0809 train_loss= 1.17851 train_rmse= 0.87477 val_loss= 1.44456 val_rmse= 1.10652 \t\ttime= 0.56748\n",
      "[*] Epoch: 0810 train_loss= 1.17152 train_rmse= 0.86864 val_loss= 1.44818 val_rmse= 1.11051 \t\ttime= 0.60339\n",
      "[*] Epoch: 0811 train_loss= 1.18473 train_rmse= 0.88269 val_loss= 1.44951 val_rmse= 1.11307 \t\ttime= 0.53557\n",
      "[*] Epoch: 0812 train_loss= 1.17166 train_rmse= 0.86674 val_loss= 1.44765 val_rmse= 1.11260 \t\ttime= 0.61237\n",
      "[*] Epoch: 0813 train_loss= 1.17691 train_rmse= 0.87114 val_loss= 1.44269 val_rmse= 1.10834 \t\ttime= 0.63829\n",
      "[*] Epoch: 0814 train_loss= 1.17057 train_rmse= 0.86583 val_loss= 1.44131 val_rmse= 1.10623 \t\ttime= 0.59640\n",
      "[*] Epoch: 0815 train_loss= 1.17079 train_rmse= 0.86495 val_loss= 1.44514 val_rmse= 1.10933 \t\ttime= 0.60040\n",
      "[*] Epoch: 0816 train_loss= 1.17786 train_rmse= 0.87363 val_loss= 1.45143 val_rmse= 1.11414 \t\ttime= 0.62124\n",
      "[*] Epoch: 0817 train_loss= 1.16225 train_rmse= 0.86450 val_loss= 1.45522 val_rmse= 1.11713 \t\ttime= 0.58743\n",
      "[*] Epoch: 0818 train_loss= 1.17200 train_rmse= 0.87025 val_loss= 1.45411 val_rmse= 1.11557 \t\ttime= 0.59042\n",
      "[*] Epoch: 0819 train_loss= 1.17155 train_rmse= 0.87077 val_loss= 1.44967 val_rmse= 1.11118 \t\ttime= 0.59242\n",
      "[*] Epoch: 0820 train_loss= 1.16471 train_rmse= 0.86565 val_loss= 1.44448 val_rmse= 1.10692 \t\ttime= 0.66722\n",
      "[*] Epoch: 0821 train_loss= 1.15845 train_rmse= 0.86235 val_loss= 1.44335 val_rmse= 1.10733 \t\ttime= 0.55452\n",
      "[*] Epoch: 0822 train_loss= 1.16777 train_rmse= 0.86659 val_loss= 1.44797 val_rmse= 1.11382 \t\ttime= 0.62433\n",
      "[*] Epoch: 0823 train_loss= 1.17702 train_rmse= 0.87589 val_loss= 1.44713 val_rmse= 1.11312 \t\ttime= 0.59541\n",
      "[*] Epoch: 0824 train_loss= 1.16600 train_rmse= 0.86522 val_loss= 1.44486 val_rmse= 1.11073 \t\ttime= 0.52260\n",
      "[*] Epoch: 0825 train_loss= 1.16044 train_rmse= 0.86056 val_loss= 1.44569 val_rmse= 1.11152 \t\ttime= 0.61635\n",
      "[*] Epoch: 0826 train_loss= 1.18180 train_rmse= 0.87917 val_loss= 1.44842 val_rmse= 1.11494 \t\ttime= 0.60937\n",
      "[*] Epoch: 0827 train_loss= 1.16463 train_rmse= 0.86832 val_loss= 1.44815 val_rmse= 1.11489 \t\ttime= 0.61535\n",
      "[*] Epoch: 0828 train_loss= 1.17137 train_rmse= 0.87166 val_loss= 1.44551 val_rmse= 1.11243 \t\ttime= 0.60139\n",
      "[*] Epoch: 0829 train_loss= 1.16179 train_rmse= 0.86229 val_loss= 1.44457 val_rmse= 1.11065 \t\ttime= 0.58444\n",
      "[*] Epoch: 0830 train_loss= 1.18332 train_rmse= 0.87281 val_loss= 1.44595 val_rmse= 1.11268 \t\ttime= 0.61834\n",
      "[*] Epoch: 0831 train_loss= 1.16821 train_rmse= 0.86666 val_loss= 1.45278 val_rmse= 1.12202 \t\ttime= 0.54455\n",
      "[*] Epoch: 0832 train_loss= 1.17518 train_rmse= 0.87254 val_loss= 1.45507 val_rmse= 1.12459 \t\ttime= 0.60837\n",
      "[*] Epoch: 0833 train_loss= 1.16430 train_rmse= 0.86792 val_loss= 1.45340 val_rmse= 1.12207 \t\ttime= 0.64927\n",
      "[*] Epoch: 0834 train_loss= 1.17060 train_rmse= 0.86854 val_loss= 1.45211 val_rmse= 1.11913 \t\ttime= 0.57247\n",
      "[*] Epoch: 0835 train_loss= 1.16391 train_rmse= 0.86474 val_loss= 1.44897 val_rmse= 1.11474 \t\ttime= 0.57646\n",
      "[*] Epoch: 0836 train_loss= 1.16096 train_rmse= 0.86150 val_loss= 1.45133 val_rmse= 1.11792 \t\ttime= 0.60738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0837 train_loss= 1.16522 train_rmse= 0.86518 val_loss= 1.45491 val_rmse= 1.12212 \t\ttime= 0.58145\n",
      "[*] Epoch: 0838 train_loss= 1.17347 train_rmse= 0.87018 val_loss= 1.45788 val_rmse= 1.12410 \t\ttime= 0.60039\n",
      "[*] Epoch: 0839 train_loss= 1.16100 train_rmse= 0.86119 val_loss= 1.45854 val_rmse= 1.12372 \t\ttime= 0.58245\n",
      "[*] Epoch: 0840 train_loss= 1.16339 train_rmse= 0.86423 val_loss= 1.45675 val_rmse= 1.11989 \t\ttime= 0.66921\n",
      "[*] Epoch: 0841 train_loss= 1.17273 train_rmse= 0.87016 val_loss= 1.45333 val_rmse= 1.11505 \t\ttime= 0.56549\n",
      "[*] Epoch: 0842 train_loss= 1.17976 train_rmse= 0.87371 val_loss= 1.45264 val_rmse= 1.11605 \t\ttime= 0.62931\n",
      "[*] Epoch: 0843 train_loss= 1.16103 train_rmse= 0.86405 val_loss= 1.45261 val_rmse= 1.11877 \t\ttime= 0.56948\n",
      "[*] Epoch: 0844 train_loss= 1.16336 train_rmse= 0.86230 val_loss= 1.45175 val_rmse= 1.12013 \t\ttime= 0.52260\n",
      "[*] Epoch: 0845 train_loss= 1.16207 train_rmse= 0.86239 val_loss= 1.44880 val_rmse= 1.11667 \t\ttime= 0.60239\n",
      "[*] Epoch: 0846 train_loss= 1.16977 train_rmse= 0.86759 val_loss= 1.44654 val_rmse= 1.11168 \t\ttime= 0.63630\n",
      "[*] Epoch: 0847 train_loss= 1.16541 train_rmse= 0.86336 val_loss= 1.45263 val_rmse= 1.11536 \t\ttime= 0.62133\n",
      "[*] Epoch: 0848 train_loss= 1.16204 train_rmse= 0.86435 val_loss= 1.45644 val_rmse= 1.11771 \t\ttime= 0.58045\n",
      "[*] Epoch: 0849 train_loss= 1.16456 train_rmse= 0.86392 val_loss= 1.45784 val_rmse= 1.11945 \t\ttime= 0.58643\n",
      "[*] Epoch: 0850 train_loss= 1.16406 train_rmse= 0.86563 val_loss= 1.45898 val_rmse= 1.12192 \t\ttime= 0.61336\n",
      "[*] Epoch: 0851 train_loss= 1.16222 train_rmse= 0.86458 val_loss= 1.45804 val_rmse= 1.12107 \t\ttime= 0.53856\n",
      "[*] Epoch: 0852 train_loss= 1.16660 train_rmse= 0.86745 val_loss= 1.45522 val_rmse= 1.11814 \t\ttime= 0.60538\n",
      "[*] Epoch: 0853 train_loss= 1.17530 train_rmse= 0.87654 val_loss= 1.44585 val_rmse= 1.10893 \t\ttime= 0.66921\n",
      "[*] Epoch: 0854 train_loss= 1.15707 train_rmse= 0.86021 val_loss= 1.44216 val_rmse= 1.10733 \t\ttime= 0.62632\n",
      "[*] Epoch: 0855 train_loss= 1.17400 train_rmse= 0.86509 val_loss= 1.44605 val_rmse= 1.11264 \t\ttime= 0.68417\n",
      "[*] Epoch: 0856 train_loss= 1.16220 train_rmse= 0.86118 val_loss= 1.45189 val_rmse= 1.11883 \t\ttime= 0.64826\n",
      "[*] Epoch: 0857 train_loss= 1.17861 train_rmse= 0.87943 val_loss= 1.45243 val_rmse= 1.11866 \t\ttime= 0.54953\n",
      "[*] Epoch: 0858 train_loss= 1.15894 train_rmse= 0.85790 val_loss= 1.45294 val_rmse= 1.11690 \t\ttime= 0.64926\n",
      "[*] Epoch: 0859 train_loss= 1.15643 train_rmse= 0.86034 val_loss= 1.45177 val_rmse= 1.11396 \t\ttime= 0.69514\n",
      "[*] Epoch: 0860 train_loss= 1.17100 train_rmse= 0.87025 val_loss= 1.45018 val_rmse= 1.11198 \t\ttime= 0.61535\n",
      "[*] Epoch: 0861 train_loss= 1.16581 train_rmse= 0.86775 val_loss= 1.44770 val_rmse= 1.11015 \t\ttime= 0.64627\n",
      "[*] Epoch: 0862 train_loss= 1.15398 train_rmse= 0.85982 val_loss= 1.44564 val_rmse= 1.10925 \t\ttime= 0.63231\n",
      "[*] Epoch: 0863 train_loss= 1.15873 train_rmse= 0.85524 val_loss= 1.44752 val_rmse= 1.11231 \t\ttime= 0.59940\n",
      "[*] Epoch: 0864 train_loss= 1.16156 train_rmse= 0.86320 val_loss= 1.44939 val_rmse= 1.11456 \t\ttime= 0.62932\n",
      "[*] Epoch: 0865 train_loss= 1.15811 train_rmse= 0.85970 val_loss= 1.45204 val_rmse= 1.11639 \t\ttime= 0.62034\n",
      "[*] Epoch: 0866 train_loss= 1.15344 train_rmse= 0.85322 val_loss= 1.45369 val_rmse= 1.11646 \t\ttime= 0.65724\n",
      "[*] Epoch: 0867 train_loss= 1.16190 train_rmse= 0.86349 val_loss= 1.45618 val_rmse= 1.11821 \t\ttime= 0.62633\n",
      "[*] Epoch: 0868 train_loss= 1.15749 train_rmse= 0.86091 val_loss= 1.45588 val_rmse= 1.11869 \t\ttime= 0.61735\n",
      "[*] Epoch: 0869 train_loss= 1.16225 train_rmse= 0.86600 val_loss= 1.45197 val_rmse= 1.11554 \t\ttime= 0.66422\n",
      "[*] Epoch: 0870 train_loss= 1.16277 train_rmse= 0.86158 val_loss= 1.45051 val_rmse= 1.11609 \t\ttime= 0.58444\n",
      "[*] Epoch: 0871 train_loss= 1.15249 train_rmse= 0.85951 val_loss= 1.44787 val_rmse= 1.11419 \t\ttime= 0.63231\n",
      "[*] Epoch: 0872 train_loss= 1.16290 train_rmse= 0.86341 val_loss= 1.44612 val_rmse= 1.11164 \t\ttime= 0.69414\n",
      "[*] Epoch: 0873 train_loss= 1.15917 train_rmse= 0.85802 val_loss= 1.44676 val_rmse= 1.11040 \t\ttime= 0.61038\n",
      "[*] Epoch: 0874 train_loss= 1.14661 train_rmse= 0.85429 val_loss= 1.45104 val_rmse= 1.11465 \t\ttime= 0.63032\n",
      "[*] Epoch: 0875 train_loss= 1.14757 train_rmse= 0.85360 val_loss= 1.45280 val_rmse= 1.11629 \t\ttime= 0.67021\n",
      "[*] Epoch: 0876 train_loss= 1.16639 train_rmse= 0.86716 val_loss= 1.45492 val_rmse= 1.11979 \t\ttime= 0.55053\n",
      "[*] Epoch: 0877 train_loss= 1.15719 train_rmse= 0.86074 val_loss= 1.45126 val_rmse= 1.11650 \t\ttime= 0.63730\n",
      "[*] Epoch: 0878 train_loss= 1.16875 train_rmse= 0.86716 val_loss= 1.44749 val_rmse= 1.11147 \t\ttime= 0.67918\n",
      "[*] Epoch: 0879 train_loss= 1.16532 train_rmse= 0.86794 val_loss= 1.44347 val_rmse= 1.10588 \t\ttime= 0.62931\n",
      "[*] Epoch: 0880 train_loss= 1.15544 train_rmse= 0.85585 val_loss= 1.44701 val_rmse= 1.11041 \t\ttime= 0.63929\n",
      "[*] Epoch: 0881 train_loss= 1.15321 train_rmse= 0.85548 val_loss= 1.45236 val_rmse= 1.11744 \t\ttime= 0.68517\n",
      "[*] Epoch: 0882 train_loss= 1.16650 train_rmse= 0.87072 val_loss= 1.45314 val_rmse= 1.11862 \t\ttime= 0.57944\n",
      "[*] Epoch: 0883 train_loss= 1.15626 train_rmse= 0.86099 val_loss= 1.45681 val_rmse= 1.12284 \t\ttime= 0.68816\n",
      "[*] Epoch: 0884 train_loss= 1.15786 train_rmse= 0.85991 val_loss= 1.45965 val_rmse= 1.12603 \t\ttime= 0.67221\n",
      "[*] Epoch: 0885 train_loss= 1.15602 train_rmse= 0.86132 val_loss= 1.45954 val_rmse= 1.12577 \t\ttime= 0.66123\n",
      "[*] Epoch: 0886 train_loss= 1.16550 train_rmse= 0.87216 val_loss= 1.45486 val_rmse= 1.11939 \t\ttime= 0.65226\n",
      "[*] Epoch: 0887 train_loss= 1.14946 train_rmse= 0.85230 val_loss= 1.45171 val_rmse= 1.11496 \t\ttime= 0.64027\n",
      "[*] Epoch: 0888 train_loss= 1.15759 train_rmse= 0.85936 val_loss= 1.44914 val_rmse= 1.11247 \t\ttime= 0.57247\n",
      "[*] Epoch: 0889 train_loss= 1.15284 train_rmse= 0.86071 val_loss= 1.44836 val_rmse= 1.11243 \t\ttime= 0.62034\n",
      "[*] Epoch: 0890 train_loss= 1.16217 train_rmse= 0.86165 val_loss= 1.44723 val_rmse= 1.11203 \t\ttime= 0.66323\n",
      "[*] Epoch: 0891 train_loss= 1.13861 train_rmse= 0.84545 val_loss= 1.44938 val_rmse= 1.11393 \t\ttime= 0.66223\n",
      "[*] Epoch: 0892 train_loss= 1.15381 train_rmse= 0.85152 val_loss= 1.45412 val_rmse= 1.11727 \t\ttime= 0.61635\n",
      "[*] Epoch: 0893 train_loss= 1.16222 train_rmse= 0.86343 val_loss= 1.45886 val_rmse= 1.12228 \t\ttime= 0.59939\n",
      "[*] Epoch: 0894 train_loss= 1.14179 train_rmse= 0.85038 val_loss= 1.46185 val_rmse= 1.12526 \t\ttime= 0.61934\n",
      "[*] Epoch: 0895 train_loss= 1.15529 train_rmse= 0.85755 val_loss= 1.46177 val_rmse= 1.12585 \t\ttime= 0.54155\n",
      "[*] Epoch: 0896 train_loss= 1.16646 train_rmse= 0.87484 val_loss= 1.45679 val_rmse= 1.12076 \t\ttime= 0.58643\n",
      "[*] Epoch: 0897 train_loss= 1.14722 train_rmse= 0.85246 val_loss= 1.45183 val_rmse= 1.11573 \t\ttime= 0.67619\n",
      "[*] Epoch: 0898 train_loss= 1.15279 train_rmse= 0.85912 val_loss= 1.44629 val_rmse= 1.10978 \t\ttime= 0.59940\n",
      "[*] Epoch: 0899 train_loss= 1.15250 train_rmse= 0.85658 val_loss= 1.44323 val_rmse= 1.10685 \t\ttime= 0.60239\n",
      "[*] Epoch: 0900 train_loss= 1.16231 train_rmse= 0.86195 val_loss= 1.44702 val_rmse= 1.11127 \t\ttime= 0.59142\n",
      "[*] Epoch: 0901 train_loss= 1.14828 train_rmse= 0.85032 val_loss= 1.45347 val_rmse= 1.11764 \t\ttime= 0.57845\n",
      "[*] Epoch: 0902 train_loss= 1.15427 train_rmse= 0.85738 val_loss= 1.45679 val_rmse= 1.11966 \t\ttime= 0.60239\n",
      "[*] Epoch: 0903 train_loss= 1.15135 train_rmse= 0.85754 val_loss= 1.45683 val_rmse= 1.11780 \t\ttime= 0.62733\n",
      "[*] Epoch: 0904 train_loss= 1.14566 train_rmse= 0.85275 val_loss= 1.45726 val_rmse= 1.11799 \t\ttime= 0.64328\n",
      "[*] Epoch: 0905 train_loss= 1.14848 train_rmse= 0.85210 val_loss= 1.45560 val_rmse= 1.11724 \t\ttime= 0.55552\n",
      "[*] Epoch: 0906 train_loss= 1.14810 train_rmse= 0.85534 val_loss= 1.45304 val_rmse= 1.11661 \t\ttime= 0.58643\n",
      "[*] Epoch: 0907 train_loss= 1.14536 train_rmse= 0.84760 val_loss= 1.45424 val_rmse= 1.11946 \t\ttime= 0.59541\n",
      "[*] Epoch: 0908 train_loss= 1.14593 train_rmse= 0.84870 val_loss= 1.45712 val_rmse= 1.12297 \t\ttime= 0.53158\n",
      "[*] Epoch: 0909 train_loss= 1.14672 train_rmse= 0.85486 val_loss= 1.45704 val_rmse= 1.12227 \t\ttime= 0.62633\n",
      "[*] Epoch: 0910 train_loss= 1.14443 train_rmse= 0.85124 val_loss= 1.45600 val_rmse= 1.12027 \t\ttime= 0.63331\n",
      "[*] Epoch: 0911 train_loss= 1.14403 train_rmse= 0.85179 val_loss= 1.45528 val_rmse= 1.11831 \t\ttime= 0.60139\n",
      "[*] Epoch: 0912 train_loss= 1.15353 train_rmse= 0.85226 val_loss= 1.45879 val_rmse= 1.12210 \t\ttime= 0.58145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0913 train_loss= 1.15142 train_rmse= 0.85908 val_loss= 1.46042 val_rmse= 1.12344 \t\ttime= 0.58942\n",
      "[*] Epoch: 0914 train_loss= 1.15493 train_rmse= 0.86822 val_loss= 1.45717 val_rmse= 1.11910 \t\ttime= 0.61435\n",
      "[*] Epoch: 0915 train_loss= 1.14688 train_rmse= 0.85222 val_loss= 1.45167 val_rmse= 1.11263 \t\ttime= 0.56150\n",
      "[*] Epoch: 0916 train_loss= 1.15672 train_rmse= 0.86026 val_loss= 1.44855 val_rmse= 1.10927 \t\ttime= 0.59341\n",
      "[*] Epoch: 0917 train_loss= 1.15414 train_rmse= 0.85726 val_loss= 1.44591 val_rmse= 1.10693 \t\ttime= 0.65225\n",
      "[*] Epoch: 0918 train_loss= 1.14842 train_rmse= 0.85355 val_loss= 1.44599 val_rmse= 1.10816 \t\ttime= 0.55452\n",
      "[*] Epoch: 0919 train_loss= 1.15138 train_rmse= 0.84854 val_loss= 1.45617 val_rmse= 1.12119 \t\ttime= 0.61136\n",
      "[*] Epoch: 0920 train_loss= 1.15097 train_rmse= 0.85824 val_loss= 1.46431 val_rmse= 1.13042 \t\ttime= 0.58743\n",
      "[*] Epoch: 0921 train_loss= 1.14966 train_rmse= 0.85810 val_loss= 1.46521 val_rmse= 1.12997 \t\ttime= 0.54554\n",
      "[*] Epoch: 0922 train_loss= 1.15520 train_rmse= 0.85696 val_loss= 1.46423 val_rmse= 1.12737 \t\ttime= 0.57347\n",
      "[*] Epoch: 0923 train_loss= 1.15875 train_rmse= 0.86396 val_loss= 1.45895 val_rmse= 1.12115 \t\ttime= 0.61735\n",
      "[*] Epoch: 0924 train_loss= 1.15268 train_rmse= 0.85617 val_loss= 1.45513 val_rmse= 1.11937 \t\ttime= 0.64128\n",
      "[*] Epoch: 0925 train_loss= 1.14009 train_rmse= 0.85117 val_loss= 1.45608 val_rmse= 1.12386 \t\ttime= 0.54654\n",
      "[*] Epoch: 0926 train_loss= 1.15371 train_rmse= 0.85573 val_loss= 1.45555 val_rmse= 1.12443 \t\ttime= 0.61635\n",
      "[*] Epoch: 0927 train_loss= 1.15733 train_rmse= 0.86359 val_loss= 1.44982 val_rmse= 1.11638 \t\ttime= 0.58643\n",
      "[*] Epoch: 0928 train_loss= 1.14505 train_rmse= 0.85347 val_loss= 1.44511 val_rmse= 1.10814 \t\ttime= 0.52859\n",
      "[*] Epoch: 0929 train_loss= 1.14236 train_rmse= 0.85084 val_loss= 1.44790 val_rmse= 1.10970 \t\ttime= 0.58045\n",
      "[*] Epoch: 0930 train_loss= 1.14720 train_rmse= 0.84944 val_loss= 1.45531 val_rmse= 1.11808 \t\ttime= 0.61137\n",
      "[*] Epoch: 0931 train_loss= 1.13574 train_rmse= 0.84518 val_loss= 1.46404 val_rmse= 1.12913 \t\ttime= 0.62633\n",
      "[*] Epoch: 0932 train_loss= 1.14638 train_rmse= 0.85450 val_loss= 1.46792 val_rmse= 1.13438 \t\ttime= 0.57546\n",
      "[*] Epoch: 0933 train_loss= 1.15561 train_rmse= 0.86159 val_loss= 1.46387 val_rmse= 1.12968 \t\ttime= 0.57945\n",
      "[*] Epoch: 0934 train_loss= 1.13701 train_rmse= 0.84878 val_loss= 1.45685 val_rmse= 1.12101 \t\ttime= 0.60139\n",
      "[*] Epoch: 0935 train_loss= 1.14597 train_rmse= 0.85511 val_loss= 1.45046 val_rmse= 1.11287 \t\ttime= 0.52759\n",
      "[*] Epoch: 0936 train_loss= 1.15121 train_rmse= 0.85691 val_loss= 1.44729 val_rmse= 1.10991 \t\ttime= 0.60538\n",
      "[*] Epoch: 0937 train_loss= 1.14741 train_rmse= 0.85573 val_loss= 1.44773 val_rmse= 1.11192 \t\ttime= 0.64029\n",
      "[*] Epoch: 0938 train_loss= 1.14886 train_rmse= 0.85491 val_loss= 1.45213 val_rmse= 1.11780 \t\ttime= 0.56748\n",
      "[*] Epoch: 0939 train_loss= 1.13971 train_rmse= 0.84783 val_loss= 1.45639 val_rmse= 1.12200 \t\ttime= 0.59242\n",
      "[*] Epoch: 0940 train_loss= 1.15568 train_rmse= 0.85965 val_loss= 1.45590 val_rmse= 1.12024 \t\ttime= 0.58244\n",
      "[*] Epoch: 0941 train_loss= 1.13913 train_rmse= 0.84774 val_loss= 1.45360 val_rmse= 1.11662 \t\ttime= 0.57746\n",
      "[*] Epoch: 0942 train_loss= 1.15943 train_rmse= 0.86268 val_loss= 1.44966 val_rmse= 1.11183 \t\ttime= 0.55652\n",
      "[*] Epoch: 0943 train_loss= 1.13369 train_rmse= 0.84176 val_loss= 1.45040 val_rmse= 1.11336 \t\ttime= 0.60438\n",
      "[*] Epoch: 0944 train_loss= 1.14128 train_rmse= 0.84602 val_loss= 1.45347 val_rmse= 1.11713 \t\ttime= 0.68118\n",
      "[*] Epoch: 0945 train_loss= 1.14448 train_rmse= 0.85242 val_loss= 1.45492 val_rmse= 1.11906 \t\ttime= 0.53854\n",
      "[*] Epoch: 0946 train_loss= 1.13434 train_rmse= 0.84676 val_loss= 1.45307 val_rmse= 1.11685 \t\ttime= 0.58444\n",
      "[*] Epoch: 0947 train_loss= 1.14255 train_rmse= 0.85382 val_loss= 1.45396 val_rmse= 1.11724 \t\ttime= 0.59042\n",
      "[*] Epoch: 0948 train_loss= 1.14456 train_rmse= 0.85282 val_loss= 1.45627 val_rmse= 1.11848 \t\ttime= 0.55751\n",
      "[*] Epoch: 0949 train_loss= 1.13449 train_rmse= 0.84290 val_loss= 1.45995 val_rmse= 1.12251 \t\ttime= 0.58045\n",
      "[*] Epoch: 0950 train_loss= 1.14782 train_rmse= 0.85333 val_loss= 1.46113 val_rmse= 1.12432 \t\ttime= 0.60039\n",
      "[*] Epoch: 0951 train_loss= 1.13485 train_rmse= 0.84485 val_loss= 1.46313 val_rmse= 1.12779 \t\ttime= 0.64127\n",
      "[*] Epoch: 0952 train_loss= 1.14060 train_rmse= 0.84798 val_loss= 1.46464 val_rmse= 1.13037 \t\ttime= 0.54255\n",
      "[*] Epoch: 0953 train_loss= 1.14121 train_rmse= 0.85665 val_loss= 1.46166 val_rmse= 1.12751 \t\ttime= 0.60739\n",
      "[*] Epoch: 0954 train_loss= 1.13735 train_rmse= 0.84999 val_loss= 1.45506 val_rmse= 1.11944 \t\ttime= 0.58444\n",
      "[*] Epoch: 0955 train_loss= 1.12858 train_rmse= 0.83976 val_loss= 1.45309 val_rmse= 1.11677 \t\ttime= 0.53058\n",
      "[*] Epoch: 0956 train_loss= 1.14517 train_rmse= 0.85485 val_loss= 1.45349 val_rmse= 1.11649 \t\ttime= 0.62234\n",
      "[*] Epoch: 0957 train_loss= 1.13432 train_rmse= 0.84641 val_loss= 1.45358 val_rmse= 1.11554 \t\ttime= 0.68717\n",
      "[*] Epoch: 0958 train_loss= 1.13054 train_rmse= 0.83902 val_loss= 1.45920 val_rmse= 1.12199 \t\ttime= 0.62134\n",
      "[*] Epoch: 0959 train_loss= 1.14116 train_rmse= 0.85013 val_loss= 1.46371 val_rmse= 1.12755 \t\ttime= 0.65325\n",
      "[*] Epoch: 0960 train_loss= 1.13843 train_rmse= 0.84577 val_loss= 1.46700 val_rmse= 1.13119 \t\ttime= 0.63331\n",
      "[*] Epoch: 0961 train_loss= 1.15007 train_rmse= 0.86140 val_loss= 1.46528 val_rmse= 1.12863 \t\ttime= 0.64527\n",
      "[*] Epoch: 0962 train_loss= 1.13213 train_rmse= 0.84396 val_loss= 1.46064 val_rmse= 1.12347 \t\ttime= 0.63231\n",
      "[*] Epoch: 0963 train_loss= 1.14450 train_rmse= 0.85338 val_loss= 1.45550 val_rmse= 1.11811 \t\ttime= 0.61037\n",
      "[*] Epoch: 0964 train_loss= 1.14932 train_rmse= 0.85829 val_loss= 1.45001 val_rmse= 1.11312 \t\ttime= 0.64727\n",
      "[*] Epoch: 0965 train_loss= 1.12845 train_rmse= 0.84089 val_loss= 1.44647 val_rmse= 1.11037 \t\ttime= 0.59940\n",
      "[*] Epoch: 0966 train_loss= 1.13661 train_rmse= 0.84478 val_loss= 1.44701 val_rmse= 1.11243 \t\ttime= 0.59041\n",
      "[*] Epoch: 0967 train_loss= 1.14923 train_rmse= 0.84821 val_loss= 1.45279 val_rmse= 1.11776 \t\ttime= 0.61934\n",
      "[*] Epoch: 0968 train_loss= 1.13925 train_rmse= 0.84514 val_loss= 1.45888 val_rmse= 1.12240 \t\ttime= 0.54355\n",
      "[*] Epoch: 0969 train_loss= 1.14299 train_rmse= 0.85341 val_loss= 1.46075 val_rmse= 1.12201 \t\ttime= 0.64228\n",
      "[*] Epoch: 0970 train_loss= 1.13781 train_rmse= 0.84559 val_loss= 1.46098 val_rmse= 1.12215 \t\ttime= 0.62832\n",
      "[*] Epoch: 0971 train_loss= 1.14636 train_rmse= 0.85263 val_loss= 1.45869 val_rmse= 1.12112 \t\ttime= 0.56748\n",
      "[*] Epoch: 0972 train_loss= 1.15070 train_rmse= 0.85776 val_loss= 1.45662 val_rmse= 1.12142 \t\ttime= 0.58144\n",
      "[*] Epoch: 0973 train_loss= 1.12860 train_rmse= 0.84264 val_loss= 1.45519 val_rmse= 1.12153 \t\ttime= 0.61635\n",
      "[*] Epoch: 0974 train_loss= 1.14034 train_rmse= 0.84922 val_loss= 1.45235 val_rmse= 1.11768 \t\ttime= 0.58143\n",
      "[*] Epoch: 0975 train_loss= 1.13538 train_rmse= 0.84614 val_loss= 1.45086 val_rmse= 1.11468 \t\ttime= 0.55551\n",
      "[*] Epoch: 0976 train_loss= 1.14695 train_rmse= 0.85628 val_loss= 1.44889 val_rmse= 1.10991 \t\ttime= 0.57845\n",
      "[*] Epoch: 0977 train_loss= 1.13198 train_rmse= 0.84578 val_loss= 1.44874 val_rmse= 1.10838 \t\ttime= 0.68816\n",
      "[*] Epoch: 0978 train_loss= 1.11978 train_rmse= 0.83577 val_loss= 1.45115 val_rmse= 1.11189 \t\ttime= 0.54554\n",
      "[*] Epoch: 0979 train_loss= 1.13028 train_rmse= 0.84224 val_loss= 1.45590 val_rmse= 1.11937 \t\ttime= 0.57746\n",
      "[*] Epoch: 0980 train_loss= 1.13347 train_rmse= 0.84722 val_loss= 1.45911 val_rmse= 1.12405 \t\ttime= 0.58942\n",
      "[*] Epoch: 0981 train_loss= 1.12400 train_rmse= 0.83785 val_loss= 1.46054 val_rmse= 1.12566 \t\ttime= 0.54653\n",
      "[*] Epoch: 0982 train_loss= 1.13150 train_rmse= 0.84160 val_loss= 1.46048 val_rmse= 1.12444 \t\ttime= 0.61037\n",
      "[*] Epoch: 0983 train_loss= 1.12477 train_rmse= 0.84194 val_loss= 1.45697 val_rmse= 1.11904 \t\ttime= 0.60139\n",
      "[*] Epoch: 0984 train_loss= 1.13273 train_rmse= 0.84338 val_loss= 1.45215 val_rmse= 1.11297 \t\ttime= 0.63829\n",
      "[*] Epoch: 0985 train_loss= 1.13765 train_rmse= 0.84834 val_loss= 1.44741 val_rmse= 1.10811 \t\ttime= 0.56449\n",
      "[*] Epoch: 0986 train_loss= 1.14112 train_rmse= 0.85238 val_loss= 1.44734 val_rmse= 1.10938 \t\ttime= 0.60737\n",
      "[*] Epoch: 0987 train_loss= 1.14488 train_rmse= 0.84982 val_loss= 1.45103 val_rmse= 1.11476 \t\ttime= 0.58743\n",
      "[*] Epoch: 0988 train_loss= 1.13287 train_rmse= 0.84759 val_loss= 1.45529 val_rmse= 1.12038 \t\ttime= 0.54055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 0989 train_loss= 1.12619 train_rmse= 0.84009 val_loss= 1.45899 val_rmse= 1.12435 \t\ttime= 0.58942\n",
      "[*] Epoch: 0990 train_loss= 1.13873 train_rmse= 0.84787 val_loss= 1.45950 val_rmse= 1.12286 \t\ttime= 0.63632\n",
      "[*] Epoch: 0991 train_loss= 1.13294 train_rmse= 0.84119 val_loss= 1.45988 val_rmse= 1.12121 \t\ttime= 0.58144\n",
      "[*] Epoch: 0992 train_loss= 1.13766 train_rmse= 0.84418 val_loss= 1.46054 val_rmse= 1.12226 \t\ttime= 0.59242\n",
      "[*] Epoch: 0993 train_loss= 1.13023 train_rmse= 0.84058 val_loss= 1.45955 val_rmse= 1.12276 \t\ttime= 0.58743\n",
      "[*] Epoch: 0994 train_loss= 1.12431 train_rmse= 0.83708 val_loss= 1.45865 val_rmse= 1.12367 \t\ttime= 0.58842\n",
      "[*] Epoch: 0995 train_loss= 1.13064 train_rmse= 0.84384 val_loss= 1.45646 val_rmse= 1.12276 \t\ttime= 0.55551\n",
      "[*] Epoch: 0996 train_loss= 1.14199 train_rmse= 0.85287 val_loss= 1.45160 val_rmse= 1.11731 \t\ttime= 0.58145\n",
      "[*] Epoch: 0997 train_loss= 1.12432 train_rmse= 0.84069 val_loss= 1.45025 val_rmse= 1.11508 \t\ttime= 0.62832\n",
      "[*] Epoch: 0998 train_loss= 1.12564 train_rmse= 0.84328 val_loss= 1.44977 val_rmse= 1.11310 \t\ttime= 0.57247\n",
      "[*] Epoch: 0999 train_loss= 1.12464 train_rmse= 0.83837 val_loss= 1.45332 val_rmse= 1.11568 \t\ttime= 0.60139\n",
      "[*] Epoch: 1000 train_loss= 1.12765 train_rmse= 0.84123 val_loss= 1.45849 val_rmse= 1.12002 \t\ttime= 0.58843\n",
      "[*] Epoch: 1001 train_loss= 1.12644 train_rmse= 0.83926 val_loss= 1.46277 val_rmse= 1.12465 \t\ttime= 0.54654\n",
      "[*] Epoch: 1002 train_loss= 1.14077 train_rmse= 0.84877 val_loss= 1.46169 val_rmse= 1.12439 \t\ttime= 0.57048\n",
      "[*] Epoch: 1003 train_loss= 1.14164 train_rmse= 0.85290 val_loss= 1.45539 val_rmse= 1.11878 \t\ttime= 0.59840\n",
      "[*] Epoch: 1004 train_loss= 1.13074 train_rmse= 0.83983 val_loss= 1.45134 val_rmse= 1.11523 \t\ttime= 0.65126\n",
      "[*] Epoch: 1005 train_loss= 1.13597 train_rmse= 0.84557 val_loss= 1.44866 val_rmse= 1.11155 \t\ttime= 0.55352\n",
      "[*] Epoch: 1006 train_loss= 1.13964 train_rmse= 0.84771 val_loss= 1.45142 val_rmse= 1.11462 \t\ttime= 0.58045\n",
      "[*] Epoch: 1007 train_loss= 1.12325 train_rmse= 0.83956 val_loss= 1.45391 val_rmse= 1.11758 \t\ttime= 0.60938\n",
      "[*] Epoch: 1008 train_loss= 1.12616 train_rmse= 0.83794 val_loss= 1.45659 val_rmse= 1.11992 \t\ttime= 0.52859\n",
      "[*] Epoch: 1009 train_loss= 1.12913 train_rmse= 0.84441 val_loss= 1.45755 val_rmse= 1.11992 \t\ttime= 0.60139\n",
      "[*] Epoch: 1010 train_loss= 1.12531 train_rmse= 0.84429 val_loss= 1.45583 val_rmse= 1.11673 \t\ttime= 0.60239\n",
      "[*] Epoch: 1011 train_loss= 1.12327 train_rmse= 0.83931 val_loss= 1.45482 val_rmse= 1.11497 \t\ttime= 0.61834\n",
      "[*] Epoch: 1012 train_loss= 1.12327 train_rmse= 0.83662 val_loss= 1.45663 val_rmse= 1.11751 \t\ttime= 0.57147\n",
      "[*] Epoch: 1013 train_loss= 1.12650 train_rmse= 0.84100 val_loss= 1.45635 val_rmse= 1.11767 \t\ttime= 0.58942\n",
      "[*] Epoch: 1014 train_loss= 1.12608 train_rmse= 0.84027 val_loss= 1.46193 val_rmse= 1.12497 \t\ttime= 0.58943\n",
      "[*] Epoch: 1015 train_loss= 1.12662 train_rmse= 0.84440 val_loss= 1.46479 val_rmse= 1.12882 \t\ttime= 0.53856\n",
      "[*] Epoch: 1016 train_loss= 1.13613 train_rmse= 0.84820 val_loss= 1.46251 val_rmse= 1.12529 \t\ttime= 0.61834\n",
      "[*] Epoch: 1017 train_loss= 1.13010 train_rmse= 0.84323 val_loss= 1.45738 val_rmse= 1.11802 \t\ttime= 0.60438\n",
      "[*] Epoch: 1018 train_loss= 1.12462 train_rmse= 0.83869 val_loss= 1.45571 val_rmse= 1.11599 \t\ttime= 0.59640\n",
      "[*] Epoch: 1019 train_loss= 1.14237 train_rmse= 0.85349 val_loss= 1.45322 val_rmse= 1.11379 \t\ttime= 0.58643\n",
      "[*] Epoch: 1020 train_loss= 1.12930 train_rmse= 0.84163 val_loss= 1.45241 val_rmse= 1.11399 \t\ttime= 0.58942\n",
      "[*] Epoch: 1021 train_loss= 1.13089 train_rmse= 0.84229 val_loss= 1.45176 val_rmse= 1.11361 \t\ttime= 0.57446\n",
      "[*] Epoch: 1022 train_loss= 1.12103 train_rmse= 0.83636 val_loss= 1.45060 val_rmse= 1.11166 \t\ttime= 0.53956\n",
      "[*] Epoch: 1023 train_loss= 1.13078 train_rmse= 0.84431 val_loss= 1.45165 val_rmse= 1.11214 \t\ttime= 0.59242\n",
      "[*] Epoch: 1024 train_loss= 1.12694 train_rmse= 0.83738 val_loss= 1.45432 val_rmse= 1.11503 \t\ttime= 0.66722\n",
      "[*] Epoch: 1025 train_loss= 1.13074 train_rmse= 0.84519 val_loss= 1.45787 val_rmse= 1.11879 \t\ttime= 0.56050\n",
      "[*] Epoch: 1026 train_loss= 1.11949 train_rmse= 0.83318 val_loss= 1.46114 val_rmse= 1.12228 \t\ttime= 0.59442\n",
      "[*] Epoch: 1027 train_loss= 1.12571 train_rmse= 0.84042 val_loss= 1.46415 val_rmse= 1.12667 \t\ttime= 0.58145\n",
      "[*] Epoch: 1028 train_loss= 1.11854 train_rmse= 0.83340 val_loss= 1.46663 val_rmse= 1.13002 \t\ttime= 0.57147\n",
      "[*] Epoch: 1029 train_loss= 1.13149 train_rmse= 0.84622 val_loss= 1.46471 val_rmse= 1.12818 \t\ttime= 0.55950\n",
      "[*] Epoch: 1030 train_loss= 1.13437 train_rmse= 0.84961 val_loss= 1.46096 val_rmse= 1.12353 \t\ttime= 0.59341\n",
      "[*] Epoch: 1031 train_loss= 1.11965 train_rmse= 0.84010 val_loss= 1.45649 val_rmse= 1.11873 \t\ttime= 0.64727\n",
      "[*] Epoch: 1032 train_loss= 1.11937 train_rmse= 0.83388 val_loss= 1.45440 val_rmse= 1.11756 \t\ttime= 0.55950\n",
      "[*] Epoch: 1033 train_loss= 1.12559 train_rmse= 0.83645 val_loss= 1.45503 val_rmse= 1.11813 \t\ttime= 0.59840\n",
      "[*] Epoch: 1034 train_loss= 1.12840 train_rmse= 0.84671 val_loss= 1.45628 val_rmse= 1.11959 \t\ttime= 0.59142\n",
      "[*] Epoch: 1035 train_loss= 1.11766 train_rmse= 0.83307 val_loss= 1.45871 val_rmse= 1.12040 \t\ttime= 0.53557\n",
      "[*] Epoch: 1036 train_loss= 1.11568 train_rmse= 0.83284 val_loss= 1.46138 val_rmse= 1.12246 \t\ttime= 0.60039\n",
      "[*] Epoch: 1037 train_loss= 1.11236 train_rmse= 0.83048 val_loss= 1.46572 val_rmse= 1.12770 \t\ttime= 0.60738\n",
      "[*] Epoch: 1038 train_loss= 1.12953 train_rmse= 0.83652 val_loss= 1.46679 val_rmse= 1.12981 \t\ttime= 0.61734\n",
      "[*] Epoch: 1039 train_loss= 1.12519 train_rmse= 0.83919 val_loss= 1.46692 val_rmse= 1.12928 \t\ttime= 0.59840\n",
      "[*] Epoch: 1040 train_loss= 1.11616 train_rmse= 0.83207 val_loss= 1.46513 val_rmse= 1.12599 \t\ttime= 0.60738\n",
      "[*] Epoch: 1041 train_loss= 1.12800 train_rmse= 0.84580 val_loss= 1.46257 val_rmse= 1.12247 \t\ttime= 0.61137\n",
      "[*] Epoch: 1042 train_loss= 1.14271 train_rmse= 0.85124 val_loss= 1.46118 val_rmse= 1.12120 \t\ttime= 0.53357\n",
      "[*] Epoch: 1043 train_loss= 1.12174 train_rmse= 0.83630 val_loss= 1.46173 val_rmse= 1.12320 \t\ttime= 0.62234\n",
      "[*] Epoch: 1044 train_loss= 1.11914 train_rmse= 0.83527 val_loss= 1.46319 val_rmse= 1.12614 \t\ttime= 0.61136\n",
      "[*] Epoch: 1045 train_loss= 1.12945 train_rmse= 0.84285 val_loss= 1.46083 val_rmse= 1.12324 \t\ttime= 0.61336\n",
      "[*] Epoch: 1046 train_loss= 1.11997 train_rmse= 0.83127 val_loss= 1.46151 val_rmse= 1.12302 \t\ttime= 0.60139\n",
      "[*] Epoch: 1047 train_loss= 1.12391 train_rmse= 0.83354 val_loss= 1.46451 val_rmse= 1.12523 \t\ttime= 0.62134\n",
      "[*] Epoch: 1048 train_loss= 1.12603 train_rmse= 0.83737 val_loss= 1.46550 val_rmse= 1.12613 \t\ttime= 0.58743\n",
      "[*] Epoch: 1049 train_loss= 1.12522 train_rmse= 0.84034 val_loss= 1.46285 val_rmse= 1.12375 \t\ttime= 0.62034\n",
      "[*] Epoch: 1050 train_loss= 1.13321 train_rmse= 0.84947 val_loss= 1.45900 val_rmse= 1.11973 \t\ttime= 0.57945\n",
      "[*] Epoch: 1051 train_loss= 1.11486 train_rmse= 0.83713 val_loss= 1.45584 val_rmse= 1.11690 \t\ttime= 0.64328\n",
      "[*] Epoch: 1052 train_loss= 1.11727 train_rmse= 0.83638 val_loss= 1.45353 val_rmse= 1.11448 \t\ttime= 0.53259\n",
      "[*] Epoch: 1053 train_loss= 1.11286 train_rmse= 0.82664 val_loss= 1.45610 val_rmse= 1.11760 \t\ttime= 0.60638\n",
      "[*] Epoch: 1054 train_loss= 1.12130 train_rmse= 0.83493 val_loss= 1.46158 val_rmse= 1.12351 \t\ttime= 0.59242\n",
      "[*] Epoch: 1055 train_loss= 1.11862 train_rmse= 0.83410 val_loss= 1.46572 val_rmse= 1.12832 \t\ttime= 0.53059\n",
      "[*] Epoch: 1056 train_loss= 1.13012 train_rmse= 0.84311 val_loss= 1.46678 val_rmse= 1.12873 \t\ttime= 0.60937\n",
      "[*] Epoch: 1057 train_loss= 1.11383 train_rmse= 0.82983 val_loss= 1.46675 val_rmse= 1.12867 \t\ttime= 0.64128\n",
      "[*] Epoch: 1058 train_loss= 1.12573 train_rmse= 0.83958 val_loss= 1.46390 val_rmse= 1.12535 \t\ttime= 0.61635\n",
      "[*] Epoch: 1059 train_loss= 1.11137 train_rmse= 0.82748 val_loss= 1.46151 val_rmse= 1.12322 \t\ttime= 0.54853\n",
      "[*] Epoch: 1060 train_loss= 1.12183 train_rmse= 0.84364 val_loss= 1.45696 val_rmse= 1.11899 \t\ttime= 0.57247\n",
      "[*] Epoch: 1061 train_loss= 1.11823 train_rmse= 0.83198 val_loss= 1.45518 val_rmse= 1.11714 \t\ttime= 0.58443\n",
      "[*] Epoch: 1062 train_loss= 1.13063 train_rmse= 0.84488 val_loss= 1.45219 val_rmse= 1.11332 \t\ttime= 0.53058\n",
      "[*] Epoch: 1063 train_loss= 1.11643 train_rmse= 0.83101 val_loss= 1.45352 val_rmse= 1.11449 \t\ttime= 0.59840\n",
      "[*] Epoch: 1064 train_loss= 1.12533 train_rmse= 0.83830 val_loss= 1.45675 val_rmse= 1.11797 \t\ttime= 0.60937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1065 train_loss= 1.11362 train_rmse= 0.83249 val_loss= 1.46071 val_rmse= 1.12263 \t\ttime= 0.62832\n",
      "[*] Epoch: 1066 train_loss= 1.11365 train_rmse= 0.83105 val_loss= 1.46597 val_rmse= 1.12807 \t\ttime= 0.61037\n",
      "[*] Epoch: 1067 train_loss= 1.12014 train_rmse= 0.83712 val_loss= 1.46814 val_rmse= 1.13022 \t\ttime= 0.59341\n",
      "[*] Epoch: 1068 train_loss= 1.11311 train_rmse= 0.83106 val_loss= 1.46939 val_rmse= 1.13090 \t\ttime= 0.57945\n",
      "[*] Epoch: 1069 train_loss= 1.11008 train_rmse= 0.83209 val_loss= 1.46773 val_rmse= 1.12927 \t\ttime= 0.54854\n",
      "[*] Epoch: 1070 train_loss= 1.13135 train_rmse= 0.84317 val_loss= 1.46264 val_rmse= 1.12456 \t\ttime= 0.60139\n",
      "[*] Epoch: 1071 train_loss= 1.12044 train_rmse= 0.83813 val_loss= 1.45599 val_rmse= 1.11799 \t\ttime= 0.70512\n",
      "[*] Epoch: 1072 train_loss= 1.12230 train_rmse= 0.83146 val_loss= 1.45367 val_rmse= 1.11585 \t\ttime= 0.53058\n",
      "[*] Epoch: 1073 train_loss= 1.11457 train_rmse= 0.83124 val_loss= 1.45640 val_rmse= 1.11834 \t\ttime= 0.60438\n",
      "[*] Epoch: 1074 train_loss= 1.12298 train_rmse= 0.83811 val_loss= 1.45785 val_rmse= 1.11899 \t\ttime= 0.61735\n",
      "[*] Epoch: 1075 train_loss= 1.12361 train_rmse= 0.83801 val_loss= 1.46251 val_rmse= 1.12411 \t\ttime= 0.54554\n",
      "[*] Epoch: 1076 train_loss= 1.11165 train_rmse= 0.82536 val_loss= 1.46737 val_rmse= 1.13035 \t\ttime= 0.58543\n",
      "[*] Epoch: 1077 train_loss= 1.12439 train_rmse= 0.83988 val_loss= 1.46780 val_rmse= 1.13193 \t\ttime= 0.59441\n",
      "[*] Epoch: 1078 train_loss= 1.12111 train_rmse= 0.83998 val_loss= 1.46429 val_rmse= 1.12834 \t\ttime= 0.64527\n",
      "[*] Epoch: 1079 train_loss= 1.11607 train_rmse= 0.83119 val_loss= 1.46011 val_rmse= 1.12334 \t\ttime= 0.55552\n",
      "[*] Epoch: 1080 train_loss= 1.11876 train_rmse= 0.83387 val_loss= 1.45966 val_rmse= 1.12294 \t\ttime= 0.57945\n",
      "[*] Epoch: 1081 train_loss= 1.11184 train_rmse= 0.83242 val_loss= 1.45784 val_rmse= 1.12036 \t\ttime= 0.57048\n",
      "[*] Epoch: 1082 train_loss= 1.10913 train_rmse= 0.83022 val_loss= 1.45738 val_rmse= 1.11862 \t\ttime= 0.56846\n",
      "[*] Epoch: 1083 train_loss= 1.10916 train_rmse= 0.82668 val_loss= 1.45755 val_rmse= 1.11806 \t\ttime= 0.62633\n",
      "[*] Epoch: 1084 train_loss= 1.11316 train_rmse= 0.83211 val_loss= 1.45553 val_rmse= 1.11540 \t\ttime= 0.60638\n",
      "[*] Epoch: 1085 train_loss= 1.10782 train_rmse= 0.82746 val_loss= 1.45632 val_rmse= 1.11692 \t\ttime= 0.59541\n",
      "[*] Epoch: 1086 train_loss= 1.12023 train_rmse= 0.83405 val_loss= 1.46371 val_rmse= 1.12585 \t\ttime= 0.59042\n",
      "[*] Epoch: 1087 train_loss= 1.11290 train_rmse= 0.82591 val_loss= 1.47009 val_rmse= 1.13277 \t\ttime= 0.58244\n",
      "[*] Epoch: 1088 train_loss= 1.11899 train_rmse= 0.83630 val_loss= 1.47094 val_rmse= 1.13308 \t\ttime= 0.58244\n",
      "[*] Epoch: 1089 train_loss= 1.12280 train_rmse= 0.84406 val_loss= 1.46741 val_rmse= 1.12844 \t\ttime= 0.53557\n",
      "[*] Epoch: 1090 train_loss= 1.11387 train_rmse= 0.83193 val_loss= 1.46286 val_rmse= 1.12441 \t\ttime= 0.60139\n",
      "[*] Epoch: 1091 train_loss= 1.11238 train_rmse= 0.83254 val_loss= 1.45798 val_rmse= 1.12039 \t\ttime= 0.65425\n",
      "[*] Epoch: 1092 train_loss= 1.10100 train_rmse= 0.82368 val_loss= 1.45611 val_rmse= 1.11947 \t\ttime= 0.56748\n",
      "[*] Epoch: 1093 train_loss= 1.10845 train_rmse= 0.82896 val_loss= 1.45370 val_rmse= 1.11687 \t\ttime= 0.58344\n",
      "[*] Epoch: 1094 train_loss= 1.12513 train_rmse= 0.83345 val_loss= 1.45632 val_rmse= 1.11878 \t\ttime= 0.57446\n",
      "[*] Epoch: 1095 train_loss= 1.11205 train_rmse= 0.82970 val_loss= 1.45816 val_rmse= 1.11945 \t\ttime= 0.58943\n",
      "[*] Epoch: 1096 train_loss= 1.11208 train_rmse= 0.83308 val_loss= 1.46044 val_rmse= 1.12100 \t\ttime= 0.55352\n",
      "[*] Epoch: 1097 train_loss= 1.11223 train_rmse= 0.83415 val_loss= 1.46492 val_rmse= 1.12642 \t\ttime= 0.61636\n",
      "[*] Epoch: 1098 train_loss= 1.11543 train_rmse= 0.83277 val_loss= 1.46644 val_rmse= 1.12943 \t\ttime= 0.70013\n",
      "[*] Epoch: 1099 train_loss= 1.11895 train_rmse= 0.83433 val_loss= 1.46599 val_rmse= 1.12961 \t\ttime= 0.56050\n",
      "[*] Epoch: 1100 train_loss= 1.10468 train_rmse= 0.82611 val_loss= 1.46446 val_rmse= 1.12789 \t\ttime= 0.61237\n",
      "[*] Epoch: 1101 train_loss= 1.11870 train_rmse= 0.83879 val_loss= 1.46156 val_rmse= 1.12412 \t\ttime= 0.58643\n",
      "[*] Epoch: 1102 train_loss= 1.11099 train_rmse= 0.83275 val_loss= 1.45986 val_rmse= 1.12173 \t\ttime= 0.53458\n",
      "[*] Epoch: 1103 train_loss= 1.11120 train_rmse= 0.82899 val_loss= 1.46051 val_rmse= 1.12242 \t\ttime= 0.59042\n",
      "[*] Epoch: 1104 train_loss= 1.10618 train_rmse= 0.83032 val_loss= 1.46178 val_rmse= 1.12359 \t\ttime= 0.61036\n",
      "[*] Epoch: 1105 train_loss= 1.11777 train_rmse= 0.83292 val_loss= 1.46188 val_rmse= 1.12338 \t\ttime= 0.61436\n",
      "[*] Epoch: 1106 train_loss= 1.11194 train_rmse= 0.82706 val_loss= 1.46286 val_rmse= 1.12342 \t\ttime= 0.59242\n",
      "[*] Epoch: 1107 train_loss= 1.11144 train_rmse= 0.83022 val_loss= 1.46374 val_rmse= 1.12404 \t\ttime= 0.56848\n",
      "[*] Epoch: 1108 train_loss= 1.12095 train_rmse= 0.83992 val_loss= 1.46182 val_rmse= 1.12236 \t\ttime= 0.59242\n",
      "[*] Epoch: 1109 train_loss= 1.10303 train_rmse= 0.82407 val_loss= 1.45935 val_rmse= 1.12051 \t\ttime= 0.53856\n",
      "[*] Epoch: 1110 train_loss= 1.12234 train_rmse= 0.83701 val_loss= 1.45778 val_rmse= 1.12055 \t\ttime= 0.59940\n",
      "[*] Epoch: 1111 train_loss= 1.11405 train_rmse= 0.83146 val_loss= 1.45692 val_rmse= 1.12049 \t\ttime= 0.63829\n",
      "[*] Epoch: 1112 train_loss= 1.10395 train_rmse= 0.82420 val_loss= 1.45672 val_rmse= 1.11977 \t\ttime= 0.58344\n",
      "[*] Epoch: 1113 train_loss= 1.10307 train_rmse= 0.82611 val_loss= 1.45812 val_rmse= 1.12044 \t\ttime= 0.59142\n",
      "[*] Epoch: 1114 train_loss= 1.11403 train_rmse= 0.83249 val_loss= 1.45937 val_rmse= 1.12216 \t\ttime= 0.60538\n",
      "[*] Epoch: 1115 train_loss= 1.10470 train_rmse= 0.82689 val_loss= 1.45885 val_rmse= 1.12160 \t\ttime= 0.58344\n",
      "[*] Epoch: 1116 train_loss= 1.11838 train_rmse= 0.84428 val_loss= 1.45629 val_rmse= 1.11752 \t\ttime= 0.55751\n",
      "[*] Epoch: 1117 train_loss= 1.11357 train_rmse= 0.83233 val_loss= 1.45552 val_rmse= 1.11723 \t\ttime= 0.58643\n",
      "[*] Epoch: 1118 train_loss= 1.11145 train_rmse= 0.83413 val_loss= 1.45584 val_rmse= 1.11799 \t\ttime= 0.66422\n",
      "[*] Epoch: 1119 train_loss= 1.10531 train_rmse= 0.82788 val_loss= 1.45512 val_rmse= 1.11790 \t\ttime= 0.54554\n",
      "[*] Epoch: 1120 train_loss= 1.10129 train_rmse= 0.82405 val_loss= 1.45610 val_rmse= 1.11914 \t\ttime= 0.59940\n",
      "[*] Epoch: 1121 train_loss= 1.11020 train_rmse= 0.82727 val_loss= 1.45666 val_rmse= 1.11876 \t\ttime= 0.58743\n",
      "[*] Epoch: 1122 train_loss= 1.09931 train_rmse= 0.81969 val_loss= 1.46020 val_rmse= 1.12173 \t\ttime= 0.54355\n",
      "[*] Epoch: 1123 train_loss= 1.11146 train_rmse= 0.83157 val_loss= 1.46273 val_rmse= 1.12390 \t\ttime= 0.59940\n",
      "[*] Epoch: 1124 train_loss= 1.10787 train_rmse= 0.82814 val_loss= 1.46216 val_rmse= 1.12328 \t\ttime= 0.60438\n",
      "[*] Epoch: 1125 train_loss= 1.10633 train_rmse= 0.82300 val_loss= 1.46250 val_rmse= 1.12426 \t\ttime= 0.66023\n",
      "[*] Epoch: 1126 train_loss= 1.10865 train_rmse= 0.82747 val_loss= 1.46229 val_rmse= 1.12439 \t\ttime= 0.57546\n",
      "[*] Epoch: 1127 train_loss= 1.10956 train_rmse= 0.83118 val_loss= 1.46201 val_rmse= 1.12478 \t\ttime= 0.60039\n",
      "[*] Epoch: 1128 train_loss= 1.11090 train_rmse= 0.83134 val_loss= 1.46157 val_rmse= 1.12354 \t\ttime= 0.58843\n",
      "[*] Epoch: 1129 train_loss= 1.11169 train_rmse= 0.82753 val_loss= 1.46072 val_rmse= 1.12123 \t\ttime= 0.55153\n",
      "[*] Epoch: 1130 train_loss= 1.10988 train_rmse= 0.82395 val_loss= 1.46444 val_rmse= 1.12494 \t\ttime= 0.61535\n",
      "[*] Epoch: 1131 train_loss= 1.10620 train_rmse= 0.82577 val_loss= 1.46752 val_rmse= 1.12834 \t\ttime= 0.73106\n",
      "[*] Epoch: 1132 train_loss= 1.10314 train_rmse= 0.82277 val_loss= 1.46925 val_rmse= 1.13028 \t\ttime= 0.62433\n",
      "[*] Epoch: 1133 train_loss= 1.10360 train_rmse= 0.82917 val_loss= 1.46796 val_rmse= 1.12933 \t\ttime= 0.60738\n",
      "[*] Epoch: 1134 train_loss= 1.11212 train_rmse= 0.83424 val_loss= 1.46210 val_rmse= 1.12298 \t\ttime= 0.59740\n",
      "[*] Epoch: 1135 train_loss= 1.10317 train_rmse= 0.82223 val_loss= 1.45662 val_rmse= 1.11662 \t\ttime= 0.57745\n",
      "[*] Epoch: 1136 train_loss= 1.10055 train_rmse= 0.82453 val_loss= 1.45500 val_rmse= 1.11512 \t\ttime= 0.59042\n",
      "[*] Epoch: 1137 train_loss= 1.10825 train_rmse= 0.83174 val_loss= 1.45462 val_rmse= 1.11445 \t\ttime= 0.61835\n",
      "[*] Epoch: 1138 train_loss= 1.09299 train_rmse= 0.81719 val_loss= 1.46072 val_rmse= 1.12156 \t\ttime= 0.67320\n",
      "[*] Epoch: 1139 train_loss= 1.11483 train_rmse= 0.83507 val_loss= 1.46456 val_rmse= 1.12551 \t\ttime= 0.60040\n",
      "[*] Epoch: 1140 train_loss= 1.09301 train_rmse= 0.82057 val_loss= 1.46510 val_rmse= 1.12545 \t\ttime= 0.61536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1141 train_loss= 1.10582 train_rmse= 0.82737 val_loss= 1.46422 val_rmse= 1.12386 \t\ttime= 0.62133\n",
      "[*] Epoch: 1142 train_loss= 1.10512 train_rmse= 0.82589 val_loss= 1.46176 val_rmse= 1.12131 \t\ttime= 0.55152\n",
      "[*] Epoch: 1143 train_loss= 1.11363 train_rmse= 0.83101 val_loss= 1.45986 val_rmse= 1.12030 \t\ttime= 0.59042\n",
      "[*] Epoch: 1144 train_loss= 1.10100 train_rmse= 0.82319 val_loss= 1.45994 val_rmse= 1.12170 \t\ttime= 0.63929\n",
      "[*] Epoch: 1145 train_loss= 1.09673 train_rmse= 0.82287 val_loss= 1.46175 val_rmse= 1.12465 \t\ttime= 0.57147\n",
      "[*] Epoch: 1146 train_loss= 1.10563 train_rmse= 0.82946 val_loss= 1.46149 val_rmse= 1.12409 \t\ttime= 0.62831\n",
      "[*] Epoch: 1147 train_loss= 1.08676 train_rmse= 0.80949 val_loss= 1.46432 val_rmse= 1.12660 \t\ttime= 0.62134\n",
      "[*] Epoch: 1148 train_loss= 1.10622 train_rmse= 0.82797 val_loss= 1.46484 val_rmse= 1.12568 \t\ttime= 0.55651\n",
      "[*] Epoch: 1149 train_loss= 1.10025 train_rmse= 0.82466 val_loss= 1.46347 val_rmse= 1.12297 \t\ttime= 0.56748\n",
      "[*] Epoch: 1150 train_loss= 1.10806 train_rmse= 0.82812 val_loss= 1.46114 val_rmse= 1.12036 \t\ttime= 0.59541\n",
      "[*] Epoch: 1151 train_loss= 1.09917 train_rmse= 0.82197 val_loss= 1.45911 val_rmse= 1.11889 \t\ttime= 0.69315\n",
      "[*] Epoch: 1152 train_loss= 1.10091 train_rmse= 0.82060 val_loss= 1.46202 val_rmse= 1.12406 \t\ttime= 0.53856\n",
      "[*] Epoch: 1153 train_loss= 1.09027 train_rmse= 0.81537 val_loss= 1.46775 val_rmse= 1.13216 \t\ttime= 0.64228\n",
      "[*] Epoch: 1154 train_loss= 1.10803 train_rmse= 0.82883 val_loss= 1.47122 val_rmse= 1.13660 \t\ttime= 0.60538\n",
      "[*] Epoch: 1155 train_loss= 1.11262 train_rmse= 0.83708 val_loss= 1.46982 val_rmse= 1.13382 \t\ttime= 0.54455\n",
      "[*] Epoch: 1156 train_loss= 1.11685 train_rmse= 0.83676 val_loss= 1.46337 val_rmse= 1.12406 \t\ttime= 0.61536\n",
      "[*] Epoch: 1157 train_loss= 1.10146 train_rmse= 0.82298 val_loss= 1.45712 val_rmse= 1.11555 \t\ttime= 0.65425\n",
      "[*] Epoch: 1158 train_loss= 1.11593 train_rmse= 0.83469 val_loss= 1.45163 val_rmse= 1.10916 \t\ttime= 0.59541\n",
      "[*] Epoch: 1159 train_loss= 1.10806 train_rmse= 0.82739 val_loss= 1.45376 val_rmse= 1.11361 \t\ttime= 0.61835\n",
      "[*] Epoch: 1160 train_loss= 1.10521 train_rmse= 0.82812 val_loss= 1.45813 val_rmse= 1.12013 \t\ttime= 0.62234\n",
      "[*] Epoch: 1161 train_loss= 1.10097 train_rmse= 0.82746 val_loss= 1.46132 val_rmse= 1.12380 \t\ttime= 0.59740\n",
      "[*] Epoch: 1162 train_loss= 1.10606 train_rmse= 0.82698 val_loss= 1.46835 val_rmse= 1.13174 \t\ttime= 0.57247\n",
      "[*] Epoch: 1163 train_loss= 1.09723 train_rmse= 0.82575 val_loss= 1.47022 val_rmse= 1.13286 \t\ttime= 0.61635\n",
      "[*] Epoch: 1164 train_loss= 1.10233 train_rmse= 0.82753 val_loss= 1.47049 val_rmse= 1.13222 \t\ttime= 0.63929\n",
      "[*] Epoch: 1165 train_loss= 1.10361 train_rmse= 0.82600 val_loss= 1.47024 val_rmse= 1.13154 \t\ttime= 0.56948\n",
      "[*] Epoch: 1166 train_loss= 1.10104 train_rmse= 0.82245 val_loss= 1.46965 val_rmse= 1.13088 \t\ttime= 0.57845\n",
      "[*] Epoch: 1167 train_loss= 1.09651 train_rmse= 0.82010 val_loss= 1.47128 val_rmse= 1.13340 \t\ttime= 0.61635\n",
      "[*] Epoch: 1168 train_loss= 1.11121 train_rmse= 0.83473 val_loss= 1.47036 val_rmse= 1.13256 \t\ttime= 0.53756\n",
      "[*] Epoch: 1169 train_loss= 1.10441 train_rmse= 0.82794 val_loss= 1.46846 val_rmse= 1.12970 \t\ttime= 0.60837\n",
      "[*] Epoch: 1170 train_loss= 1.09825 train_rmse= 0.82297 val_loss= 1.46618 val_rmse= 1.12598 \t\ttime= 0.62732\n",
      "[*] Epoch: 1171 train_loss= 1.10409 train_rmse= 0.82640 val_loss= 1.46551 val_rmse= 1.12464 \t\ttime= 0.64927\n",
      "[*] Epoch: 1172 train_loss= 1.09555 train_rmse= 0.82086 val_loss= 1.46698 val_rmse= 1.12579 \t\ttime= 0.63330\n",
      "[*] Epoch: 1173 train_loss= 1.09818 train_rmse= 0.82789 val_loss= 1.46683 val_rmse= 1.12540 \t\ttime= 0.58444\n",
      "[*] Epoch: 1174 train_loss= 1.10281 train_rmse= 0.82708 val_loss= 1.46844 val_rmse= 1.12786 \t\ttime= 0.60837\n",
      "[*] Epoch: 1175 train_loss= 1.09795 train_rmse= 0.82493 val_loss= 1.46853 val_rmse= 1.12845 \t\ttime= 0.55452\n",
      "[*] Epoch: 1176 train_loss= 1.10440 train_rmse= 0.82658 val_loss= 1.46695 val_rmse= 1.12652 \t\ttime= 0.61735\n",
      "[*] Epoch: 1177 train_loss= 1.09832 train_rmse= 0.82787 val_loss= 1.46575 val_rmse= 1.12556 \t\ttime= 0.65924\n",
      "[*] Epoch: 1178 train_loss= 1.10940 train_rmse= 0.83164 val_loss= 1.46173 val_rmse= 1.12116 \t\ttime= 0.56050\n",
      "[*] Epoch: 1179 train_loss= 1.09387 train_rmse= 0.81723 val_loss= 1.45805 val_rmse= 1.11774 \t\ttime= 0.59142\n",
      "[*] Epoch: 1180 train_loss= 1.10096 train_rmse= 0.82374 val_loss= 1.45871 val_rmse= 1.11886 \t\ttime= 0.60338\n",
      "[*] Epoch: 1181 train_loss= 1.09549 train_rmse= 0.82035 val_loss= 1.46008 val_rmse= 1.12036 \t\ttime= 0.58045\n",
      "[*] Epoch: 1182 train_loss= 1.09821 train_rmse= 0.82431 val_loss= 1.46080 val_rmse= 1.11990 \t\ttime= 0.59142\n",
      "[*] Epoch: 1183 train_loss= 1.10126 train_rmse= 0.82677 val_loss= 1.46348 val_rmse= 1.12191 \t\ttime= 0.59042\n",
      "[*] Epoch: 1184 train_loss= 1.09851 train_rmse= 0.82420 val_loss= 1.46669 val_rmse= 1.12546 \t\ttime= 0.66023\n",
      "[*] Epoch: 1185 train_loss= 1.09144 train_rmse= 0.81711 val_loss= 1.46789 val_rmse= 1.12767 \t\ttime= 0.55253\n",
      "[*] Epoch: 1186 train_loss= 1.10338 train_rmse= 0.82615 val_loss= 1.46933 val_rmse= 1.13047 \t\ttime= 0.60339\n",
      "[*] Epoch: 1187 train_loss= 1.10350 train_rmse= 0.82667 val_loss= 1.46665 val_rmse= 1.12809 \t\ttime= 0.61336\n",
      "[*] Epoch: 1188 train_loss= 1.10703 train_rmse= 0.82888 val_loss= 1.46143 val_rmse= 1.12155 \t\ttime= 0.56449\n",
      "[*] Epoch: 1189 train_loss= 1.11059 train_rmse= 0.83444 val_loss= 1.45529 val_rmse= 1.11347 \t\ttime= 0.61036\n",
      "[*] Epoch: 1190 train_loss= 1.11212 train_rmse= 0.83584 val_loss= 1.45220 val_rmse= 1.10955 \t\ttime= 0.62433\n",
      "[*] Epoch: 1191 train_loss= 1.10098 train_rmse= 0.82472 val_loss= 1.45180 val_rmse= 1.10916 \t\ttime= 0.60339\n",
      "[*] Epoch: 1192 train_loss= 1.09805 train_rmse= 0.82137 val_loss= 1.45458 val_rmse= 1.11331 \t\ttime= 0.62234\n",
      "[*] Epoch: 1193 train_loss= 1.10144 train_rmse= 0.82934 val_loss= 1.46008 val_rmse= 1.12135 \t\ttime= 0.61934\n",
      "[*] Epoch: 1194 train_loss= 1.09847 train_rmse= 0.82276 val_loss= 1.46425 val_rmse= 1.12679 \t\ttime= 0.59241\n",
      "[*] Epoch: 1195 train_loss= 1.09904 train_rmse= 0.82212 val_loss= 1.46762 val_rmse= 1.13021 \t\ttime= 0.56549\n",
      "[*] Epoch: 1196 train_loss= 1.10748 train_rmse= 0.82833 val_loss= 1.46879 val_rmse= 1.13040 \t\ttime= 0.59440\n",
      "[*] Epoch: 1197 train_loss= 1.09981 train_rmse= 0.82100 val_loss= 1.46885 val_rmse= 1.12941 \t\ttime= 0.67420\n",
      "[*] Epoch: 1198 train_loss= 1.10048 train_rmse= 0.82691 val_loss= 1.46721 val_rmse= 1.12642 \t\ttime= 0.58144\n",
      "[*] Epoch: 1199 train_loss= 1.10751 train_rmse= 0.83302 val_loss= 1.46243 val_rmse= 1.12087 \t\ttime= 0.59740\n",
      "[*] Epoch: 1200 train_loss= 1.08772 train_rmse= 0.81773 val_loss= 1.46024 val_rmse= 1.11962 \t\ttime= 0.61536\n",
      "[*] Epoch: 1201 train_loss= 1.10416 train_rmse= 0.83027 val_loss= 1.45744 val_rmse= 1.11706 \t\ttime= 0.53058\n",
      "[*] Epoch: 1202 train_loss= 1.09606 train_rmse= 0.81818 val_loss= 1.45875 val_rmse= 1.11900 \t\ttime= 0.62433\n",
      "[*] Epoch: 1203 train_loss= 1.09369 train_rmse= 0.81712 val_loss= 1.46159 val_rmse= 1.12196 \t\ttime= 0.61835\n",
      "[*] Epoch: 1204 train_loss= 1.09895 train_rmse= 0.82397 val_loss= 1.46305 val_rmse= 1.12258 \t\ttime= 0.65126\n",
      "[*] Epoch: 1205 train_loss= 1.08688 train_rmse= 0.81365 val_loss= 1.46514 val_rmse= 1.12391 \t\ttime= 0.59541\n",
      "[*] Epoch: 1206 train_loss= 1.08759 train_rmse= 0.81266 val_loss= 1.46681 val_rmse= 1.12548 \t\ttime= 0.59441\n",
      "[*] Epoch: 1207 train_loss= 1.10242 train_rmse= 0.82561 val_loss= 1.46777 val_rmse= 1.12724 \t\ttime= 0.60040\n",
      "[*] Epoch: 1208 train_loss= 1.10202 train_rmse= 0.82565 val_loss= 1.46933 val_rmse= 1.13009 \t\ttime= 0.55153\n",
      "[*] Epoch: 1209 train_loss= 1.10942 train_rmse= 0.82907 val_loss= 1.46757 val_rmse= 1.12899 \t\ttime= 0.58743\n",
      "[*] Epoch: 1210 train_loss= 1.09666 train_rmse= 0.82172 val_loss= 1.46523 val_rmse= 1.12609 \t\ttime= 0.67320\n",
      "[*] Epoch: 1211 train_loss= 1.08071 train_rmse= 0.81278 val_loss= 1.46542 val_rmse= 1.12595 \t\ttime= 0.55851\n",
      "[*] Epoch: 1212 train_loss= 1.08965 train_rmse= 0.81717 val_loss= 1.46834 val_rmse= 1.12830 \t\ttime= 0.63131\n",
      "[*] Epoch: 1213 train_loss= 1.10715 train_rmse= 0.83187 val_loss= 1.46839 val_rmse= 1.12699 \t\ttime= 0.61835\n",
      "[*] Epoch: 1214 train_loss= 1.09453 train_rmse= 0.82083 val_loss= 1.46733 val_rmse= 1.12503 \t\ttime= 0.57247\n",
      "[*] Epoch: 1215 train_loss= 1.08952 train_rmse= 0.81705 val_loss= 1.46530 val_rmse= 1.12297 \t\ttime= 0.58943\n",
      "[*] Epoch: 1216 train_loss= 1.09009 train_rmse= 0.81311 val_loss= 1.46501 val_rmse= 1.12322 \t\ttime= 0.61536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1217 train_loss= 1.10345 train_rmse= 0.82281 val_loss= 1.46533 val_rmse= 1.12439 \t\ttime= 0.64228\n",
      "[*] Epoch: 1218 train_loss= 1.08987 train_rmse= 0.81920 val_loss= 1.46593 val_rmse= 1.12581 \t\ttime= 0.57446\n",
      "[*] Epoch: 1219 train_loss= 1.10085 train_rmse= 0.82541 val_loss= 1.46936 val_rmse= 1.13123 \t\ttime= 0.58543\n",
      "[*] Epoch: 1220 train_loss= 1.10324 train_rmse= 0.82778 val_loss= 1.47091 val_rmse= 1.13384 \t\ttime= 0.60738\n",
      "[*] Epoch: 1221 train_loss= 1.09201 train_rmse= 0.81627 val_loss= 1.47045 val_rmse= 1.13295 \t\ttime= 0.55950\n",
      "[*] Epoch: 1222 train_loss= 1.09979 train_rmse= 0.82473 val_loss= 1.46793 val_rmse= 1.12926 \t\ttime= 0.59042\n",
      "[*] Epoch: 1223 train_loss= 1.09182 train_rmse= 0.82202 val_loss= 1.46667 val_rmse= 1.12693 \t\ttime= 0.63131\n",
      "[*] Epoch: 1224 train_loss= 1.08808 train_rmse= 0.81951 val_loss= 1.46837 val_rmse= 1.12885 \t\ttime= 0.58842\n",
      "[*] Epoch: 1225 train_loss= 1.10134 train_rmse= 0.82478 val_loss= 1.47103 val_rmse= 1.13173 \t\ttime= 0.59242\n",
      "[*] Epoch: 1226 train_loss= 1.09512 train_rmse= 0.82264 val_loss= 1.47138 val_rmse= 1.13164 \t\ttime= 0.61236\n",
      "[*] Epoch: 1227 train_loss= 1.09388 train_rmse= 0.82022 val_loss= 1.46966 val_rmse= 1.12973 \t\ttime= 0.60738\n",
      "[*] Epoch: 1228 train_loss= 1.09997 train_rmse= 0.82625 val_loss= 1.46513 val_rmse= 1.12469 \t\ttime= 0.56848\n",
      "[*] Epoch: 1229 train_loss= 1.08736 train_rmse= 0.81647 val_loss= 1.46255 val_rmse= 1.12204 \t\ttime= 0.59341\n",
      "[*] Epoch: 1230 train_loss= 1.10498 train_rmse= 0.82803 val_loss= 1.46057 val_rmse= 1.12025 \t\ttime= 0.65525\n",
      "[*] Epoch: 1231 train_loss= 1.09579 train_rmse= 0.82414 val_loss= 1.46005 val_rmse= 1.11924 \t\ttime= 0.55750\n",
      "[*] Epoch: 1232 train_loss= 1.08690 train_rmse= 0.81344 val_loss= 1.46413 val_rmse= 1.12292 \t\ttime= 0.57546\n",
      "[*] Epoch: 1233 train_loss= 1.09586 train_rmse= 0.81971 val_loss= 1.46907 val_rmse= 1.12769 \t\ttime= 0.59640\n",
      "[*] Epoch: 1234 train_loss= 1.09062 train_rmse= 0.81866 val_loss= 1.47289 val_rmse= 1.13119 \t\ttime= 0.55452\n",
      "[*] Epoch: 1235 train_loss= 1.07957 train_rmse= 0.81245 val_loss= 1.47505 val_rmse= 1.13375 \t\ttime= 0.57346\n",
      "[*] Epoch: 1236 train_loss= 1.08443 train_rmse= 0.81407 val_loss= 1.47628 val_rmse= 1.13622 \t\ttime= 0.57646\n",
      "[*] Epoch: 1237 train_loss= 1.07989 train_rmse= 0.81029 val_loss= 1.47489 val_rmse= 1.13593 \t\ttime= 0.67021\n",
      "[*] Epoch: 1238 train_loss= 1.10221 train_rmse= 0.83038 val_loss= 1.47178 val_rmse= 1.13281 \t\ttime= 0.55950\n",
      "[*] Epoch: 1239 train_loss= 1.09699 train_rmse= 0.82023 val_loss= 1.46761 val_rmse= 1.12813 \t\ttime= 0.62334\n",
      "[*] Epoch: 1240 train_loss= 1.07863 train_rmse= 0.80813 val_loss= 1.46651 val_rmse= 1.12673 \t\ttime= 0.60937\n",
      "[*] Epoch: 1241 train_loss= 1.08538 train_rmse= 0.81251 val_loss= 1.46792 val_rmse= 1.12831 \t\ttime= 0.54255\n",
      "[*] Epoch: 1242 train_loss= 1.08317 train_rmse= 0.81564 val_loss= 1.46993 val_rmse= 1.13050 \t\ttime= 0.59640\n",
      "[*] Epoch: 1243 train_loss= 1.08956 train_rmse= 0.81564 val_loss= 1.47337 val_rmse= 1.13376 \t\ttime= 0.63131\n",
      "[*] Epoch: 1244 train_loss= 1.10033 train_rmse= 0.82250 val_loss= 1.47291 val_rmse= 1.13280 \t\ttime= 0.62433\n",
      "[*] Epoch: 1245 train_loss= 1.09122 train_rmse= 0.81705 val_loss= 1.47291 val_rmse= 1.13288 \t\ttime= 0.61137\n",
      "[*] Epoch: 1246 train_loss= 1.09292 train_rmse= 0.82136 val_loss= 1.47139 val_rmse= 1.13119 \t\ttime= 0.60139\n",
      "[*] Epoch: 1247 train_loss= 1.09516 train_rmse= 0.81759 val_loss= 1.46961 val_rmse= 1.12961 \t\ttime= 0.61236\n",
      "[*] Epoch: 1248 train_loss= 1.08177 train_rmse= 0.81207 val_loss= 1.46961 val_rmse= 1.13022 \t\ttime= 0.55551\n",
      "[*] Epoch: 1249 train_loss= 1.08305 train_rmse= 0.81794 val_loss= 1.46866 val_rmse= 1.12916 \t\ttime= 0.58543\n",
      "[*] Epoch: 1250 train_loss= 1.09943 train_rmse= 0.82718 val_loss= 1.46589 val_rmse= 1.12578 \t\ttime= 0.64926\n",
      "[*] Epoch: 1251 train_loss= 1.08444 train_rmse= 0.81176 val_loss= 1.46606 val_rmse= 1.12542 \t\ttime= 0.57546\n",
      "[*] Epoch: 1252 train_loss= 1.09986 train_rmse= 0.82323 val_loss= 1.46476 val_rmse= 1.12444 \t\ttime= 0.59840\n",
      "[*] Epoch: 1253 train_loss= 1.08920 train_rmse= 0.81619 val_loss= 1.46346 val_rmse= 1.12326 \t\ttime= 0.61735\n",
      "[*] Epoch: 1254 train_loss= 1.07230 train_rmse= 0.80770 val_loss= 1.46467 val_rmse= 1.12553 \t\ttime= 0.55352\n",
      "[*] Epoch: 1255 train_loss= 1.09453 train_rmse= 0.82450 val_loss= 1.46750 val_rmse= 1.12949 \t\ttime= 0.60239\n",
      "[*] Epoch: 1256 train_loss= 1.08028 train_rmse= 0.80859 val_loss= 1.46961 val_rmse= 1.13283 \t\ttime= 0.62034\n",
      "[*] Epoch: 1257 train_loss= 1.08035 train_rmse= 0.81292 val_loss= 1.47072 val_rmse= 1.13406 \t\ttime= 0.64128\n",
      "[*] Epoch: 1258 train_loss= 1.08362 train_rmse= 0.81365 val_loss= 1.46976 val_rmse= 1.13246 \t\ttime= 0.58842\n",
      "[*] Epoch: 1259 train_loss= 1.08798 train_rmse= 0.81685 val_loss= 1.46812 val_rmse= 1.12917 \t\ttime= 0.58145\n",
      "[*] Epoch: 1260 train_loss= 1.07786 train_rmse= 0.81283 val_loss= 1.46515 val_rmse= 1.12474 \t\ttime= 0.61535\n",
      "[*] Epoch: 1261 train_loss= 1.08901 train_rmse= 0.81301 val_loss= 1.46162 val_rmse= 1.12054 \t\ttime= 0.56649\n",
      "[*] Epoch: 1262 train_loss= 1.09237 train_rmse= 0.81210 val_loss= 1.46187 val_rmse= 1.12172 \t\ttime= 0.61635\n",
      "[*] Epoch: 1263 train_loss= 1.08928 train_rmse= 0.81446 val_loss= 1.46632 val_rmse= 1.12662 \t\ttime= 0.70812\n",
      "[*] Epoch: 1264 train_loss= 1.08824 train_rmse= 0.81619 val_loss= 1.47098 val_rmse= 1.13197 \t\ttime= 0.58344\n",
      "[*] Epoch: 1265 train_loss= 1.07606 train_rmse= 0.80946 val_loss= 1.47354 val_rmse= 1.13352 \t\ttime= 0.62633\n",
      "[*] Epoch: 1266 train_loss= 1.07900 train_rmse= 0.80964 val_loss= 1.47422 val_rmse= 1.13268 \t\ttime= 0.61934\n",
      "[*] Epoch: 1267 train_loss= 1.09155 train_rmse= 0.81803 val_loss= 1.47123 val_rmse= 1.12851 \t\ttime= 0.57446\n",
      "[*] Epoch: 1268 train_loss= 1.08365 train_rmse= 0.80931 val_loss= 1.46976 val_rmse= 1.12724 \t\ttime= 0.61835\n",
      "[*] Epoch: 1269 train_loss= 1.09200 train_rmse= 0.81915 val_loss= 1.46979 val_rmse= 1.12879 \t\ttime= 0.61136\n",
      "[*] Epoch: 1270 train_loss= 1.07660 train_rmse= 0.80897 val_loss= 1.46849 val_rmse= 1.12864 \t\ttime= 0.62433\n",
      "[*] Epoch: 1271 train_loss= 1.07346 train_rmse= 0.80691 val_loss= 1.46663 val_rmse= 1.12657 \t\ttime= 0.58942\n",
      "[*] Epoch: 1272 train_loss= 1.07990 train_rmse= 0.81053 val_loss= 1.46637 val_rmse= 1.12602 \t\ttime= 0.62134\n",
      "[*] Epoch: 1273 train_loss= 1.08680 train_rmse= 0.81543 val_loss= 1.46655 val_rmse= 1.12547 \t\ttime= 0.60937\n",
      "[*] Epoch: 1274 train_loss= 1.08340 train_rmse= 0.80854 val_loss= 1.46900 val_rmse= 1.12868 \t\ttime= 0.54853\n",
      "[*] Epoch: 1275 train_loss= 1.08671 train_rmse= 0.81569 val_loss= 1.47007 val_rmse= 1.13049 \t\ttime= 0.61036\n",
      "[*] Epoch: 1276 train_loss= 1.09376 train_rmse= 0.81964 val_loss= 1.47077 val_rmse= 1.13093 \t\ttime= 0.64726\n",
      "[*] Epoch: 1277 train_loss= 1.08625 train_rmse= 0.81225 val_loss= 1.47266 val_rmse= 1.13301 \t\ttime= 0.58145\n",
      "[*] Epoch: 1278 train_loss= 1.07103 train_rmse= 0.80748 val_loss= 1.47284 val_rmse= 1.13332 \t\ttime= 0.61436\n",
      "[*] Epoch: 1279 train_loss= 1.08546 train_rmse= 0.81378 val_loss= 1.47337 val_rmse= 1.13386 \t\ttime= 0.60438\n",
      "[*] Epoch: 1280 train_loss= 1.07918 train_rmse= 0.80814 val_loss= 1.47315 val_rmse= 1.13414 \t\ttime= 0.58144\n",
      "[*] Epoch: 1281 train_loss= 1.08755 train_rmse= 0.81854 val_loss= 1.47188 val_rmse= 1.13400 \t\ttime= 0.57546\n",
      "[*] Epoch: 1282 train_loss= 1.07925 train_rmse= 0.81011 val_loss= 1.46913 val_rmse= 1.13161 \t\ttime= 0.57346\n",
      "[*] Epoch: 1283 train_loss= 1.09432 train_rmse= 0.82301 val_loss= 1.46580 val_rmse= 1.12697 \t\ttime= 0.67619\n",
      "[*] Epoch: 1284 train_loss= 1.07548 train_rmse= 0.80647 val_loss= 1.46558 val_rmse= 1.12569 \t\ttime= 0.55352\n",
      "[*] Epoch: 1285 train_loss= 1.07197 train_rmse= 0.80747 val_loss= 1.46363 val_rmse= 1.12276 \t\ttime= 0.61535\n",
      "[*] Epoch: 1286 train_loss= 1.08199 train_rmse= 0.81210 val_loss= 1.46594 val_rmse= 1.12461 \t\ttime= 0.62233\n",
      "[*] Epoch: 1287 train_loss= 1.08180 train_rmse= 0.81219 val_loss= 1.46862 val_rmse= 1.12723 \t\ttime= 0.51861\n",
      "[*] Epoch: 1288 train_loss= 1.09339 train_rmse= 0.82259 val_loss= 1.46988 val_rmse= 1.12886 \t\ttime= 0.60239\n",
      "[*] Epoch: 1289 train_loss= 1.08285 train_rmse= 0.81084 val_loss= 1.46983 val_rmse= 1.12953 \t\ttime= 0.61535\n",
      "[*] Epoch: 1290 train_loss= 1.07301 train_rmse= 0.80557 val_loss= 1.47072 val_rmse= 1.13186 \t\ttime= 0.62832\n",
      "[*] Epoch: 1291 train_loss= 1.09605 train_rmse= 0.82160 val_loss= 1.47212 val_rmse= 1.13363 \t\ttime= 0.58144\n",
      "[*] Epoch: 1292 train_loss= 1.08180 train_rmse= 0.81483 val_loss= 1.47311 val_rmse= 1.13479 \t\ttime= 0.59042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1293 train_loss= 1.07985 train_rmse= 0.81432 val_loss= 1.47371 val_rmse= 1.13544 \t\ttime= 0.63829\n",
      "[*] Epoch: 1294 train_loss= 1.10712 train_rmse= 0.83850 val_loss= 1.47363 val_rmse= 1.13509 \t\ttime= 0.54155\n",
      "[*] Epoch: 1295 train_loss= 1.07755 train_rmse= 0.80806 val_loss= 1.47294 val_rmse= 1.13385 \t\ttime= 0.60937\n",
      "[*] Epoch: 1296 train_loss= 1.08226 train_rmse= 0.81076 val_loss= 1.47182 val_rmse= 1.13220 \t\ttime= 0.66422\n",
      "[*] Epoch: 1297 train_loss= 1.08624 train_rmse= 0.81388 val_loss= 1.46836 val_rmse= 1.12809 \t\ttime= 0.60538\n",
      "[*] Epoch: 1298 train_loss= 1.07595 train_rmse= 0.80848 val_loss= 1.46516 val_rmse= 1.12547 \t\ttime= 0.61236\n",
      "[*] Epoch: 1299 train_loss= 1.09075 train_rmse= 0.81688 val_loss= 1.46386 val_rmse= 1.12458 \t\ttime= 0.59640\n",
      "[*] Epoch: 1300 train_loss= 1.08429 train_rmse= 0.81324 val_loss= 1.46803 val_rmse= 1.12907 \t\ttime= 0.54654\n",
      "[*] Epoch: 1301 train_loss= 1.08406 train_rmse= 0.81273 val_loss= 1.47267 val_rmse= 1.13438 \t\ttime= 0.59840\n",
      "[*] Epoch: 1302 train_loss= 1.07986 train_rmse= 0.80728 val_loss= 1.47754 val_rmse= 1.13927 \t\ttime= 0.63032\n",
      "[*] Epoch: 1303 train_loss= 1.08301 train_rmse= 0.81301 val_loss= 1.47929 val_rmse= 1.14043 \t\ttime= 0.65325\n",
      "[*] Epoch: 1304 train_loss= 1.06878 train_rmse= 0.80604 val_loss= 1.47890 val_rmse= 1.13940 \t\ttime= 0.57047\n",
      "[*] Epoch: 1305 train_loss= 1.07483 train_rmse= 0.81496 val_loss= 1.47712 val_rmse= 1.13665 \t\ttime= 0.58244\n",
      "[*] Epoch: 1306 train_loss= 1.07112 train_rmse= 0.80564 val_loss= 1.47554 val_rmse= 1.13550 \t\ttime= 0.61935\n",
      "[*] Epoch: 1307 train_loss= 1.09754 train_rmse= 0.82267 val_loss= 1.47262 val_rmse= 1.13355 \t\ttime= 0.54953\n",
      "[*] Epoch: 1308 train_loss= 1.07719 train_rmse= 0.81264 val_loss= 1.47019 val_rmse= 1.13208 \t\ttime= 0.60538\n",
      "[*] Epoch: 1309 train_loss= 1.08967 train_rmse= 0.82059 val_loss= 1.46823 val_rmse= 1.13023 \t\ttime= 0.67919\n",
      "[*] Epoch: 1310 train_loss= 1.08094 train_rmse= 0.81292 val_loss= 1.46641 val_rmse= 1.12752 \t\ttime= 0.59342\n",
      "[*] Epoch: 1311 train_loss= 1.09334 train_rmse= 0.82134 val_loss= 1.46610 val_rmse= 1.12647 \t\ttime= 0.60438\n",
      "[*] Epoch: 1312 train_loss= 1.07451 train_rmse= 0.80620 val_loss= 1.46996 val_rmse= 1.12971 \t\ttime= 0.62135\n",
      "[*] Epoch: 1313 train_loss= 1.09784 train_rmse= 0.82356 val_loss= 1.47061 val_rmse= 1.12912 \t\ttime= 0.57247\n",
      "[*] Epoch: 1314 train_loss= 1.06798 train_rmse= 0.80380 val_loss= 1.47031 val_rmse= 1.12761 \t\ttime= 0.59541\n",
      "[*] Epoch: 1315 train_loss= 1.07967 train_rmse= 0.81178 val_loss= 1.46980 val_rmse= 1.12710 \t\ttime= 0.60239\n",
      "[*] Epoch: 1316 train_loss= 1.07743 train_rmse= 0.80681 val_loss= 1.46941 val_rmse= 1.12765 \t\ttime= 0.66023\n",
      "[*] Epoch: 1317 train_loss= 1.07924 train_rmse= 0.81264 val_loss= 1.46895 val_rmse= 1.12828 \t\ttime= 0.54154\n",
      "[*] Epoch: 1318 train_loss= 1.07893 train_rmse= 0.80877 val_loss= 1.46991 val_rmse= 1.12959 \t\ttime= 0.61136\n",
      "[*] Epoch: 1319 train_loss= 1.07046 train_rmse= 0.79874 val_loss= 1.47292 val_rmse= 1.13265 \t\ttime= 0.61635\n",
      "[*] Epoch: 1320 train_loss= 1.07015 train_rmse= 0.80374 val_loss= 1.47510 val_rmse= 1.13514 \t\ttime= 0.53656\n",
      "[*] Epoch: 1321 train_loss= 1.06617 train_rmse= 0.79981 val_loss= 1.47805 val_rmse= 1.13861 \t\ttime= 0.59940\n",
      "[*] Epoch: 1322 train_loss= 1.08989 train_rmse= 0.81967 val_loss= 1.47879 val_rmse= 1.14011 \t\ttime= 0.61635\n",
      "[*] Epoch: 1323 train_loss= 1.07195 train_rmse= 0.80736 val_loss= 1.47894 val_rmse= 1.14092 \t\ttime= 0.62232\n",
      "[*] Epoch: 1324 train_loss= 1.08151 train_rmse= 0.81143 val_loss= 1.47632 val_rmse= 1.13823 \t\ttime= 0.59142\n",
      "[*] Epoch: 1325 train_loss= 1.06416 train_rmse= 0.80267 val_loss= 1.47507 val_rmse= 1.13657 \t\ttime= 0.59541\n",
      "[*] Epoch: 1326 train_loss= 1.08170 train_rmse= 0.81194 val_loss= 1.47335 val_rmse= 1.13374 \t\ttime= 0.62333\n",
      "[*] Epoch: 1327 train_loss= 1.07566 train_rmse= 0.81131 val_loss= 1.47164 val_rmse= 1.13114 \t\ttime= 0.55352\n",
      "[*] Epoch: 1328 train_loss= 1.08657 train_rmse= 0.81722 val_loss= 1.47245 val_rmse= 1.13216 \t\ttime= 0.61835\n",
      "[*] Epoch: 1329 train_loss= 1.07487 train_rmse= 0.80811 val_loss= 1.47330 val_rmse= 1.13327 \t\ttime= 0.66522\n",
      "[*] Epoch: 1330 train_loss= 1.08456 train_rmse= 0.81356 val_loss= 1.47371 val_rmse= 1.13373 \t\ttime= 0.58942\n",
      "[*] Epoch: 1331 train_loss= 1.07174 train_rmse= 0.80613 val_loss= 1.47233 val_rmse= 1.13272 \t\ttime= 0.63131\n",
      "[*] Epoch: 1332 train_loss= 1.08016 train_rmse= 0.81511 val_loss= 1.46970 val_rmse= 1.13010 \t\ttime= 0.67818\n",
      "[*] Epoch: 1333 train_loss= 1.07983 train_rmse= 0.80866 val_loss= 1.47020 val_rmse= 1.13085 \t\ttime= 0.62931\n",
      "[*] Epoch: 1334 train_loss= 1.07811 train_rmse= 0.80866 val_loss= 1.47718 val_rmse= 1.13889 \t\ttime= 0.63929\n",
      "[*] Epoch: 1335 train_loss= 1.07089 train_rmse= 0.80621 val_loss= 1.48342 val_rmse= 1.14563 \t\ttime= 0.69913\n",
      "[*] Epoch: 1336 train_loss= 1.08853 train_rmse= 0.81749 val_loss= 1.48346 val_rmse= 1.14498 \t\ttime= 0.61392\n",
      "[*] Epoch: 1337 train_loss= 1.09138 train_rmse= 0.81983 val_loss= 1.47851 val_rmse= 1.13893 \t\ttime= 0.66622\n",
      "[*] Epoch: 1338 train_loss= 1.06404 train_rmse= 0.79700 val_loss= 1.47510 val_rmse= 1.13487 \t\ttime= 0.67468\n",
      "[*] Epoch: 1339 train_loss= 1.08277 train_rmse= 0.81141 val_loss= 1.47388 val_rmse= 1.13417 \t\ttime= 0.55551\n",
      "[*] Epoch: 1340 train_loss= 1.06494 train_rmse= 0.80443 val_loss= 1.47448 val_rmse= 1.13549 \t\ttime= 0.63530\n",
      "[*] Epoch: 1341 train_loss= 1.06181 train_rmse= 0.79919 val_loss= 1.47460 val_rmse= 1.13579 \t\ttime= 0.65525\n",
      "[*] Epoch: 1342 train_loss= 1.06463 train_rmse= 0.80016 val_loss= 1.47431 val_rmse= 1.13507 \t\ttime= 0.65224\n",
      "[*] Epoch: 1343 train_loss= 1.07042 train_rmse= 0.80347 val_loss= 1.47454 val_rmse= 1.13353 \t\ttime= 0.63331\n",
      "[*] Epoch: 1344 train_loss= 1.08408 train_rmse= 0.81128 val_loss= 1.47339 val_rmse= 1.13102 \t\ttime= 0.63630\n",
      "[*] Epoch: 1345 train_loss= 1.07256 train_rmse= 0.80777 val_loss= 1.47473 val_rmse= 1.13261 \t\ttime= 0.61535\n",
      "[*] Epoch: 1346 train_loss= 1.09459 train_rmse= 0.82827 val_loss= 1.47524 val_rmse= 1.13416 \t\ttime= 0.63730\n",
      "[*] Epoch: 1347 train_loss= 1.09590 train_rmse= 0.82718 val_loss= 1.47579 val_rmse= 1.13608 \t\ttime= 0.62632\n",
      "[*] Epoch: 1348 train_loss= 1.07295 train_rmse= 0.80888 val_loss= 1.47695 val_rmse= 1.13752 \t\ttime= 0.65924\n",
      "[*] Epoch: 1349 train_loss= 1.08048 train_rmse= 0.80820 val_loss= 1.47714 val_rmse= 1.13777 \t\ttime= 0.59341\n",
      "[*] Epoch: 1350 train_loss= 1.07293 train_rmse= 0.80730 val_loss= 1.47522 val_rmse= 1.13526 \t\ttime= 0.59940\n",
      "[*] Epoch: 1351 train_loss= 1.07865 train_rmse= 0.80601 val_loss= 1.47299 val_rmse= 1.13328 \t\ttime= 0.62233\n",
      "[*] Epoch: 1352 train_loss= 1.07455 train_rmse= 0.80771 val_loss= 1.47201 val_rmse= 1.13309 \t\ttime= 0.54355\n",
      "[*] Epoch: 1353 train_loss= 1.07075 train_rmse= 0.80670 val_loss= 1.47153 val_rmse= 1.13306 \t\ttime= 0.60638\n",
      "[*] Epoch: 1354 train_loss= 1.06499 train_rmse= 0.80135 val_loss= 1.47347 val_rmse= 1.13519 \t\ttime= 0.66123\n",
      "[*] Epoch: 1355 train_loss= 1.07736 train_rmse= 0.81171 val_loss= 1.47448 val_rmse= 1.13550 \t\ttime= 0.58643\n",
      "[*] Epoch: 1356 train_loss= 1.07824 train_rmse= 0.80724 val_loss= 1.47401 val_rmse= 1.13472 \t\ttime= 0.58942\n",
      "[*] Epoch: 1357 train_loss= 1.06419 train_rmse= 0.79535 val_loss= 1.47577 val_rmse= 1.13715 \t\ttime= 0.61934\n",
      "[*] Epoch: 1358 train_loss= 1.07104 train_rmse= 0.80639 val_loss= 1.47722 val_rmse= 1.13907 \t\ttime= 0.57147\n",
      "[*] Epoch: 1359 train_loss= 1.08093 train_rmse= 0.81819 val_loss= 1.47577 val_rmse= 1.13729 \t\ttime= 0.57746\n",
      "[*] Epoch: 1360 train_loss= 1.07511 train_rmse= 0.80935 val_loss= 1.47399 val_rmse= 1.13454 \t\ttime= 0.59142\n",
      "[*] Epoch: 1361 train_loss= 1.09097 train_rmse= 0.82084 val_loss= 1.47014 val_rmse= 1.12902 \t\ttime= 0.67719\n",
      "[*] Epoch: 1362 train_loss= 1.06583 train_rmse= 0.80213 val_loss= 1.46835 val_rmse= 1.12564 \t\ttime= 0.57446\n",
      "[*] Epoch: 1363 train_loss= 1.08616 train_rmse= 0.81434 val_loss= 1.47255 val_rmse= 1.13133 \t\ttime= 0.60239\n",
      "[*] Epoch: 1364 train_loss= 1.07363 train_rmse= 0.80651 val_loss= 1.47700 val_rmse= 1.13803 \t\ttime= 0.64029\n",
      "[*] Epoch: 1365 train_loss= 1.07622 train_rmse= 0.81058 val_loss= 1.47893 val_rmse= 1.14133 \t\ttime= 0.59241\n",
      "[*] Epoch: 1366 train_loss= 1.07545 train_rmse= 0.80977 val_loss= 1.47783 val_rmse= 1.14003 \t\ttime= 0.64029\n",
      "[*] Epoch: 1367 train_loss= 1.06799 train_rmse= 0.80564 val_loss= 1.47600 val_rmse= 1.13721 \t\ttime= 0.71410\n",
      "[*] Epoch: 1368 train_loss= 1.06996 train_rmse= 0.80445 val_loss= 1.47209 val_rmse= 1.13225 \t\ttime= 0.63331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1369 train_loss= 1.06870 train_rmse= 0.80677 val_loss= 1.46841 val_rmse= 1.12786 \t\ttime= 0.64328\n",
      "[*] Epoch: 1370 train_loss= 1.06843 train_rmse= 0.79932 val_loss= 1.46671 val_rmse= 1.12548 \t\ttime= 0.65924\n",
      "[*] Epoch: 1371 train_loss= 1.06662 train_rmse= 0.80443 val_loss= 1.46752 val_rmse= 1.12624 \t\ttime= 0.59242\n",
      "[*] Epoch: 1372 train_loss= 1.06591 train_rmse= 0.80318 val_loss= 1.47125 val_rmse= 1.13053 \t\ttime= 0.66223\n",
      "[*] Epoch: 1373 train_loss= 1.06748 train_rmse= 0.80044 val_loss= 1.47411 val_rmse= 1.13322 \t\ttime= 0.66921\n",
      "[*] Epoch: 1374 train_loss= 1.06357 train_rmse= 0.79876 val_loss= 1.47703 val_rmse= 1.13536 \t\ttime= 0.60837\n",
      "[*] Epoch: 1375 train_loss= 1.07000 train_rmse= 0.81230 val_loss= 1.47855 val_rmse= 1.13610 \t\ttime= 0.59740\n",
      "[*] Epoch: 1376 train_loss= 1.07008 train_rmse= 0.81010 val_loss= 1.48260 val_rmse= 1.14058 \t\ttime= 0.59242\n",
      "[*] Epoch: 1377 train_loss= 1.07760 train_rmse= 0.81009 val_loss= 1.48239 val_rmse= 1.14113 \t\ttime= 0.60737\n",
      "[*] Epoch: 1378 train_loss= 1.06090 train_rmse= 0.80057 val_loss= 1.48049 val_rmse= 1.13997 \t\ttime= 0.59242\n",
      "[*] Epoch: 1379 train_loss= 1.06508 train_rmse= 0.80022 val_loss= 1.47987 val_rmse= 1.13980 \t\ttime= 0.62433\n",
      "[*] Epoch: 1380 train_loss= 1.08071 train_rmse= 0.81344 val_loss= 1.47778 val_rmse= 1.13788 \t\ttime= 0.69215\n",
      "[*] Epoch: 1381 train_loss= 1.06215 train_rmse= 0.80157 val_loss= 1.47553 val_rmse= 1.13525 \t\ttime= 0.59042\n",
      "[*] Epoch: 1382 train_loss= 1.07223 train_rmse= 0.80538 val_loss= 1.47038 val_rmse= 1.12950 \t\ttime= 0.64129\n",
      "[*] Epoch: 1383 train_loss= 1.06806 train_rmse= 0.80699 val_loss= 1.46649 val_rmse= 1.12533 \t\ttime= 0.58044\n",
      "[*] Epoch: 1384 train_loss= 1.07773 train_rmse= 0.81268 val_loss= 1.46515 val_rmse= 1.12413 \t\ttime= 0.55452\n",
      "[*] Epoch: 1385 train_loss= 1.07881 train_rmse= 0.80962 val_loss= 1.46387 val_rmse= 1.12375 \t\ttime= 0.61735\n",
      "[*] Epoch: 1386 train_loss= 1.07669 train_rmse= 0.80643 val_loss= 1.47015 val_rmse= 1.13182 \t\ttime= 0.66323\n",
      "[*] Epoch: 1387 train_loss= 1.06802 train_rmse= 0.80379 val_loss= 1.47581 val_rmse= 1.13822 \t\ttime= 0.62333\n",
      "[*] Epoch: 1388 train_loss= 1.06778 train_rmse= 0.80097 val_loss= 1.47979 val_rmse= 1.14127 \t\ttime= 0.65126\n",
      "[*] Epoch: 1389 train_loss= 1.08032 train_rmse= 0.81547 val_loss= 1.48049 val_rmse= 1.14002 \t\ttime= 0.64926\n",
      "[*] Epoch: 1390 train_loss= 1.06441 train_rmse= 0.80035 val_loss= 1.48141 val_rmse= 1.14079 \t\ttime= 0.59042\n",
      "[*] Epoch: 1391 train_loss= 1.06576 train_rmse= 0.80580 val_loss= 1.48041 val_rmse= 1.14000 \t\ttime= 0.61735\n",
      "[*] Epoch: 1392 train_loss= 1.06110 train_rmse= 0.79940 val_loss= 1.47688 val_rmse= 1.13660 \t\ttime= 0.62135\n",
      "[*] Epoch: 1393 train_loss= 1.06406 train_rmse= 0.79964 val_loss= 1.47551 val_rmse= 1.13613 \t\ttime= 0.64527\n",
      "[*] Epoch: 1394 train_loss= 1.06934 train_rmse= 0.80921 val_loss= 1.47170 val_rmse= 1.13196 \t\ttime= 0.60140\n",
      "[*] Epoch: 1395 train_loss= 1.06779 train_rmse= 0.80443 val_loss= 1.46944 val_rmse= 1.12890 \t\ttime= 0.60438\n",
      "[*] Epoch: 1396 train_loss= 1.06512 train_rmse= 0.80281 val_loss= 1.46909 val_rmse= 1.12824 \t\ttime= 0.61635\n",
      "[*] Epoch: 1397 train_loss= 1.08781 train_rmse= 0.81668 val_loss= 1.47063 val_rmse= 1.13006 \t\ttime= 0.57546\n",
      "[*] Epoch: 1398 train_loss= 1.07484 train_rmse= 0.81061 val_loss= 1.47085 val_rmse= 1.12979 \t\ttime= 0.60438\n",
      "[*] Epoch: 1399 train_loss= 1.06613 train_rmse= 0.80009 val_loss= 1.47425 val_rmse= 1.13333 \t\ttime= 0.68617\n",
      "[*] Epoch: 1400 train_loss= 1.06280 train_rmse= 0.80102 val_loss= 1.47886 val_rmse= 1.13885 \t\ttime= 0.57646\n",
      "[*] Epoch: 1401 train_loss= 1.06080 train_rmse= 0.80381 val_loss= 1.48262 val_rmse= 1.14300 \t\ttime= 0.61237\n",
      "[*] Epoch: 1402 train_loss= 1.06716 train_rmse= 0.80408 val_loss= 1.48476 val_rmse= 1.14537 \t\ttime= 0.61735\n",
      "[*] Epoch: 1403 train_loss= 1.07914 train_rmse= 0.82156 val_loss= 1.48162 val_rmse= 1.14135 \t\ttime= 0.56250\n",
      "[*] Epoch: 1404 train_loss= 1.06973 train_rmse= 0.80933 val_loss= 1.47677 val_rmse= 1.13565 \t\ttime= 0.59840\n",
      "[*] Epoch: 1405 train_loss= 1.06023 train_rmse= 0.80086 val_loss= 1.47259 val_rmse= 1.13118 \t\ttime= 0.62333\n",
      "[*] Epoch: 1406 train_loss= 1.06240 train_rmse= 0.79753 val_loss= 1.47009 val_rmse= 1.12928 \t\ttime= 0.66024\n",
      "[*] Epoch: 1407 train_loss= 1.07516 train_rmse= 0.80490 val_loss= 1.47033 val_rmse= 1.12992 \t\ttime= 0.58743\n",
      "[*] Epoch: 1408 train_loss= 1.06199 train_rmse= 0.79948 val_loss= 1.47254 val_rmse= 1.13196 \t\ttime= 0.57446\n",
      "[*] Epoch: 1409 train_loss= 1.05901 train_rmse= 0.80063 val_loss= 1.47374 val_rmse= 1.13354 \t\ttime= 0.60339\n",
      "[*] Epoch: 1410 train_loss= 1.07012 train_rmse= 0.80530 val_loss= 1.47440 val_rmse= 1.13487 \t\ttime= 0.55551\n",
      "[*] Epoch: 1411 train_loss= 1.07535 train_rmse= 0.80758 val_loss= 1.47311 val_rmse= 1.13350 \t\ttime= 0.62732\n",
      "[*] Epoch: 1412 train_loss= 1.06131 train_rmse= 0.80048 val_loss= 1.47053 val_rmse= 1.13017 \t\ttime= 0.64727\n",
      "[*] Epoch: 1413 train_loss= 1.06542 train_rmse= 0.80217 val_loss= 1.47016 val_rmse= 1.12991 \t\ttime= 0.60139\n",
      "[*] Epoch: 1414 train_loss= 1.06340 train_rmse= 0.80526 val_loss= 1.47004 val_rmse= 1.12991 \t\ttime= 0.60239\n",
      "[*] Epoch: 1415 train_loss= 1.07216 train_rmse= 0.80619 val_loss= 1.47000 val_rmse= 1.12934 \t\ttime= 0.62933\n",
      "[*] Epoch: 1416 train_loss= 1.06794 train_rmse= 0.80512 val_loss= 1.47092 val_rmse= 1.13064 \t\ttime= 0.56649\n",
      "[*] Epoch: 1417 train_loss= 1.06687 train_rmse= 0.80373 val_loss= 1.47038 val_rmse= 1.13024 \t\ttime= 0.58444\n",
      "[*] Epoch: 1418 train_loss= 1.06391 train_rmse= 0.79815 val_loss= 1.47286 val_rmse= 1.13306 \t\ttime= 0.59740\n",
      "[*] Epoch: 1419 train_loss= 1.05512 train_rmse= 0.79443 val_loss= 1.47684 val_rmse= 1.13713 \t\ttime= 0.67220\n",
      "[*] Epoch: 1420 train_loss= 1.07942 train_rmse= 0.81324 val_loss= 1.47808 val_rmse= 1.13757 \t\ttime= 0.55352\n",
      "[*] Epoch: 1421 train_loss= 1.06334 train_rmse= 0.80006 val_loss= 1.47871 val_rmse= 1.13779 \t\ttime= 0.62034\n",
      "[*] Epoch: 1422 train_loss= 1.07307 train_rmse= 0.81268 val_loss= 1.47630 val_rmse= 1.13465 \t\ttime= 0.61137\n",
      "[*] Epoch: 1423 train_loss= 1.06372 train_rmse= 0.80235 val_loss= 1.47212 val_rmse= 1.12987 \t\ttime= 0.60039\n",
      "[*] Epoch: 1424 train_loss= 1.07375 train_rmse= 0.81048 val_loss= 1.46913 val_rmse= 1.12670 \t\ttime= 0.68816\n",
      "[*] Epoch: 1425 train_loss= 1.05716 train_rmse= 0.79846 val_loss= 1.46709 val_rmse= 1.12458 \t\ttime= 0.70213\n",
      "[*] Epoch: 1426 train_loss= 1.06486 train_rmse= 0.79734 val_loss= 1.46836 val_rmse= 1.12657 \t\ttime= 0.62333\n",
      "[*] Epoch: 1427 train_loss= 1.06518 train_rmse= 0.80019 val_loss= 1.47264 val_rmse= 1.13210 \t\ttime= 0.63331\n",
      "[*] Epoch: 1428 train_loss= 1.07351 train_rmse= 0.80814 val_loss= 1.47704 val_rmse= 1.13655 \t\ttime= 0.68417\n",
      "[*] Epoch: 1429 train_loss= 1.05672 train_rmse= 0.79991 val_loss= 1.48026 val_rmse= 1.13894 \t\ttime= 0.56748\n",
      "[*] Epoch: 1430 train_loss= 1.06759 train_rmse= 0.80047 val_loss= 1.48203 val_rmse= 1.14081 \t\ttime= 0.62932\n",
      "[*] Epoch: 1431 train_loss= 1.07334 train_rmse= 0.80924 val_loss= 1.48264 val_rmse= 1.14262 \t\ttime= 0.64827\n",
      "[*] Epoch: 1432 train_loss= 1.06495 train_rmse= 0.80489 val_loss= 1.48143 val_rmse= 1.14218 \t\ttime= 0.62632\n",
      "[*] Epoch: 1433 train_loss= 1.06125 train_rmse= 0.79935 val_loss= 1.47763 val_rmse= 1.13851 \t\ttime= 0.60438\n",
      "[*] Epoch: 1434 train_loss= 1.05897 train_rmse= 0.79767 val_loss= 1.47381 val_rmse= 1.13426 \t\ttime= 0.57646\n",
      "[*] Epoch: 1435 train_loss= 1.06160 train_rmse= 0.79706 val_loss= 1.47128 val_rmse= 1.13136 \t\ttime= 0.57646\n",
      "[*] Epoch: 1436 train_loss= 1.05902 train_rmse= 0.79721 val_loss= 1.47273 val_rmse= 1.13223 \t\ttime= 0.58244\n",
      "[*] Epoch: 1437 train_loss= 1.06884 train_rmse= 0.80836 val_loss= 1.47694 val_rmse= 1.13625 \t\ttime= 0.59741\n",
      "[*] Epoch: 1438 train_loss= 1.06746 train_rmse= 0.80630 val_loss= 1.48144 val_rmse= 1.14011 \t\ttime= 0.65425\n",
      "[*] Epoch: 1439 train_loss= 1.05588 train_rmse= 0.79964 val_loss= 1.48306 val_rmse= 1.14160 \t\ttime= 0.58643\n",
      "[*] Epoch: 1440 train_loss= 1.06695 train_rmse= 0.80360 val_loss= 1.48302 val_rmse= 1.14212 \t\ttime= 0.64627\n",
      "[*] Epoch: 1441 train_loss= 1.07578 train_rmse= 0.81186 val_loss= 1.48129 val_rmse= 1.14111 \t\ttime= 0.61236\n",
      "[*] Epoch: 1442 train_loss= 1.05384 train_rmse= 0.79533 val_loss= 1.47778 val_rmse= 1.13766 \t\ttime= 0.53457\n",
      "[*] Epoch: 1443 train_loss= 1.06337 train_rmse= 0.80212 val_loss= 1.47472 val_rmse= 1.13397 \t\ttime= 0.63929\n",
      "[*] Epoch: 1444 train_loss= 1.07129 train_rmse= 0.80189 val_loss= 1.47466 val_rmse= 1.13352 \t\ttime= 0.66123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1445 train_loss= 1.06102 train_rmse= 0.79835 val_loss= 1.47561 val_rmse= 1.13449 \t\ttime= 0.65525\n",
      "[*] Epoch: 1446 train_loss= 1.06893 train_rmse= 0.80487 val_loss= 1.47666 val_rmse= 1.13611 \t\ttime= 0.62632\n",
      "[*] Epoch: 1447 train_loss= 1.07432 train_rmse= 0.80644 val_loss= 1.47918 val_rmse= 1.14011 \t\ttime= 0.62533\n",
      "[*] Epoch: 1448 train_loss= 1.07978 train_rmse= 0.81542 val_loss= 1.47920 val_rmse= 1.14034 \t\ttime= 0.61835\n",
      "[*] Epoch: 1449 train_loss= 1.07637 train_rmse= 0.81034 val_loss= 1.47632 val_rmse= 1.13666 \t\ttime= 0.60538\n",
      "[*] Epoch: 1450 train_loss= 1.05219 train_rmse= 0.79509 val_loss= 1.47415 val_rmse= 1.13342 \t\ttime= 0.59940\n",
      "[*] Epoch: 1451 train_loss= 1.05037 train_rmse= 0.79373 val_loss= 1.47374 val_rmse= 1.13208 \t\ttime= 0.70511\n",
      "[*] Epoch: 1452 train_loss= 1.05995 train_rmse= 0.79863 val_loss= 1.47244 val_rmse= 1.13051 \t\ttime= 0.61336\n",
      "[*] Epoch: 1453 train_loss= 1.06661 train_rmse= 0.80225 val_loss= 1.47358 val_rmse= 1.13231 \t\ttime= 0.63530\n",
      "[*] Epoch: 1454 train_loss= 1.06840 train_rmse= 0.80195 val_loss= 1.47547 val_rmse= 1.13504 \t\ttime= 0.62633\n",
      "[*] Epoch: 1455 train_loss= 1.05198 train_rmse= 0.79230 val_loss= 1.47715 val_rmse= 1.13699 \t\ttime= 0.53357\n",
      "[*] Epoch: 1456 train_loss= 1.05946 train_rmse= 0.80025 val_loss= 1.47862 val_rmse= 1.13885 \t\ttime= 0.64727\n",
      "[*] Epoch: 1457 train_loss= 1.06228 train_rmse= 0.80026 val_loss= 1.48027 val_rmse= 1.14056 \t\ttime= 0.66323\n",
      "[*] Epoch: 1458 train_loss= 1.06032 train_rmse= 0.79965 val_loss= 1.48092 val_rmse= 1.14038 \t\ttime= 0.60837\n",
      "[*] Epoch: 1459 train_loss= 1.07011 train_rmse= 0.80665 val_loss= 1.48051 val_rmse= 1.13931 \t\ttime= 0.62632\n",
      "[*] Epoch: 1460 train_loss= 1.06666 train_rmse= 0.80073 val_loss= 1.48046 val_rmse= 1.13980 \t\ttime= 0.64328\n",
      "[*] Epoch: 1461 train_loss= 1.05793 train_rmse= 0.79267 val_loss= 1.48246 val_rmse= 1.14198 \t\ttime= 0.57147\n",
      "[*] Epoch: 1462 train_loss= 1.06231 train_rmse= 0.79802 val_loss= 1.48596 val_rmse= 1.14681 \t\ttime= 0.60039\n",
      "[*] Epoch: 1463 train_loss= 1.05355 train_rmse= 0.79630 val_loss= 1.48696 val_rmse= 1.14836 \t\ttime= 0.63331\n",
      "[*] Epoch: 1464 train_loss= 1.06074 train_rmse= 0.80298 val_loss= 1.48651 val_rmse= 1.14740 \t\ttime= 0.64029\n",
      "[*] Epoch: 1465 train_loss= 1.06295 train_rmse= 0.80720 val_loss= 1.48322 val_rmse= 1.14303 \t\ttime= 0.61038\n",
      "[*] Epoch: 1466 train_loss= 1.05947 train_rmse= 0.79621 val_loss= 1.48110 val_rmse= 1.13962 \t\ttime= 0.58544\n",
      "[*] Epoch: 1467 train_loss= 1.08006 train_rmse= 0.81796 val_loss= 1.47860 val_rmse= 1.13643 \t\ttime= 0.61635\n",
      "[*] Epoch: 1468 train_loss= 1.04737 train_rmse= 0.79005 val_loss= 1.47926 val_rmse= 1.13708 \t\ttime= 0.57347\n",
      "[*] Epoch: 1469 train_loss= 1.05286 train_rmse= 0.79256 val_loss= 1.48399 val_rmse= 1.14317 \t\ttime= 0.64428\n",
      "[*] Epoch: 1470 train_loss= 1.05375 train_rmse= 0.79572 val_loss= 1.48885 val_rmse= 1.14897 \t\ttime= 0.71409\n",
      "[*] Epoch: 1471 train_loss= 1.07743 train_rmse= 0.81936 val_loss= 1.48643 val_rmse= 1.14622 \t\ttime= 0.57247\n",
      "[*] Epoch: 1472 train_loss= 1.04914 train_rmse= 0.79306 val_loss= 1.48182 val_rmse= 1.14106 \t\ttime= 0.62533\n",
      "[*] Epoch: 1473 train_loss= 1.06290 train_rmse= 0.80042 val_loss= 1.47970 val_rmse= 1.13823 \t\ttime= 0.63430\n",
      "[*] Epoch: 1474 train_loss= 1.06377 train_rmse= 0.80242 val_loss= 1.47776 val_rmse= 1.13640 \t\ttime= 0.52659\n",
      "[*] Epoch: 1475 train_loss= 1.06751 train_rmse= 0.80681 val_loss= 1.47402 val_rmse= 1.13261 \t\ttime= 0.60638\n",
      "[*] Epoch: 1476 train_loss= 1.04974 train_rmse= 0.79791 val_loss= 1.47217 val_rmse= 1.13153 \t\ttime= 0.64727\n",
      "[*] Epoch: 1477 train_loss= 1.05485 train_rmse= 0.79516 val_loss= 1.47421 val_rmse= 1.13390 \t\ttime= 0.64926\n",
      "[*] Epoch: 1478 train_loss= 1.05112 train_rmse= 0.78974 val_loss= 1.48068 val_rmse= 1.14075 \t\ttime= 0.59740\n",
      "[*] Epoch: 1479 train_loss= 1.06296 train_rmse= 0.80018 val_loss= 1.48803 val_rmse= 1.14842 \t\ttime= 0.60638\n",
      "[*] Epoch: 1480 train_loss= 1.06870 train_rmse= 0.80604 val_loss= 1.49135 val_rmse= 1.15107 \t\ttime= 0.59840\n",
      "[*] Epoch: 1481 train_loss= 1.08382 train_rmse= 0.81651 val_loss= 1.48870 val_rmse= 1.14762 \t\ttime= 0.56349\n",
      "[*] Epoch: 1482 train_loss= 1.06367 train_rmse= 0.80370 val_loss= 1.48391 val_rmse= 1.14267 \t\ttime= 0.59840\n",
      "[*] Epoch: 1483 train_loss= 1.06516 train_rmse= 0.80070 val_loss= 1.48243 val_rmse= 1.14287 \t\ttime= 0.65026\n",
      "[*] Epoch: 1484 train_loss= 1.05623 train_rmse= 0.79933 val_loss= 1.48107 val_rmse= 1.14333 \t\ttime= 0.59142\n",
      "[*] Epoch: 1485 train_loss= 1.07034 train_rmse= 0.80666 val_loss= 1.47996 val_rmse= 1.14279 \t\ttime= 0.62234\n",
      "[*] Epoch: 1486 train_loss= 1.06988 train_rmse= 0.80374 val_loss= 1.48071 val_rmse= 1.14199 \t\ttime= 0.62233\n",
      "[*] Epoch: 1487 train_loss= 1.06795 train_rmse= 0.80236 val_loss= 1.48207 val_rmse= 1.14143 \t\ttime= 0.54255\n",
      "[*] Epoch: 1488 train_loss= 1.06688 train_rmse= 0.80372 val_loss= 1.48087 val_rmse= 1.13869 \t\ttime= 0.61137\n",
      "[*] Epoch: 1489 train_loss= 1.06830 train_rmse= 0.80731 val_loss= 1.47735 val_rmse= 1.13416 \t\ttime= 0.65624\n",
      "[*] Epoch: 1490 train_loss= 1.05335 train_rmse= 0.78982 val_loss= 1.47464 val_rmse= 1.13211 \t\ttime= 0.65824\n",
      "[*] Epoch: 1491 train_loss= 1.05301 train_rmse= 0.79277 val_loss= 1.47537 val_rmse= 1.13525 \t\ttime= 0.61236\n",
      "[*] Epoch: 1492 train_loss= 1.05611 train_rmse= 0.79550 val_loss= 1.47907 val_rmse= 1.14113 \t\ttime= 0.59939\n",
      "[*] Epoch: 1493 train_loss= 1.06534 train_rmse= 0.80096 val_loss= 1.48292 val_rmse= 1.14515 \t\ttime= 0.59541\n",
      "[*] Epoch: 1494 train_loss= 1.05346 train_rmse= 0.79493 val_loss= 1.48400 val_rmse= 1.14544 \t\ttime= 0.55851\n",
      "[*] Epoch: 1495 train_loss= 1.05680 train_rmse= 0.79215 val_loss= 1.48661 val_rmse= 1.14604 \t\ttime= 0.60339\n",
      "[*] Epoch: 1496 train_loss= 1.06446 train_rmse= 0.80396 val_loss= 1.48763 val_rmse= 1.14619 \t\ttime= 0.66025\n",
      "[*] Epoch: 1497 train_loss= 1.06921 train_rmse= 0.80436 val_loss= 1.48417 val_rmse= 1.14291 \t\ttime= 0.57546\n",
      "[*] Epoch: 1498 train_loss= 1.05772 train_rmse= 0.79398 val_loss= 1.48191 val_rmse= 1.14223 \t\ttime= 0.62733\n",
      "[*] Epoch: 1499 train_loss= 1.05313 train_rmse= 0.79303 val_loss= 1.47876 val_rmse= 1.14038 \t\ttime= 0.61037\n",
      "[*] Epoch: 1500 train_loss= 1.05620 train_rmse= 0.79619 val_loss= 1.47606 val_rmse= 1.13804 \t\ttime= 0.56050\n",
      "[*] Epoch: 1501 train_loss= 1.06826 train_rmse= 0.80474 val_loss= 1.47205 val_rmse= 1.13330 \t\ttime= 0.60937\n",
      "[*] Epoch: 1502 train_loss= 1.05442 train_rmse= 0.79675 val_loss= 1.47111 val_rmse= 1.13111 \t\ttime= 0.66123\n",
      "[*] Epoch: 1503 train_loss= 1.06632 train_rmse= 0.80284 val_loss= 1.47386 val_rmse= 1.13255 \t\ttime= 0.64328\n",
      "[*] Epoch: 1504 train_loss= 1.05507 train_rmse= 0.79197 val_loss= 1.47640 val_rmse= 1.13490 \t\ttime= 0.57646\n",
      "[*] Epoch: 1505 train_loss= 1.05822 train_rmse= 0.79620 val_loss= 1.47826 val_rmse= 1.13780 \t\ttime= 0.60638\n",
      "[*] Epoch: 1506 train_loss= 1.08254 train_rmse= 0.81606 val_loss= 1.47549 val_rmse= 1.13574 \t\ttime= 0.61934\n",
      "[*] Epoch: 1507 train_loss= 1.05377 train_rmse= 0.79835 val_loss= 1.47197 val_rmse= 1.13272 \t\ttime= 0.56050\n",
      "[*] Epoch: 1508 train_loss= 1.06437 train_rmse= 0.79905 val_loss= 1.47146 val_rmse= 1.13226 \t\ttime= 0.61037\n",
      "[*] Epoch: 1509 train_loss= 1.06283 train_rmse= 0.80051 val_loss= 1.47537 val_rmse= 1.13627 \t\ttime= 0.67919\n",
      "[*] Epoch: 1510 train_loss= 1.05475 train_rmse= 0.79990 val_loss= 1.47772 val_rmse= 1.13846 \t\ttime= 0.56848\n",
      "[*] Epoch: 1511 train_loss= 1.05331 train_rmse= 0.79668 val_loss= 1.47806 val_rmse= 1.13830 \t\ttime= 0.62433\n",
      "[*] Epoch: 1512 train_loss= 1.06922 train_rmse= 0.80733 val_loss= 1.47616 val_rmse= 1.13556 \t\ttime= 0.61835\n",
      "[*] Epoch: 1513 train_loss= 1.04570 train_rmse= 0.79152 val_loss= 1.47618 val_rmse= 1.13586 \t\ttime= 0.55751\n",
      "[*] Epoch: 1514 train_loss= 1.07118 train_rmse= 0.80442 val_loss= 1.47464 val_rmse= 1.13498 \t\ttime= 0.61536\n",
      "[*] Epoch: 1515 train_loss= 1.06608 train_rmse= 0.80444 val_loss= 1.47182 val_rmse= 1.13186 \t\ttime= 0.61136\n",
      "[*] Epoch: 1516 train_loss= 1.06588 train_rmse= 0.80810 val_loss= 1.47025 val_rmse= 1.12932 \t\ttime= 0.64827\n",
      "[*] Epoch: 1517 train_loss= 1.05176 train_rmse= 0.78757 val_loss= 1.47087 val_rmse= 1.12865 \t\ttime= 0.59141\n",
      "[*] Epoch: 1518 train_loss= 1.05808 train_rmse= 0.80016 val_loss= 1.47204 val_rmse= 1.12960 \t\ttime= 0.60040\n",
      "[*] Epoch: 1519 train_loss= 1.04776 train_rmse= 0.78847 val_loss= 1.47248 val_rmse= 1.13020 \t\ttime= 0.62333\n",
      "[*] Epoch: 1520 train_loss= 1.04940 train_rmse= 0.79173 val_loss= 1.47626 val_rmse= 1.13525 \t\ttime= 0.52560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1521 train_loss= 1.05575 train_rmse= 0.79530 val_loss= 1.47947 val_rmse= 1.13971 \t\ttime= 0.61735\n",
      "[*] Epoch: 1522 train_loss= 1.05225 train_rmse= 0.79586 val_loss= 1.47987 val_rmse= 1.14052 \t\ttime= 0.67520\n",
      "[*] Epoch: 1523 train_loss= 1.05619 train_rmse= 0.79866 val_loss= 1.48066 val_rmse= 1.14063 \t\ttime= 0.57347\n",
      "[*] Epoch: 1524 train_loss= 1.04747 train_rmse= 0.79275 val_loss= 1.48142 val_rmse= 1.14036 \t\ttime= 0.60837\n",
      "[*] Epoch: 1525 train_loss= 1.05179 train_rmse= 0.79448 val_loss= 1.48048 val_rmse= 1.13841 \t\ttime= 0.62732\n",
      "[*] Epoch: 1526 train_loss= 1.05588 train_rmse= 0.79972 val_loss= 1.47870 val_rmse= 1.13594 \t\ttime= 0.57945\n",
      "[*] Epoch: 1527 train_loss= 1.05707 train_rmse= 0.79746 val_loss= 1.47532 val_rmse= 1.13313 \t\ttime= 0.58843\n",
      "[*] Epoch: 1528 train_loss= 1.06458 train_rmse= 0.80494 val_loss= 1.47284 val_rmse= 1.13150 \t\ttime= 0.59940\n",
      "[*] Epoch: 1529 train_loss= 1.04912 train_rmse= 0.79182 val_loss= 1.47305 val_rmse= 1.13178 \t\ttime= 0.67220\n",
      "[*] Epoch: 1530 train_loss= 1.06092 train_rmse= 0.80007 val_loss= 1.47552 val_rmse= 1.13270 \t\ttime= 0.58245\n",
      "[*] Epoch: 1531 train_loss= 1.04849 train_rmse= 0.79016 val_loss= 1.47717 val_rmse= 1.13289 \t\ttime= 0.59541\n",
      "[*] Epoch: 1532 train_loss= 1.04090 train_rmse= 0.78432 val_loss= 1.47924 val_rmse= 1.13540 \t\ttime= 0.62234\n",
      "[*] Epoch: 1533 train_loss= 1.06010 train_rmse= 0.79750 val_loss= 1.48007 val_rmse= 1.13763 \t\ttime= 0.55551\n",
      "[*] Epoch: 1534 train_loss= 1.04466 train_rmse= 0.78816 val_loss= 1.47995 val_rmse= 1.13956 \t\ttime= 0.62434\n",
      "[*] Epoch: 1535 train_loss= 1.05879 train_rmse= 0.80064 val_loss= 1.47958 val_rmse= 1.13969 \t\ttime= 0.67220\n",
      "[*] Epoch: 1536 train_loss= 1.06882 train_rmse= 0.80920 val_loss= 1.47824 val_rmse= 1.13812 \t\ttime= 0.61136\n",
      "[*] Epoch: 1537 train_loss= 1.05153 train_rmse= 0.79459 val_loss= 1.47924 val_rmse= 1.13850 \t\ttime= 0.62732\n",
      "[*] Epoch: 1538 train_loss= 1.05187 train_rmse= 0.79527 val_loss= 1.48266 val_rmse= 1.14115 \t\ttime= 0.64527\n",
      "[*] Epoch: 1539 train_loss= 1.03692 train_rmse= 0.78337 val_loss= 1.48537 val_rmse= 1.14320 \t\ttime= 0.58045\n",
      "[*] Epoch: 1540 train_loss= 1.05560 train_rmse= 0.79697 val_loss= 1.48358 val_rmse= 1.14118 \t\ttime= 0.61037\n",
      "[*] Epoch: 1541 train_loss= 1.06430 train_rmse= 0.80625 val_loss= 1.47863 val_rmse= 1.13657 \t\ttime= 0.62034\n",
      "[*] Epoch: 1542 train_loss= 1.04691 train_rmse= 0.78795 val_loss= 1.47366 val_rmse= 1.13230 \t\ttime= 0.64527\n",
      "[*] Epoch: 1543 train_loss= 1.04365 train_rmse= 0.78523 val_loss= 1.47375 val_rmse= 1.13239 \t\ttime= 0.59840\n",
      "[*] Epoch: 1544 train_loss= 1.05521 train_rmse= 0.79855 val_loss= 1.47289 val_rmse= 1.13089 \t\ttime= 0.62732\n",
      "[*] Epoch: 1545 train_loss= 1.04605 train_rmse= 0.79273 val_loss= 1.47172 val_rmse= 1.12861 \t\ttime= 0.63031\n",
      "[*] Epoch: 1546 train_loss= 1.05219 train_rmse= 0.79509 val_loss= 1.47331 val_rmse= 1.13028 \t\ttime= 0.59242\n",
      "[*] Epoch: 1547 train_loss= 1.06210 train_rmse= 0.79794 val_loss= 1.47519 val_rmse= 1.13275 \t\ttime= 0.64727\n",
      "[*] Epoch: 1548 train_loss= 1.04709 train_rmse= 0.78950 val_loss= 1.47783 val_rmse= 1.13679 \t\ttime= 0.71011\n",
      "[*] Epoch: 1549 train_loss= 1.04481 train_rmse= 0.79389 val_loss= 1.47803 val_rmse= 1.13795 \t\ttime= 0.58643\n",
      "[*] Epoch: 1550 train_loss= 1.05253 train_rmse= 0.79309 val_loss= 1.47836 val_rmse= 1.13830 \t\ttime= 0.60538\n",
      "[*] Epoch: 1551 train_loss= 1.05122 train_rmse= 0.79650 val_loss= 1.47809 val_rmse= 1.13787 \t\ttime= 0.61934\n",
      "[*] Epoch: 1552 train_loss= 1.04325 train_rmse= 0.78711 val_loss= 1.48173 val_rmse= 1.14164 \t\ttime= 0.53258\n",
      "[*] Epoch: 1553 train_loss= 1.07114 train_rmse= 0.82013 val_loss= 1.48414 val_rmse= 1.14270 \t\ttime= 0.62234\n",
      "[*] Epoch: 1554 train_loss= 1.05580 train_rmse= 0.79629 val_loss= 1.48441 val_rmse= 1.14252 \t\ttime= 0.63231\n",
      "[*] Epoch: 1555 train_loss= 1.04667 train_rmse= 0.79131 val_loss= 1.48327 val_rmse= 1.14221 \t\ttime= 0.65624\n",
      "[*] Epoch: 1556 train_loss= 1.04241 train_rmse= 0.78726 val_loss= 1.48304 val_rmse= 1.14347 \t\ttime= 0.61336\n",
      "[*] Epoch: 1557 train_loss= 1.06290 train_rmse= 0.80718 val_loss= 1.47965 val_rmse= 1.14076 \t\ttime= 0.61136\n",
      "[*] Epoch: 1558 train_loss= 1.05291 train_rmse= 0.79451 val_loss= 1.47814 val_rmse= 1.13879 \t\ttime= 0.59641\n",
      "[*] Epoch: 1559 train_loss= 1.04707 train_rmse= 0.79014 val_loss= 1.47764 val_rmse= 1.13715 \t\ttime= 0.57746\n",
      "[*] Epoch: 1560 train_loss= 1.04024 train_rmse= 0.79272 val_loss= 1.47770 val_rmse= 1.13589 \t\ttime= 0.60338\n",
      "[*] Epoch: 1561 train_loss= 1.05589 train_rmse= 0.79801 val_loss= 1.47862 val_rmse= 1.13596 \t\ttime= 0.67320\n",
      "[*] Epoch: 1562 train_loss= 1.05392 train_rmse= 0.80282 val_loss= 1.47930 val_rmse= 1.13755 \t\ttime= 0.55950\n",
      "[*] Epoch: 1563 train_loss= 1.05096 train_rmse= 0.79406 val_loss= 1.47837 val_rmse= 1.13861 \t\ttime= 0.62034\n",
      "[*] Epoch: 1564 train_loss= 1.03862 train_rmse= 0.78695 val_loss= 1.48114 val_rmse= 1.14301 \t\ttime= 0.63530\n",
      "[*] Epoch: 1565 train_loss= 1.05536 train_rmse= 0.79195 val_loss= 1.48615 val_rmse= 1.14780 \t\ttime= 0.52558\n",
      "[*] Epoch: 1566 train_loss= 1.04437 train_rmse= 0.79302 val_loss= 1.48944 val_rmse= 1.14954 \t\ttime= 0.61835\n",
      "[*] Epoch: 1567 train_loss= 1.06106 train_rmse= 0.80116 val_loss= 1.48858 val_rmse= 1.14753 \t\ttime= 0.62632\n",
      "[*] Epoch: 1568 train_loss= 1.05653 train_rmse= 0.79749 val_loss= 1.48579 val_rmse= 1.14441 \t\ttime= 0.66721\n",
      "[*] Epoch: 1569 train_loss= 1.04940 train_rmse= 0.79266 val_loss= 1.48244 val_rmse= 1.14190 \t\ttime= 0.64029\n",
      "[*] Epoch: 1570 train_loss= 1.05008 train_rmse= 0.79193 val_loss= 1.48159 val_rmse= 1.14236 \t\ttime= 0.60538\n",
      "[*] Epoch: 1571 train_loss= 1.05832 train_rmse= 0.80232 val_loss= 1.48204 val_rmse= 1.14281 \t\ttime= 0.60039\n",
      "[*] Epoch: 1572 train_loss= 1.05866 train_rmse= 0.80052 val_loss= 1.48168 val_rmse= 1.14072 \t\ttime= 0.55950\n",
      "[*] Epoch: 1573 train_loss= 1.04037 train_rmse= 0.78564 val_loss= 1.48254 val_rmse= 1.14011 \t\ttime= 0.63630\n",
      "[*] Epoch: 1574 train_loss= 1.04331 train_rmse= 0.78823 val_loss= 1.48549 val_rmse= 1.14296 \t\ttime= 0.65624\n",
      "[*] Epoch: 1575 train_loss= 1.04969 train_rmse= 0.79304 val_loss= 1.48666 val_rmse= 1.14451 \t\ttime= 0.57446\n",
      "[*] Epoch: 1576 train_loss= 1.05517 train_rmse= 0.79780 val_loss= 1.48593 val_rmse= 1.14441 \t\ttime= 0.61436\n",
      "[*] Epoch: 1577 train_loss= 1.04418 train_rmse= 0.78764 val_loss= 1.48431 val_rmse= 1.14265 \t\ttime= 0.61934\n",
      "[*] Epoch: 1578 train_loss= 1.06033 train_rmse= 0.80039 val_loss= 1.48118 val_rmse= 1.13905 \t\ttime= 0.53856\n",
      "[*] Epoch: 1579 train_loss= 1.04466 train_rmse= 0.78905 val_loss= 1.47853 val_rmse= 1.13658 \t\ttime= 0.61336\n",
      "[*] Epoch: 1580 train_loss= 1.05007 train_rmse= 0.79179 val_loss= 1.47663 val_rmse= 1.13500 \t\ttime= 0.63630\n",
      "[*] Epoch: 1581 train_loss= 1.04442 train_rmse= 0.78885 val_loss= 1.47742 val_rmse= 1.13562 \t\ttime= 0.65226\n",
      "[*] Epoch: 1582 train_loss= 1.04207 train_rmse= 0.78367 val_loss= 1.48033 val_rmse= 1.13856 \t\ttime= 0.61536\n",
      "[*] Epoch: 1583 train_loss= 1.03929 train_rmse= 0.78487 val_loss= 1.48505 val_rmse= 1.14390 \t\ttime= 0.61436\n",
      "[*] Epoch: 1584 train_loss= 1.04423 train_rmse= 0.78795 val_loss= 1.48806 val_rmse= 1.14800 \t\ttime= 0.61236\n",
      "[*] Epoch: 1585 train_loss= 1.03266 train_rmse= 0.78279 val_loss= 1.48899 val_rmse= 1.14898 \t\ttime= 0.57347\n",
      "[*] Epoch: 1586 train_loss= 1.05137 train_rmse= 0.79364 val_loss= 1.48800 val_rmse= 1.14767 \t\ttime= 0.58742\n",
      "[*] Epoch: 1587 train_loss= 1.05492 train_rmse= 0.79900 val_loss= 1.48403 val_rmse= 1.14337 \t\ttime= 0.67819\n",
      "[*] Epoch: 1588 train_loss= 1.06570 train_rmse= 0.80619 val_loss= 1.47806 val_rmse= 1.13713 \t\ttime= 0.59840\n",
      "[*] Epoch: 1589 train_loss= 1.04964 train_rmse= 0.79559 val_loss= 1.47208 val_rmse= 1.13155 \t\ttime= 0.60438\n",
      "[*] Epoch: 1590 train_loss= 1.05004 train_rmse= 0.79504 val_loss= 1.46813 val_rmse= 1.12771 \t\ttime= 0.61934\n",
      "[*] Epoch: 1591 train_loss= 1.05488 train_rmse= 0.79327 val_loss= 1.47083 val_rmse= 1.12952 \t\ttime= 0.54056\n",
      "[*] Epoch: 1592 train_loss= 1.05037 train_rmse= 0.79404 val_loss= 1.47438 val_rmse= 1.13194 \t\ttime= 0.63032\n",
      "[*] Epoch: 1593 train_loss= 1.04271 train_rmse= 0.78725 val_loss= 1.47656 val_rmse= 1.13393 \t\ttime= 0.66921\n",
      "[*] Epoch: 1594 train_loss= 1.04304 train_rmse= 0.78430 val_loss= 1.47995 val_rmse= 1.13796 \t\ttime= 0.67818\n",
      "[*] Epoch: 1595 train_loss= 1.05311 train_rmse= 0.79192 val_loss= 1.48241 val_rmse= 1.14216 \t\ttime= 0.62832\n",
      "[*] Epoch: 1596 train_loss= 1.04946 train_rmse= 0.79286 val_loss= 1.48610 val_rmse= 1.14711 \t\ttime= 0.66922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1597 train_loss= 1.06234 train_rmse= 0.80424 val_loss= 1.48892 val_rmse= 1.15021 \t\ttime= 0.61137\n",
      "[*] Epoch: 1598 train_loss= 1.05600 train_rmse= 0.80015 val_loss= 1.48859 val_rmse= 1.14927 \t\ttime= 0.69314\n",
      "[*] Epoch: 1599 train_loss= 1.04244 train_rmse= 0.78780 val_loss= 1.48711 val_rmse= 1.14756 \t\ttime= 0.68417\n",
      "[*] Epoch: 1600 train_loss= 1.05282 train_rmse= 0.79567 val_loss= 1.48305 val_rmse= 1.14364 \t\ttime= 0.66622\n",
      "[*] Epoch: 1601 train_loss= 1.04181 train_rmse= 0.79162 val_loss= 1.48228 val_rmse= 1.14329 \t\ttime= 0.62134\n",
      "[*] Epoch: 1602 train_loss= 1.04590 train_rmse= 0.79114 val_loss= 1.48203 val_rmse= 1.14280 \t\ttime= 0.63730\n",
      "[*] Epoch: 1603 train_loss= 1.05240 train_rmse= 0.79635 val_loss= 1.48380 val_rmse= 1.14444 \t\ttime= 0.62033\n",
      "[*] Epoch: 1604 train_loss= 1.04465 train_rmse= 0.78950 val_loss= 1.48548 val_rmse= 1.14586 \t\ttime= 0.65425\n",
      "[*] Epoch: 1605 train_loss= 1.05448 train_rmse= 0.79518 val_loss= 1.48709 val_rmse= 1.14719 \t\ttime= 0.62931\n",
      "[*] Epoch: 1606 train_loss= 1.04709 train_rmse= 0.79681 val_loss= 1.48843 val_rmse= 1.14823 \t\ttime= 0.67320\n",
      "[*] Epoch: 1607 train_loss= 1.06975 train_rmse= 0.80576 val_loss= 1.48770 val_rmse= 1.14759 \t\ttime= 0.62632\n",
      "[*] Epoch: 1608 train_loss= 1.06237 train_rmse= 0.80439 val_loss= 1.48574 val_rmse= 1.14540 \t\ttime= 0.60538\n",
      "[*] Epoch: 1609 train_loss= 1.05713 train_rmse= 0.79916 val_loss= 1.48345 val_rmse= 1.14289 \t\ttime= 0.61735\n",
      "[*] Epoch: 1610 train_loss= 1.06383 train_rmse= 0.80901 val_loss= 1.48055 val_rmse= 1.13936 \t\ttime= 0.60638\n",
      "[*] Epoch: 1611 train_loss= 1.02923 train_rmse= 0.77997 val_loss= 1.47912 val_rmse= 1.13769 \t\ttime= 0.61635\n",
      "[*] Epoch: 1612 train_loss= 1.03913 train_rmse= 0.78673 val_loss= 1.47705 val_rmse= 1.13590 \t\ttime= 0.70812\n",
      "[*] Epoch: 1613 train_loss= 1.03438 train_rmse= 0.78302 val_loss= 1.47731 val_rmse= 1.13647 \t\ttime= 0.59441\n",
      "[*] Epoch: 1614 train_loss= 1.04421 train_rmse= 0.79084 val_loss= 1.47694 val_rmse= 1.13607 \t\ttime= 0.62732\n",
      "[*] Epoch: 1615 train_loss= 1.04747 train_rmse= 0.79547 val_loss= 1.47783 val_rmse= 1.13733 \t\ttime= 0.65226\n",
      "[*] Epoch: 1616 train_loss= 1.03242 train_rmse= 0.78119 val_loss= 1.48132 val_rmse= 1.14104 \t\ttime= 0.61037\n",
      "[*] Epoch: 1617 train_loss= 1.05155 train_rmse= 0.79399 val_loss= 1.48324 val_rmse= 1.14327 \t\ttime= 0.64129\n",
      "[*] Epoch: 1618 train_loss= 1.04204 train_rmse= 0.78779 val_loss= 1.48421 val_rmse= 1.14430 \t\ttime= 0.68816\n",
      "[*] Epoch: 1619 train_loss= 1.04210 train_rmse= 0.78869 val_loss= 1.48539 val_rmse= 1.14591 \t\ttime= 0.60936\n",
      "[*] Epoch: 1620 train_loss= 1.03244 train_rmse= 0.78202 val_loss= 1.48632 val_rmse= 1.14684 \t\ttime= 0.63431\n",
      "[*] Epoch: 1621 train_loss= 1.04093 train_rmse= 0.78652 val_loss= 1.48900 val_rmse= 1.14962 \t\ttime= 0.61636\n",
      "[*] Epoch: 1622 train_loss= 1.04267 train_rmse= 0.79211 val_loss= 1.48900 val_rmse= 1.14956 \t\ttime= 0.58244\n",
      "[*] Epoch: 1623 train_loss= 1.05214 train_rmse= 0.79525 val_loss= 1.48712 val_rmse= 1.14785 \t\ttime= 0.60240\n",
      "[*] Epoch: 1624 train_loss= 1.05115 train_rmse= 0.79610 val_loss= 1.48465 val_rmse= 1.14593 \t\ttime= 0.63032\n",
      "[*] Epoch: 1625 train_loss= 1.04351 train_rmse= 0.78796 val_loss= 1.48148 val_rmse= 1.14203 \t\ttime= 0.63630\n",
      "[*] Epoch: 1626 train_loss= 1.03932 train_rmse= 0.78285 val_loss= 1.48218 val_rmse= 1.14220 \t\ttime= 0.66522\n",
      "[*] Epoch: 1627 train_loss= 1.03716 train_rmse= 0.78701 val_loss= 1.48242 val_rmse= 1.14179 \t\ttime= 0.63429\n",
      "[*] Epoch: 1628 train_loss= 1.03643 train_rmse= 0.78569 val_loss= 1.48340 val_rmse= 1.14205 \t\ttime= 0.63430\n",
      "[*] Epoch: 1629 train_loss= 1.04346 train_rmse= 0.78897 val_loss= 1.48641 val_rmse= 1.14496 \t\ttime= 0.58744\n",
      "[*] Epoch: 1630 train_loss= 1.04085 train_rmse= 0.79540 val_loss= 1.48850 val_rmse= 1.14651 \t\ttime= 0.59242\n",
      "[*] Epoch: 1631 train_loss= 1.04923 train_rmse= 0.79700 val_loss= 1.48770 val_rmse= 1.14615 \t\ttime= 0.66023\n",
      "[*] Epoch: 1632 train_loss= 1.04032 train_rmse= 0.78944 val_loss= 1.48559 val_rmse= 1.14437 \t\ttime= 0.56549\n",
      "[*] Epoch: 1633 train_loss= 1.05860 train_rmse= 0.79998 val_loss= 1.48381 val_rmse= 1.14259 \t\ttime= 0.62134\n",
      "[*] Epoch: 1634 train_loss= 1.04360 train_rmse= 0.79006 val_loss= 1.48216 val_rmse= 1.14042 \t\ttime= 0.61036\n",
      "[*] Epoch: 1635 train_loss= 1.06518 train_rmse= 0.80273 val_loss= 1.47930 val_rmse= 1.13787 \t\ttime= 0.55253\n",
      "[*] Epoch: 1636 train_loss= 1.03650 train_rmse= 0.78093 val_loss= 1.47919 val_rmse= 1.13779 \t\ttime= 0.64827\n",
      "[*] Epoch: 1637 train_loss= 1.03062 train_rmse= 0.77944 val_loss= 1.48125 val_rmse= 1.13981 \t\ttime= 0.62433\n",
      "[*] Epoch: 1638 train_loss= 1.04455 train_rmse= 0.78953 val_loss= 1.48458 val_rmse= 1.14193 \t\ttime= 0.65425\n",
      "[*] Epoch: 1639 train_loss= 1.04615 train_rmse= 0.78589 val_loss= 1.48684 val_rmse= 1.14340 \t\ttime= 0.60638\n",
      "[*] Epoch: 1640 train_loss= 1.04442 train_rmse= 0.78855 val_loss= 1.48759 val_rmse= 1.14494 \t\ttime= 0.58942\n",
      "[*] Epoch: 1641 train_loss= 1.05264 train_rmse= 0.79640 val_loss= 1.48720 val_rmse= 1.14595 \t\ttime= 0.60737\n",
      "[*] Epoch: 1642 train_loss= 1.03842 train_rmse= 0.78733 val_loss= 1.48625 val_rmse= 1.14682 \t\ttime= 0.55751\n",
      "[*] Epoch: 1643 train_loss= 1.04447 train_rmse= 0.79093 val_loss= 1.48342 val_rmse= 1.14480 \t\ttime= 0.65824\n",
      "[*] Epoch: 1644 train_loss= 1.03959 train_rmse= 0.78806 val_loss= 1.48071 val_rmse= 1.14125 \t\ttime= 0.67719\n",
      "[*] Epoch: 1645 train_loss= 1.03667 train_rmse= 0.78329 val_loss= 1.48154 val_rmse= 1.14074 \t\ttime= 0.55452\n",
      "[*] Epoch: 1646 train_loss= 1.04292 train_rmse= 0.79139 val_loss= 1.48579 val_rmse= 1.14476 \t\ttime= 0.61436\n",
      "[*] Epoch: 1647 train_loss= 1.03874 train_rmse= 0.79057 val_loss= 1.48823 val_rmse= 1.14748 \t\ttime= 0.63630\n",
      "[*] Epoch: 1648 train_loss= 1.04206 train_rmse= 0.78889 val_loss= 1.48826 val_rmse= 1.14820 \t\ttime= 0.55551\n",
      "[*] Epoch: 1649 train_loss= 1.05435 train_rmse= 0.79683 val_loss= 1.48740 val_rmse= 1.14795 \t\ttime= 0.63729\n",
      "[*] Epoch: 1650 train_loss= 1.04708 train_rmse= 0.78734 val_loss= 1.48713 val_rmse= 1.14770 \t\ttime= 0.62932\n",
      "[*] Epoch: 1651 train_loss= 1.05421 train_rmse= 0.79945 val_loss= 1.48409 val_rmse= 1.14410 \t\ttime= 0.65325\n",
      "[*] Epoch: 1652 train_loss= 1.04165 train_rmse= 0.78723 val_loss= 1.48308 val_rmse= 1.14257 \t\ttime= 0.64129\n",
      "[*] Epoch: 1653 train_loss= 1.03506 train_rmse= 0.78330 val_loss= 1.48459 val_rmse= 1.14370 \t\ttime= 0.62234\n",
      "[*] Epoch: 1654 train_loss= 1.03679 train_rmse= 0.78766 val_loss= 1.48585 val_rmse= 1.14562 \t\ttime= 0.58544\n",
      "[*] Epoch: 1655 train_loss= 1.04787 train_rmse= 0.79986 val_loss= 1.48878 val_rmse= 1.15007 \t\ttime= 0.57945\n",
      "[*] Epoch: 1656 train_loss= 1.04858 train_rmse= 0.79331 val_loss= 1.48867 val_rmse= 1.15090 \t\ttime= 0.58942\n",
      "[*] Epoch: 1657 train_loss= 1.03251 train_rmse= 0.78121 val_loss= 1.48751 val_rmse= 1.14942 \t\ttime= 0.68417\n",
      "[*] Epoch: 1658 train_loss= 1.04544 train_rmse= 0.78918 val_loss= 1.48636 val_rmse= 1.14716 \t\ttime= 0.56249\n",
      "[*] Epoch: 1659 train_loss= 1.03478 train_rmse= 0.78868 val_loss= 1.48598 val_rmse= 1.14568 \t\ttime= 0.64428\n",
      "[*] Epoch: 1660 train_loss= 1.04106 train_rmse= 0.78726 val_loss= 1.48417 val_rmse= 1.14335 \t\ttime= 0.63630\n",
      "[*] Epoch: 1661 train_loss= 1.04168 train_rmse= 0.78518 val_loss= 1.48197 val_rmse= 1.14151 \t\ttime= 0.56150\n",
      "[*] Epoch: 1662 train_loss= 1.03927 train_rmse= 0.78648 val_loss= 1.47980 val_rmse= 1.13981 \t\ttime= 0.62633\n",
      "[*] Epoch: 1663 train_loss= 1.04975 train_rmse= 0.79589 val_loss= 1.48095 val_rmse= 1.14147 \t\ttime= 0.66722\n",
      "[*] Epoch: 1664 train_loss= 1.03020 train_rmse= 0.78404 val_loss= 1.48218 val_rmse= 1.14185 \t\ttime= 0.57845\n",
      "[*] Epoch: 1665 train_loss= 1.02632 train_rmse= 0.77818 val_loss= 1.48290 val_rmse= 1.14132 \t\ttime= 0.63231\n",
      "[*] Epoch: 1666 train_loss= 1.03028 train_rmse= 0.78117 val_loss= 1.48708 val_rmse= 1.14488 \t\ttime= 0.62832\n",
      "[*] Epoch: 1667 train_loss= 1.03773 train_rmse= 0.78562 val_loss= 1.48812 val_rmse= 1.14578 \t\ttime= 0.60239\n",
      "[*] Epoch: 1668 train_loss= 1.04001 train_rmse= 0.78808 val_loss= 1.48691 val_rmse= 1.14559 \t\ttime= 0.59740\n",
      "[*] Epoch: 1669 train_loss= 1.04138 train_rmse= 0.78764 val_loss= 1.48432 val_rmse= 1.14476 \t\ttime= 0.62333\n",
      "[*] Epoch: 1670 train_loss= 1.02118 train_rmse= 0.77393 val_loss= 1.48113 val_rmse= 1.14254 \t\ttime= 0.67420\n",
      "[*] Epoch: 1671 train_loss= 1.03821 train_rmse= 0.78326 val_loss= 1.48082 val_rmse= 1.14209 \t\ttime= 0.58843\n",
      "[*] Epoch: 1672 train_loss= 1.05640 train_rmse= 0.80233 val_loss= 1.48093 val_rmse= 1.14057 \t\ttime= 0.59341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1673 train_loss= 1.04938 train_rmse= 0.79325 val_loss= 1.48231 val_rmse= 1.14025 \t\ttime= 0.63031\n",
      "[*] Epoch: 1674 train_loss= 1.02380 train_rmse= 0.77811 val_loss= 1.48483 val_rmse= 1.14296 \t\ttime= 0.60339\n",
      "[*] Epoch: 1675 train_loss= 1.04406 train_rmse= 0.78612 val_loss= 1.48595 val_rmse= 1.14572 \t\ttime= 0.62433\n",
      "[*] Epoch: 1676 train_loss= 1.04952 train_rmse= 0.79600 val_loss= 1.48258 val_rmse= 1.14363 \t\ttime= 0.68916\n",
      "[*] Epoch: 1677 train_loss= 1.04490 train_rmse= 0.79151 val_loss= 1.48011 val_rmse= 1.14141 \t\ttime= 0.59142\n",
      "[*] Epoch: 1678 train_loss= 1.04281 train_rmse= 0.78738 val_loss= 1.47906 val_rmse= 1.13928 \t\ttime= 0.62632\n",
      "[*] Epoch: 1679 train_loss= 1.03820 train_rmse= 0.78525 val_loss= 1.48104 val_rmse= 1.13974 \t\ttime= 0.65424\n",
      "[*] Epoch: 1680 train_loss= 1.04740 train_rmse= 0.79136 val_loss= 1.48404 val_rmse= 1.14149 \t\ttime= 0.63929\n",
      "[*] Epoch: 1681 train_loss= 1.03610 train_rmse= 0.78563 val_loss= 1.48414 val_rmse= 1.14128 \t\ttime= 0.64328\n",
      "[*] Epoch: 1682 train_loss= 1.04737 train_rmse= 0.79088 val_loss= 1.48422 val_rmse= 1.14323 \t\ttime= 0.79787\n",
      "[*] Epoch: 1683 train_loss= 1.04710 train_rmse= 0.79591 val_loss= 1.48312 val_rmse= 1.14371 \t\ttime= 0.58045\n",
      "[*] Epoch: 1684 train_loss= 1.04640 train_rmse= 0.79192 val_loss= 1.48154 val_rmse= 1.14308 \t\ttime= 0.61336\n",
      "[*] Epoch: 1685 train_loss= 1.04103 train_rmse= 0.78350 val_loss= 1.48121 val_rmse= 1.14188 \t\ttime= 0.62932\n",
      "[*] Epoch: 1686 train_loss= 1.02724 train_rmse= 0.77829 val_loss= 1.48189 val_rmse= 1.14022 \t\ttime= 0.56349\n",
      "[*] Epoch: 1687 train_loss= 1.03815 train_rmse= 0.78601 val_loss= 1.48187 val_rmse= 1.13818 \t\ttime= 0.67819\n",
      "[*] Epoch: 1688 train_loss= 1.03833 train_rmse= 0.78836 val_loss= 1.48139 val_rmse= 1.13711 \t\ttime= 0.76197\n",
      "[*] Epoch: 1689 train_loss= 1.02799 train_rmse= 0.77842 val_loss= 1.48211 val_rmse= 1.13907 \t\ttime= 0.75299\n",
      "[*] Epoch: 1690 train_loss= 1.02872 train_rmse= 0.77773 val_loss= 1.48237 val_rmse= 1.14096 \t\ttime= 0.74102\n",
      "[*] Epoch: 1691 train_loss= 1.02813 train_rmse= 0.78333 val_loss= 1.48447 val_rmse= 1.14413 \t\ttime= 0.79287\n",
      "[*] Epoch: 1692 train_loss= 1.03229 train_rmse= 0.78485 val_loss= 1.48647 val_rmse= 1.14627 \t\ttime= 0.65824\n",
      "[*] Epoch: 1693 train_loss= 1.02782 train_rmse= 0.78038 val_loss= 1.48768 val_rmse= 1.14659 \t\ttime= 0.62234\n",
      "[*] Epoch: 1694 train_loss= 1.04530 train_rmse= 0.79283 val_loss= 1.48666 val_rmse= 1.14432 \t\ttime= 0.68816\n",
      "[*] Epoch: 1695 train_loss= 1.04823 train_rmse= 0.79667 val_loss= 1.48309 val_rmse= 1.13985 \t\ttime= 0.53656\n",
      "[*] Epoch: 1696 train_loss= 1.04498 train_rmse= 0.78975 val_loss= 1.47800 val_rmse= 1.13486 \t\ttime= 0.60739\n",
      "[*] Epoch: 1697 train_loss= 1.04065 train_rmse= 0.78601 val_loss= 1.47719 val_rmse= 1.13523 \t\ttime= 0.63630\n",
      "[*] Epoch: 1698 train_loss= 1.03089 train_rmse= 0.78293 val_loss= 1.47599 val_rmse= 1.13497 \t\ttime= 0.53757\n",
      "[*] Epoch: 1699 train_loss= 1.03419 train_rmse= 0.78163 val_loss= 1.47806 val_rmse= 1.13736 \t\ttime= 0.60439\n",
      "[*] Epoch: 1700 train_loss= 1.02602 train_rmse= 0.77550 val_loss= 1.48173 val_rmse= 1.14100 \t\ttime= 0.61935\n",
      "[*] Epoch: 1701 train_loss= 1.05049 train_rmse= 0.79512 val_loss= 1.48186 val_rmse= 1.13998 \t\ttime= 0.64627\n",
      "[*] Epoch: 1702 train_loss= 1.02585 train_rmse= 0.78142 val_loss= 1.47997 val_rmse= 1.13695 \t\ttime= 0.60538\n",
      "[*] Epoch: 1703 train_loss= 1.03816 train_rmse= 0.78725 val_loss= 1.47899 val_rmse= 1.13554 \t\ttime= 0.56848\n",
      "[*] Epoch: 1704 train_loss= 1.03837 train_rmse= 0.78578 val_loss= 1.47803 val_rmse= 1.13542 \t\ttime= 0.57845\n",
      "[*] Epoch: 1705 train_loss= 1.03934 train_rmse= 0.78873 val_loss= 1.47681 val_rmse= 1.13535 \t\ttime= 0.55851\n",
      "[*] Epoch: 1706 train_loss= 1.03561 train_rmse= 0.78369 val_loss= 1.47575 val_rmse= 1.13507 \t\ttime= 0.62932\n",
      "[*] Epoch: 1707 train_loss= 1.02980 train_rmse= 0.78159 val_loss= 1.47657 val_rmse= 1.13499 \t\ttime= 0.67619\n",
      "[*] Epoch: 1708 train_loss= 1.04010 train_rmse= 0.79243 val_loss= 1.47838 val_rmse= 1.13683 \t\ttime= 0.59840\n",
      "[*] Epoch: 1709 train_loss= 1.03506 train_rmse= 0.78231 val_loss= 1.48022 val_rmse= 1.13874 \t\ttime= 0.61037\n",
      "[*] Epoch: 1710 train_loss= 1.03668 train_rmse= 0.78451 val_loss= 1.48108 val_rmse= 1.13931 \t\ttime= 0.61535\n",
      "[*] Epoch: 1711 train_loss= 1.03846 train_rmse= 0.78288 val_loss= 1.48217 val_rmse= 1.14166 \t\ttime= 0.54953\n",
      "[*] Epoch: 1712 train_loss= 1.02405 train_rmse= 0.77908 val_loss= 1.48105 val_rmse= 1.14152 \t\ttime= 0.59142\n",
      "[*] Epoch: 1713 train_loss= 1.03944 train_rmse= 0.78447 val_loss= 1.48070 val_rmse= 1.14171 \t\ttime= 0.62234\n",
      "[*] Epoch: 1714 train_loss= 1.03365 train_rmse= 0.77873 val_loss= 1.48160 val_rmse= 1.14167 \t\ttime= 0.66821\n",
      "[*] Epoch: 1715 train_loss= 1.03434 train_rmse= 0.78426 val_loss= 1.48147 val_rmse= 1.14020 \t\ttime= 0.57945\n",
      "[*] Epoch: 1716 train_loss= 1.01794 train_rmse= 0.77258 val_loss= 1.48298 val_rmse= 1.14033 \t\ttime= 0.59341\n",
      "[*] Epoch: 1717 train_loss= 1.02490 train_rmse= 0.77004 val_loss= 1.48781 val_rmse= 1.14568 \t\ttime= 0.61835\n",
      "[*] Epoch: 1718 train_loss= 1.02851 train_rmse= 0.77699 val_loss= 1.49017 val_rmse= 1.14929 \t\ttime= 0.55552\n",
      "[*] Epoch: 1719 train_loss= 1.03281 train_rmse= 0.78731 val_loss= 1.48946 val_rmse= 1.14948 \t\ttime= 0.60837\n",
      "[*] Epoch: 1720 train_loss= 1.02661 train_rmse= 0.78390 val_loss= 1.48645 val_rmse= 1.14595 \t\ttime= 0.65724\n",
      "[*] Epoch: 1721 train_loss= 1.02417 train_rmse= 0.77951 val_loss= 1.48355 val_rmse= 1.14230 \t\ttime= 0.59242\n",
      "[*] Epoch: 1722 train_loss= 1.03486 train_rmse= 0.78517 val_loss= 1.48263 val_rmse= 1.14116 \t\ttime= 0.65824\n",
      "[*] Epoch: 1723 train_loss= 1.02536 train_rmse= 0.78020 val_loss= 1.48282 val_rmse= 1.14146 \t\ttime= 0.61635\n",
      "[*] Epoch: 1724 train_loss= 1.03697 train_rmse= 0.78432 val_loss= 1.48186 val_rmse= 1.14026 \t\ttime= 0.55651\n",
      "[*] Epoch: 1725 train_loss= 1.02590 train_rmse= 0.77584 val_loss= 1.48263 val_rmse= 1.14023 \t\ttime= 0.60339\n",
      "[*] Epoch: 1726 train_loss= 1.03496 train_rmse= 0.78806 val_loss= 1.48182 val_rmse= 1.13882 \t\ttime= 0.59441\n",
      "[*] Epoch: 1727 train_loss= 1.02327 train_rmse= 0.77773 val_loss= 1.48187 val_rmse= 1.13916 \t\ttime= 0.64627\n",
      "[*] Epoch: 1728 train_loss= 1.04427 train_rmse= 0.78924 val_loss= 1.48110 val_rmse= 1.13887 \t\ttime= 0.57447\n",
      "[*] Epoch: 1729 train_loss= 1.04063 train_rmse= 0.78710 val_loss= 1.48015 val_rmse= 1.13871 \t\ttime= 0.58643\n",
      "[*] Epoch: 1730 train_loss= 1.03389 train_rmse= 0.78432 val_loss= 1.47986 val_rmse= 1.13916 \t\ttime= 0.62632\n",
      "[*] Epoch: 1731 train_loss= 1.05720 train_rmse= 0.79984 val_loss= 1.47887 val_rmse= 1.13821 \t\ttime= 0.55452\n",
      "[*] Epoch: 1732 train_loss= 1.03677 train_rmse= 0.78298 val_loss= 1.47806 val_rmse= 1.13747 \t\ttime= 0.61336\n",
      "[*] Epoch: 1733 train_loss= 1.04120 train_rmse= 0.78983 val_loss= 1.47690 val_rmse= 1.13592 \t\ttime= 0.65026\n",
      "[*] Epoch: 1734 train_loss= 1.02171 train_rmse= 0.77306 val_loss= 1.47799 val_rmse= 1.13694 \t\ttime= 0.59641\n",
      "[*] Epoch: 1735 train_loss= 1.03310 train_rmse= 0.77845 val_loss= 1.48023 val_rmse= 1.13962 \t\ttime= 0.61835\n",
      "[*] Epoch: 1736 train_loss= 1.03455 train_rmse= 0.78744 val_loss= 1.48109 val_rmse= 1.14061 \t\ttime= 0.60239\n",
      "[*] Epoch: 1737 train_loss= 1.04118 train_rmse= 0.79266 val_loss= 1.48066 val_rmse= 1.14011 \t\ttime= 0.58843\n",
      "[*] Epoch: 1738 train_loss= 1.02747 train_rmse= 0.78062 val_loss= 1.47832 val_rmse= 1.13771 \t\ttime= 0.60339\n",
      "[*] Epoch: 1739 train_loss= 1.03278 train_rmse= 0.78487 val_loss= 1.47652 val_rmse= 1.13588 \t\ttime= 0.59541\n",
      "[*] Epoch: 1740 train_loss= 1.02950 train_rmse= 0.77468 val_loss= 1.47789 val_rmse= 1.13738 \t\ttime= 0.65325\n",
      "[*] Epoch: 1741 train_loss= 1.04563 train_rmse= 0.79527 val_loss= 1.47926 val_rmse= 1.13883 \t\ttime= 0.55950\n",
      "[*] Epoch: 1742 train_loss= 1.03229 train_rmse= 0.78683 val_loss= 1.48054 val_rmse= 1.14001 \t\ttime= 0.60638\n",
      "[*] Epoch: 1743 train_loss= 1.03200 train_rmse= 0.78116 val_loss= 1.48221 val_rmse= 1.14089 \t\ttime= 0.63830\n",
      "[*] Epoch: 1744 train_loss= 1.02682 train_rmse= 0.77732 val_loss= 1.48511 val_rmse= 1.14401 \t\ttime= 0.51961\n",
      "[*] Epoch: 1745 train_loss= 1.02254 train_rmse= 0.77541 val_loss= 1.48711 val_rmse= 1.14686 \t\ttime= 0.61535\n",
      "[*] Epoch: 1746 train_loss= 1.05419 train_rmse= 0.80123 val_loss= 1.48536 val_rmse= 1.14519 \t\ttime= 0.63131\n",
      "[*] Epoch: 1747 train_loss= 1.03798 train_rmse= 0.78468 val_loss= 1.48331 val_rmse= 1.14262 \t\ttime= 0.65524\n",
      "[*] Epoch: 1748 train_loss= 1.03508 train_rmse= 0.78631 val_loss= 1.48226 val_rmse= 1.14110 \t\ttime= 0.61436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1749 train_loss= 1.01656 train_rmse= 0.77225 val_loss= 1.48021 val_rmse= 1.13920 \t\ttime= 0.61635\n",
      "[*] Epoch: 1750 train_loss= 1.01843 train_rmse= 0.77908 val_loss= 1.48051 val_rmse= 1.14021 \t\ttime= 0.60139\n",
      "[*] Epoch: 1751 train_loss= 1.01969 train_rmse= 0.77896 val_loss= 1.48128 val_rmse= 1.14116 \t\ttime= 0.60239\n",
      "[*] Epoch: 1752 train_loss= 1.02826 train_rmse= 0.78006 val_loss= 1.48121 val_rmse= 1.14039 \t\ttime= 0.60738\n",
      "[*] Epoch: 1753 train_loss= 1.03396 train_rmse= 0.78491 val_loss= 1.48224 val_rmse= 1.13993 \t\ttime= 0.68617\n",
      "[*] Epoch: 1754 train_loss= 1.03599 train_rmse= 0.78446 val_loss= 1.48227 val_rmse= 1.13940 \t\ttime= 0.60738\n",
      "[*] Epoch: 1755 train_loss= 1.03746 train_rmse= 0.78433 val_loss= 1.48295 val_rmse= 1.14048 \t\ttime= 0.64727\n",
      "[*] Epoch: 1756 train_loss= 1.03019 train_rmse= 0.77969 val_loss= 1.48489 val_rmse= 1.14368 \t\ttime= 0.63730\n",
      "[*] Epoch: 1757 train_loss= 1.02961 train_rmse= 0.78308 val_loss= 1.48632 val_rmse= 1.14654 \t\ttime= 0.54853\n",
      "[*] Epoch: 1758 train_loss= 1.02909 train_rmse= 0.78024 val_loss= 1.48667 val_rmse= 1.14659 \t\ttime= 0.62433\n",
      "[*] Epoch: 1759 train_loss= 1.02959 train_rmse= 0.78019 val_loss= 1.48746 val_rmse= 1.14600 \t\ttime= 0.70512\n",
      "[*] Epoch: 1760 train_loss= 1.04180 train_rmse= 0.78974 val_loss= 1.48753 val_rmse= 1.14434 \t\ttime= 0.58842\n",
      "[*] Epoch: 1761 train_loss= 1.04640 train_rmse= 0.78932 val_loss= 1.48671 val_rmse= 1.14357 \t\ttime= 0.62234\n",
      "[*] Epoch: 1762 train_loss= 1.01872 train_rmse= 0.77443 val_loss= 1.48559 val_rmse= 1.14354 \t\ttime= 0.64727\n",
      "[*] Epoch: 1763 train_loss= 1.01866 train_rmse= 0.77263 val_loss= 1.48631 val_rmse= 1.14564 \t\ttime= 0.61835\n",
      "[*] Epoch: 1764 train_loss= 1.01439 train_rmse= 0.77139 val_loss= 1.48649 val_rmse= 1.14589 \t\ttime= 0.65425\n",
      "[*] Epoch: 1765 train_loss= 1.03271 train_rmse= 0.79029 val_loss= 1.48706 val_rmse= 1.14594 \t\ttime= 0.65226\n",
      "[*] Epoch: 1766 train_loss= 1.02354 train_rmse= 0.78164 val_loss= 1.48749 val_rmse= 1.14565 \t\ttime= 0.63530\n",
      "[*] Epoch: 1767 train_loss= 1.01463 train_rmse= 0.77300 val_loss= 1.48748 val_rmse= 1.14487 \t\ttime= 0.61635\n",
      "[*] Epoch: 1768 train_loss= 1.01907 train_rmse= 0.77347 val_loss= 1.48747 val_rmse= 1.14565 \t\ttime= 0.56848\n",
      "[*] Epoch: 1769 train_loss= 1.02623 train_rmse= 0.78014 val_loss= 1.48673 val_rmse= 1.14635 \t\ttime= 0.58942\n",
      "[*] Epoch: 1770 train_loss= 1.02522 train_rmse= 0.77918 val_loss= 1.48872 val_rmse= 1.14925 \t\ttime= 0.55252\n",
      "[*] Epoch: 1771 train_loss= 1.02419 train_rmse= 0.78034 val_loss= 1.48939 val_rmse= 1.14960 \t\ttime= 0.62234\n",
      "[*] Epoch: 1772 train_loss= 1.03242 train_rmse= 0.78100 val_loss= 1.48764 val_rmse= 1.14697 \t\ttime= 0.67020\n",
      "[*] Epoch: 1773 train_loss= 1.03429 train_rmse= 0.78869 val_loss= 1.48486 val_rmse= 1.14328 \t\ttime= 0.56250\n",
      "[*] Epoch: 1774 train_loss= 1.01708 train_rmse= 0.77549 val_loss= 1.48000 val_rmse= 1.13771 \t\ttime= 0.61037\n",
      "[*] Epoch: 1775 train_loss= 1.03221 train_rmse= 0.78578 val_loss= 1.47882 val_rmse= 1.13691 \t\ttime= 0.62234\n",
      "[*] Epoch: 1776 train_loss= 1.03914 train_rmse= 0.78366 val_loss= 1.47722 val_rmse= 1.13547 \t\ttime= 0.54953\n",
      "[*] Epoch: 1777 train_loss= 1.03204 train_rmse= 0.78361 val_loss= 1.47681 val_rmse= 1.13459 \t\ttime= 0.60039\n",
      "[*] Epoch: 1778 train_loss= 1.02151 train_rmse= 0.77149 val_loss= 1.47861 val_rmse= 1.13726 \t\ttime= 0.64129\n",
      "[*] Epoch: 1779 train_loss= 1.02168 train_rmse= 0.77331 val_loss= 1.48249 val_rmse= 1.14204 \t\ttime= 0.67918\n",
      "[*] Epoch: 1780 train_loss= 1.03246 train_rmse= 0.78033 val_loss= 1.48693 val_rmse= 1.14672 \t\ttime= 0.61436\n",
      "[*] Epoch: 1781 train_loss= 1.02343 train_rmse= 0.77852 val_loss= 1.48928 val_rmse= 1.14924 \t\ttime= 0.56150\n",
      "[*] Epoch: 1782 train_loss= 1.02273 train_rmse= 0.77086 val_loss= 1.49437 val_rmse= 1.15481 \t\ttime= 0.61236\n",
      "[*] Epoch: 1783 train_loss= 1.04564 train_rmse= 0.79178 val_loss= 1.49573 val_rmse= 1.15616 \t\ttime= 0.52061\n",
      "[*] Epoch: 1784 train_loss= 1.02634 train_rmse= 0.78123 val_loss= 1.49429 val_rmse= 1.15374 \t\ttime= 0.61435\n",
      "[*] Epoch: 1785 train_loss= 1.03007 train_rmse= 0.78380 val_loss= 1.49044 val_rmse= 1.14903 \t\ttime= 0.67918\n",
      "[*] Epoch: 1786 train_loss= 1.03513 train_rmse= 0.78418 val_loss= 1.48713 val_rmse= 1.14547 \t\ttime= 0.59641\n",
      "[*] Epoch: 1787 train_loss= 1.02166 train_rmse= 0.77509 val_loss= 1.48534 val_rmse= 1.14405 \t\ttime= 0.63630\n",
      "[*] Epoch: 1788 train_loss= 1.04911 train_rmse= 0.79531 val_loss= 1.48576 val_rmse= 1.14402 \t\ttime= 0.62633\n",
      "[*] Epoch: 1789 train_loss= 1.02531 train_rmse= 0.78165 val_loss= 1.48654 val_rmse= 1.14482 \t\ttime= 0.55252\n",
      "[*] Epoch: 1790 train_loss= 1.02215 train_rmse= 0.77741 val_loss= 1.48699 val_rmse= 1.14497 \t\ttime= 0.60040\n",
      "[*] Epoch: 1791 train_loss= 1.02848 train_rmse= 0.78067 val_loss= 1.48431 val_rmse= 1.14230 \t\ttime= 0.62932\n",
      "[*] Epoch: 1792 train_loss= 1.01810 train_rmse= 0.77051 val_loss= 1.48375 val_rmse= 1.14272 \t\ttime= 0.63231\n",
      "[*] Epoch: 1793 train_loss= 1.02661 train_rmse= 0.77786 val_loss= 1.48453 val_rmse= 1.14426 \t\ttime= 0.58344\n",
      "[*] Epoch: 1794 train_loss= 1.02358 train_rmse= 0.77454 val_loss= 1.48486 val_rmse= 1.14489 \t\ttime= 0.61635\n",
      "[*] Epoch: 1795 train_loss= 1.03796 train_rmse= 0.78434 val_loss= 1.48492 val_rmse= 1.14454 \t\ttime= 0.60738\n",
      "[*] Epoch: 1796 train_loss= 1.03755 train_rmse= 0.78436 val_loss= 1.48471 val_rmse= 1.14375 \t\ttime= 0.55153\n",
      "[*] Epoch: 1797 train_loss= 1.02054 train_rmse= 0.77467 val_loss= 1.48409 val_rmse= 1.14248 \t\ttime= 0.58543\n",
      "[*] Epoch: 1798 train_loss= 1.02331 train_rmse= 0.77817 val_loss= 1.48263 val_rmse= 1.14128 \t\ttime= 0.65226\n",
      "[*] Epoch: 1799 train_loss= 1.03085 train_rmse= 0.78270 val_loss= 1.48093 val_rmse= 1.13958 \t\ttime= 0.57346\n",
      "[*] Epoch: 1800 train_loss= 1.01166 train_rmse= 0.76791 val_loss= 1.48212 val_rmse= 1.14135 \t\ttime= 0.62333\n",
      "[*] Epoch: 1801 train_loss= 1.03060 train_rmse= 0.78062 val_loss= 1.48509 val_rmse= 1.14475 \t\ttime= 0.61835\n",
      "[*] Epoch: 1802 train_loss= 1.02164 train_rmse= 0.77704 val_loss= 1.48740 val_rmse= 1.14709 \t\ttime= 0.57347\n",
      "[*] Epoch: 1803 train_loss= 1.01364 train_rmse= 0.76770 val_loss= 1.48814 val_rmse= 1.14727 \t\ttime= 0.60538\n",
      "[*] Epoch: 1804 train_loss= 1.03578 train_rmse= 0.78881 val_loss= 1.48696 val_rmse= 1.14588 \t\ttime= 0.58743\n",
      "[*] Epoch: 1805 train_loss= 1.01519 train_rmse= 0.77002 val_loss= 1.48609 val_rmse= 1.14492 \t\ttime= 0.65625\n",
      "[*] Epoch: 1806 train_loss= 1.02736 train_rmse= 0.77926 val_loss= 1.48469 val_rmse= 1.14383 \t\ttime= 0.55751\n",
      "[*] Epoch: 1807 train_loss= 1.01224 train_rmse= 0.77251 val_loss= 1.48275 val_rmse= 1.14222 \t\ttime= 0.58942\n",
      "[*] Epoch: 1808 train_loss= 1.04013 train_rmse= 0.79070 val_loss= 1.48009 val_rmse= 1.13921 \t\ttime= 0.64329\n",
      "[*] Epoch: 1809 train_loss= 1.01884 train_rmse= 0.77705 val_loss= 1.48013 val_rmse= 1.13858 \t\ttime= 0.52659\n",
      "[*] Epoch: 1810 train_loss= 1.03606 train_rmse= 0.78609 val_loss= 1.47862 val_rmse= 1.13625 \t\ttime= 0.61436\n",
      "[*] Epoch: 1811 train_loss= 1.01020 train_rmse= 0.76860 val_loss= 1.47851 val_rmse= 1.13608 \t\ttime= 0.65424\n",
      "[*] Epoch: 1812 train_loss= 1.04501 train_rmse= 0.79204 val_loss= 1.47988 val_rmse= 1.13792 \t\ttime= 0.61635\n",
      "[*] Epoch: 1813 train_loss= 1.02583 train_rmse= 0.77556 val_loss= 1.48151 val_rmse= 1.13971 \t\ttime= 0.60538\n",
      "[*] Epoch: 1814 train_loss= 1.03363 train_rmse= 0.78302 val_loss= 1.48292 val_rmse= 1.14054 \t\ttime= 0.59938\n",
      "[*] Epoch: 1815 train_loss= 1.01601 train_rmse= 0.76973 val_loss= 1.48528 val_rmse= 1.14350 \t\ttime= 0.60039\n",
      "[*] Epoch: 1816 train_loss= 1.00418 train_rmse= 0.76397 val_loss= 1.48725 val_rmse= 1.14667 \t\ttime= 0.52161\n",
      "[*] Epoch: 1817 train_loss= 1.01468 train_rmse= 0.76845 val_loss= 1.48976 val_rmse= 1.15064 \t\ttime= 0.61635\n",
      "[*] Epoch: 1818 train_loss= 1.03719 train_rmse= 0.79938 val_loss= 1.49167 val_rmse= 1.15323 \t\ttime= 0.65525\n",
      "[*] Epoch: 1819 train_loss= 1.02405 train_rmse= 0.77947 val_loss= 1.49401 val_rmse= 1.15554 \t\ttime= 0.58244\n",
      "[*] Epoch: 1820 train_loss= 1.02344 train_rmse= 0.78033 val_loss= 1.49278 val_rmse= 1.15372 \t\ttime= 0.60838\n",
      "[*] Epoch: 1821 train_loss= 1.03675 train_rmse= 0.79050 val_loss= 1.48983 val_rmse= 1.14973 \t\ttime= 0.62633\n",
      "[*] Epoch: 1822 train_loss= 1.01188 train_rmse= 0.76889 val_loss= 1.48732 val_rmse= 1.14675 \t\ttime= 0.57147\n",
      "[*] Epoch: 1823 train_loss= 1.02873 train_rmse= 0.78469 val_loss= 1.48588 val_rmse= 1.14531 \t\ttime= 0.62932\n",
      "[*] Epoch: 1824 train_loss= 1.03982 train_rmse= 0.78946 val_loss= 1.48437 val_rmse= 1.14400 \t\ttime= 0.63231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1825 train_loss= 1.03182 train_rmse= 0.78167 val_loss= 1.48407 val_rmse= 1.14351 \t\ttime= 0.68217\n",
      "[*] Epoch: 1826 train_loss= 1.02900 train_rmse= 0.77913 val_loss= 1.48311 val_rmse= 1.14118 \t\ttime= 0.60139\n",
      "[*] Epoch: 1827 train_loss= 1.03250 train_rmse= 0.78815 val_loss= 1.48139 val_rmse= 1.13809 \t\ttime= 0.63929\n",
      "[*] Epoch: 1828 train_loss= 1.03039 train_rmse= 0.77993 val_loss= 1.48397 val_rmse= 1.14011 \t\ttime= 0.58443\n",
      "[*] Epoch: 1829 train_loss= 1.03971 train_rmse= 0.79050 val_loss= 1.48723 val_rmse= 1.14416 \t\ttime= 0.57945\n",
      "[*] Epoch: 1830 train_loss= 1.05525 train_rmse= 0.79759 val_loss= 1.48905 val_rmse= 1.14753 \t\ttime= 0.58344\n",
      "[*] Epoch: 1831 train_loss= 1.02748 train_rmse= 0.77939 val_loss= 1.48933 val_rmse= 1.14900 \t\ttime= 0.66821\n",
      "[*] Epoch: 1832 train_loss= 1.03403 train_rmse= 0.78699 val_loss= 1.48965 val_rmse= 1.14975 \t\ttime= 0.56748\n",
      "[*] Epoch: 1833 train_loss= 1.02767 train_rmse= 0.77762 val_loss= 1.49271 val_rmse= 1.15191 \t\ttime= 0.61436\n",
      "[*] Epoch: 1834 train_loss= 1.00925 train_rmse= 0.77053 val_loss= 1.49432 val_rmse= 1.15295 \t\ttime= 0.60438\n",
      "[*] Epoch: 1835 train_loss= 1.01040 train_rmse= 0.77017 val_loss= 1.49468 val_rmse= 1.15299 \t\ttime= 0.55152\n",
      "[*] Epoch: 1836 train_loss= 1.02544 train_rmse= 0.78088 val_loss= 1.49341 val_rmse= 1.15260 \t\ttime= 0.57746\n",
      "[*] Epoch: 1837 train_loss= 1.02527 train_rmse= 0.78075 val_loss= 1.49031 val_rmse= 1.14959 \t\ttime= 0.60837\n",
      "[*] Epoch: 1838 train_loss= 1.01458 train_rmse= 0.76915 val_loss= 1.48957 val_rmse= 1.14902 \t\ttime= 0.66522\n",
      "[*] Epoch: 1839 train_loss= 1.00563 train_rmse= 0.76527 val_loss= 1.49023 val_rmse= 1.15024 \t\ttime= 0.58145\n",
      "[*] Epoch: 1840 train_loss= 1.01105 train_rmse= 0.76755 val_loss= 1.49222 val_rmse= 1.15195 \t\ttime= 0.57945\n",
      "[*] Epoch: 1841 train_loss= 1.03576 train_rmse= 0.78931 val_loss= 1.49177 val_rmse= 1.15006 \t\ttime= 0.61137\n",
      "[*] Epoch: 1842 train_loss= 1.01880 train_rmse= 0.77353 val_loss= 1.48931 val_rmse= 1.14579 \t\ttime= 0.56549\n",
      "[*] Epoch: 1843 train_loss= 1.01754 train_rmse= 0.77513 val_loss= 1.48767 val_rmse= 1.14407 \t\ttime= 0.60639\n",
      "[*] Epoch: 1844 train_loss= 1.01484 train_rmse= 0.76910 val_loss= 1.48940 val_rmse= 1.14663 \t\ttime= 0.66323\n",
      "[*] Epoch: 1845 train_loss= 1.04698 train_rmse= 0.79239 val_loss= 1.48941 val_rmse= 1.14802 \t\ttime= 0.59541\n",
      "[*] Epoch: 1846 train_loss= 1.02821 train_rmse= 0.77980 val_loss= 1.49032 val_rmse= 1.14995 \t\ttime= 0.63729\n",
      "[*] Epoch: 1847 train_loss= 1.02195 train_rmse= 0.77963 val_loss= 1.49021 val_rmse= 1.14952 \t\ttime= 0.60638\n",
      "[*] Epoch: 1848 train_loss= 1.04533 train_rmse= 0.79235 val_loss= 1.48776 val_rmse= 1.14671 \t\ttime= 0.57347\n",
      "[*] Epoch: 1849 train_loss= 1.02135 train_rmse= 0.78099 val_loss= 1.48548 val_rmse= 1.14395 \t\ttime= 0.61137\n",
      "[*] Epoch: 1850 train_loss= 1.01327 train_rmse= 0.77164 val_loss= 1.48463 val_rmse= 1.14308 \t\ttime= 0.59441\n",
      "[*] Epoch: 1851 train_loss= 1.02688 train_rmse= 0.77486 val_loss= 1.48291 val_rmse= 1.14207 \t\ttime= 0.67119\n",
      "[*] Epoch: 1852 train_loss= 1.01950 train_rmse= 0.77307 val_loss= 1.48357 val_rmse= 1.14274 \t\ttime= 0.60239\n",
      "[*] Epoch: 1853 train_loss= 1.02433 train_rmse= 0.77535 val_loss= 1.48645 val_rmse= 1.14539 \t\ttime= 0.57446\n",
      "[*] Epoch: 1854 train_loss= 1.02310 train_rmse= 0.78211 val_loss= 1.48843 val_rmse= 1.14717 \t\ttime= 0.61536\n",
      "[*] Epoch: 1855 train_loss= 1.02381 train_rmse= 0.77752 val_loss= 1.48872 val_rmse= 1.14721 \t\ttime= 0.57945\n",
      "[*] Epoch: 1856 train_loss= 1.03130 train_rmse= 0.78196 val_loss= 1.48658 val_rmse= 1.14483 \t\ttime= 0.64328\n",
      "[*] Epoch: 1857 train_loss= 1.01827 train_rmse= 0.77586 val_loss= 1.48556 val_rmse= 1.14402 \t\ttime= 0.67618\n",
      "[*] Epoch: 1858 train_loss= 1.02655 train_rmse= 0.78155 val_loss= 1.48534 val_rmse= 1.14401 \t\ttime= 0.59042\n",
      "[*] Epoch: 1859 train_loss= 1.02479 train_rmse= 0.77805 val_loss= 1.48320 val_rmse= 1.14121 \t\ttime= 0.59142\n",
      "[*] Epoch: 1860 train_loss= 1.03010 train_rmse= 0.78109 val_loss= 1.48416 val_rmse= 1.14209 \t\ttime= 0.63630\n",
      "[*] Epoch: 1861 train_loss= 1.02199 train_rmse= 0.77227 val_loss= 1.48558 val_rmse= 1.14371 \t\ttime= 0.56249\n",
      "[*] Epoch: 1862 train_loss= 1.01324 train_rmse= 0.77277 val_loss= 1.48597 val_rmse= 1.14494 \t\ttime= 0.58743\n",
      "[*] Epoch: 1863 train_loss= 1.00586 train_rmse= 0.76127 val_loss= 1.48727 val_rmse= 1.14783 \t\ttime= 0.65026\n",
      "[*] Epoch: 1864 train_loss= 1.01962 train_rmse= 0.77814 val_loss= 1.48950 val_rmse= 1.15110 \t\ttime= 0.67220\n",
      "[*] Epoch: 1865 train_loss= 1.03753 train_rmse= 0.79068 val_loss= 1.49064 val_rmse= 1.15168 \t\ttime= 0.55551\n",
      "[*] Epoch: 1866 train_loss= 1.01656 train_rmse= 0.77441 val_loss= 1.49024 val_rmse= 1.15101 \t\ttime= 0.58843\n",
      "[*] Epoch: 1867 train_loss= 1.01809 train_rmse= 0.77598 val_loss= 1.48794 val_rmse= 1.14789 \t\ttime= 0.62732\n",
      "[*] Epoch: 1868 train_loss= 1.02318 train_rmse= 0.77888 val_loss= 1.48523 val_rmse= 1.14466 \t\ttime= 0.53756\n",
      "[*] Epoch: 1869 train_loss= 1.02099 train_rmse= 0.77536 val_loss= 1.48508 val_rmse= 1.14405 \t\ttime= 0.63929\n",
      "[*] Epoch: 1870 train_loss= 1.03328 train_rmse= 0.78686 val_loss= 1.48570 val_rmse= 1.14471 \t\ttime= 0.67918\n",
      "[*] Epoch: 1871 train_loss= 1.03064 train_rmse= 0.78299 val_loss= 1.48622 val_rmse= 1.14504 \t\ttime= 0.61535\n",
      "[*] Epoch: 1872 train_loss= 1.02135 train_rmse= 0.77962 val_loss= 1.48599 val_rmse= 1.14392 \t\ttime= 0.62532\n",
      "[*] Epoch: 1873 train_loss= 1.03382 train_rmse= 0.79176 val_loss= 1.48272 val_rmse= 1.13932 \t\ttime= 0.62931\n",
      "[*] Epoch: 1874 train_loss= 1.03122 train_rmse= 0.77971 val_loss= 1.47844 val_rmse= 1.13480 \t\ttime= 0.58643\n",
      "[*] Epoch: 1875 train_loss= 1.03357 train_rmse= 0.78443 val_loss= 1.47670 val_rmse= 1.13421 \t\ttime= 0.65125\n",
      "[*] Epoch: 1876 train_loss= 1.02917 train_rmse= 0.78103 val_loss= 1.47743 val_rmse= 1.13688 \t\ttime= 0.64128\n",
      "[*] Epoch: 1877 train_loss= 1.03983 train_rmse= 0.78299 val_loss= 1.48133 val_rmse= 1.14103 \t\ttime= 0.62433\n",
      "[*] Epoch: 1878 train_loss= 1.03452 train_rmse= 0.78576 val_loss= 1.48259 val_rmse= 1.14109 \t\ttime= 0.54754\n",
      "[*] Epoch: 1879 train_loss= 1.02178 train_rmse= 0.77715 val_loss= 1.48557 val_rmse= 1.14290 \t\ttime= 0.60638\n",
      "[*] Epoch: 1880 train_loss= 1.03023 train_rmse= 0.77922 val_loss= 1.48849 val_rmse= 1.14629 \t\ttime= 0.57648\n",
      "[*] Epoch: 1881 train_loss= 1.02421 train_rmse= 0.77608 val_loss= 1.49023 val_rmse= 1.14957 \t\ttime= 0.54654\n",
      "[*] Epoch: 1882 train_loss= 1.02508 train_rmse= 0.78667 val_loss= 1.49016 val_rmse= 1.15024 \t\ttime= 0.60438\n",
      "[*] Epoch: 1883 train_loss= 1.02265 train_rmse= 0.77761 val_loss= 1.48884 val_rmse= 1.14834 \t\ttime= 0.65924\n",
      "[*] Epoch: 1884 train_loss= 1.02469 train_rmse= 0.78170 val_loss= 1.48515 val_rmse= 1.14379 \t\ttime= 0.57446\n",
      "[*] Epoch: 1885 train_loss= 1.01602 train_rmse= 0.76891 val_loss= 1.48588 val_rmse= 1.14426 \t\ttime= 0.61336\n",
      "[*] Epoch: 1886 train_loss= 1.02865 train_rmse= 0.78320 val_loss= 1.48449 val_rmse= 1.14231 \t\ttime= 0.60937\n",
      "[*] Epoch: 1887 train_loss= 1.02812 train_rmse= 0.77924 val_loss= 1.48284 val_rmse= 1.14042 \t\ttime= 0.61635\n",
      "[*] Epoch: 1888 train_loss= 1.01320 train_rmse= 0.76507 val_loss= 1.48358 val_rmse= 1.14096 \t\ttime= 0.61436\n",
      "[*] Epoch: 1889 train_loss= 1.02919 train_rmse= 0.78430 val_loss= 1.48445 val_rmse= 1.14230 \t\ttime= 0.64029\n",
      "[*] Epoch: 1890 train_loss= 1.01977 train_rmse= 0.77830 val_loss= 1.48518 val_rmse= 1.14352 \t\ttime= 0.70512\n",
      "[*] Epoch: 1891 train_loss= 1.00916 train_rmse= 0.76554 val_loss= 1.48675 val_rmse= 1.14612 \t\ttime= 0.64328\n",
      "[*] Epoch: 1892 train_loss= 1.03353 train_rmse= 0.78994 val_loss= 1.48704 val_rmse= 1.14693 \t\ttime= 0.66024\n",
      "[*] Epoch: 1893 train_loss= 1.03030 train_rmse= 0.78606 val_loss= 1.48759 val_rmse= 1.14731 \t\ttime= 0.63829\n",
      "[*] Epoch: 1894 train_loss= 1.01809 train_rmse= 0.77273 val_loss= 1.48688 val_rmse= 1.14669 \t\ttime= 0.59640\n",
      "[*] Epoch: 1895 train_loss= 1.02222 train_rmse= 0.77854 val_loss= 1.48421 val_rmse= 1.14380 \t\ttime= 0.63032\n",
      "[*] Epoch: 1896 train_loss= 1.01634 train_rmse= 0.76920 val_loss= 1.48053 val_rmse= 1.13955 \t\ttime= 0.68816\n",
      "[*] Epoch: 1897 train_loss= 1.01682 train_rmse= 0.76989 val_loss= 1.48070 val_rmse= 1.13950 \t\ttime= 0.59242\n",
      "[*] Epoch: 1898 train_loss= 1.01703 train_rmse= 0.77134 val_loss= 1.48186 val_rmse= 1.14092 \t\ttime= 0.61834\n",
      "[*] Epoch: 1899 train_loss= 1.01160 train_rmse= 0.77274 val_loss= 1.48449 val_rmse= 1.14428 \t\ttime= 0.68218\n",
      "[*] Epoch: 1900 train_loss= 1.02632 train_rmse= 0.77898 val_loss= 1.48580 val_rmse= 1.14517 \t\ttime= 0.56150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1901 train_loss= 1.03385 train_rmse= 0.78219 val_loss= 1.48590 val_rmse= 1.14448 \t\ttime= 0.61436\n",
      "[*] Epoch: 1902 train_loss= 1.01874 train_rmse= 0.77401 val_loss= 1.48606 val_rmse= 1.14338 \t\ttime= 0.66722\n",
      "[*] Epoch: 1903 train_loss= 1.02431 train_rmse= 0.77766 val_loss= 1.48509 val_rmse= 1.14163 \t\ttime= 0.56449\n",
      "[*] Epoch: 1904 train_loss= 1.01200 train_rmse= 0.77062 val_loss= 1.48552 val_rmse= 1.14321 \t\ttime= 0.60339\n",
      "[*] Epoch: 1905 train_loss= 1.01258 train_rmse= 0.77490 val_loss= 1.48591 val_rmse= 1.14609 \t\ttime= 0.60737\n",
      "[*] Epoch: 1906 train_loss= 1.03388 train_rmse= 0.78869 val_loss= 1.48619 val_rmse= 1.14778 \t\ttime= 0.58244\n",
      "[*] Epoch: 1907 train_loss= 1.02854 train_rmse= 0.78783 val_loss= 1.48333 val_rmse= 1.14422 \t\ttime= 0.62433\n",
      "[*] Epoch: 1908 train_loss= 1.00919 train_rmse= 0.77021 val_loss= 1.48026 val_rmse= 1.13931 \t\ttime= 0.61635\n",
      "[*] Epoch: 1909 train_loss= 1.00951 train_rmse= 0.76546 val_loss= 1.48150 val_rmse= 1.13855 \t\ttime= 0.63630\n",
      "[*] Epoch: 1910 train_loss= 1.02432 train_rmse= 0.77490 val_loss= 1.48542 val_rmse= 1.14235 \t\ttime= 0.58942\n",
      "[*] Epoch: 1911 train_loss= 1.01719 train_rmse= 0.77099 val_loss= 1.48951 val_rmse= 1.14831 \t\ttime= 0.58145\n",
      "[*] Epoch: 1912 train_loss= 1.00871 train_rmse= 0.76771 val_loss= 1.49337 val_rmse= 1.15394 \t\ttime= 0.61735\n",
      "[*] Epoch: 1913 train_loss= 1.03615 train_rmse= 0.79059 val_loss= 1.49299 val_rmse= 1.15392 \t\ttime= 0.53956\n",
      "[*] Epoch: 1914 train_loss= 1.02018 train_rmse= 0.77747 val_loss= 1.49052 val_rmse= 1.15025 \t\ttime= 0.60139\n",
      "[*] Epoch: 1915 train_loss= 1.03182 train_rmse= 0.78579 val_loss= 1.48808 val_rmse= 1.14534 \t\ttime= 0.65325\n",
      "[*] Epoch: 1916 train_loss= 1.02780 train_rmse= 0.77992 val_loss= 1.48352 val_rmse= 1.13946 \t\ttime= 0.60139\n",
      "[*] Epoch: 1917 train_loss= 1.02243 train_rmse= 0.77805 val_loss= 1.47860 val_rmse= 1.13470 \t\ttime= 0.61337\n",
      "[*] Epoch: 1918 train_loss= 1.01194 train_rmse= 0.76894 val_loss= 1.47607 val_rmse= 1.13394 \t\ttime= 0.62034\n",
      "[*] Epoch: 1919 train_loss= 1.02513 train_rmse= 0.77931 val_loss= 1.47551 val_rmse= 1.13525 \t\ttime= 0.57546\n",
      "[*] Epoch: 1920 train_loss= 1.00401 train_rmse= 0.76384 val_loss= 1.47693 val_rmse= 1.13703 \t\ttime= 0.58344\n",
      "[*] Epoch: 1921 train_loss= 1.02534 train_rmse= 0.77734 val_loss= 1.48044 val_rmse= 1.14017 \t\ttime= 0.59042\n",
      "[*] Epoch: 1922 train_loss= 1.01326 train_rmse= 0.77087 val_loss= 1.48534 val_rmse= 1.14449 \t\ttime= 0.67320\n",
      "[*] Epoch: 1923 train_loss= 1.02867 train_rmse= 0.78315 val_loss= 1.48896 val_rmse= 1.14729 \t\ttime= 0.54853\n",
      "[*] Epoch: 1924 train_loss= 1.02766 train_rmse= 0.77933 val_loss= 1.49118 val_rmse= 1.14982 \t\ttime= 0.62333\n",
      "[*] Epoch: 1925 train_loss= 1.01446 train_rmse= 0.77237 val_loss= 1.49143 val_rmse= 1.15170 \t\ttime= 0.61835\n",
      "[*] Epoch: 1926 train_loss= 1.01857 train_rmse= 0.77686 val_loss= 1.49128 val_rmse= 1.15347 \t\ttime= 0.53756\n",
      "[*] Epoch: 1927 train_loss= 1.03692 train_rmse= 0.78829 val_loss= 1.48838 val_rmse= 1.15116 \t\ttime= 0.59541\n",
      "[*] Epoch: 1928 train_loss= 1.02120 train_rmse= 0.77076 val_loss= 1.48724 val_rmse= 1.14820 \t\ttime= 0.65026\n",
      "[*] Epoch: 1929 train_loss= 1.00665 train_rmse= 0.77106 val_loss= 1.48594 val_rmse= 1.14406 \t\ttime= 0.62732\n",
      "[*] Epoch: 1930 train_loss= 1.00501 train_rmse= 0.76410 val_loss= 1.48605 val_rmse= 1.14222 \t\ttime= 0.60438\n",
      "[*] Epoch: 1931 train_loss= 1.01552 train_rmse= 0.77238 val_loss= 1.48601 val_rmse= 1.14238 \t\ttime= 0.58444\n",
      "[*] Epoch: 1932 train_loss= 1.01326 train_rmse= 0.76847 val_loss= 1.48534 val_rmse= 1.14320 \t\ttime= 0.60538\n",
      "[*] Epoch: 1933 train_loss= 1.01613 train_rmse= 0.77263 val_loss= 1.48714 val_rmse= 1.14624 \t\ttime= 0.54853\n",
      "[*] Epoch: 1934 train_loss= 1.01188 train_rmse= 0.77243 val_loss= 1.49023 val_rmse= 1.14996 \t\ttime= 0.59042\n",
      "[*] Epoch: 1935 train_loss= 1.01142 train_rmse= 0.76892 val_loss= 1.49400 val_rmse= 1.15391 \t\ttime= 0.67021\n",
      "[*] Epoch: 1936 train_loss= 1.01633 train_rmse= 0.77956 val_loss= 1.49639 val_rmse= 1.15588 \t\ttime= 0.55350\n",
      "[*] Epoch: 1937 train_loss= 1.02116 train_rmse= 0.77635 val_loss= 1.49627 val_rmse= 1.15532 \t\ttime= 0.60339\n",
      "[*] Epoch: 1938 train_loss= 1.02125 train_rmse= 0.77630 val_loss= 1.49345 val_rmse= 1.15212 \t\ttime= 0.59840\n",
      "[*] Epoch: 1939 train_loss= 1.00954 train_rmse= 0.76820 val_loss= 1.49043 val_rmse= 1.14992 \t\ttime= 0.58444\n",
      "[*] Epoch: 1940 train_loss= 1.01831 train_rmse= 0.77330 val_loss= 1.48758 val_rmse= 1.14839 \t\ttime= 0.59541\n",
      "[*] Epoch: 1941 train_loss= 1.01224 train_rmse= 0.77160 val_loss= 1.48593 val_rmse= 1.14742 \t\ttime= 0.62134\n",
      "[*] Epoch: 1942 train_loss= 1.02918 train_rmse= 0.78226 val_loss= 1.48298 val_rmse= 1.14398 \t\ttime= 0.66821\n",
      "[*] Epoch: 1943 train_loss= 1.03901 train_rmse= 0.78482 val_loss= 1.48026 val_rmse= 1.13873 \t\ttime= 0.55553\n",
      "[*] Epoch: 1944 train_loss= 1.01229 train_rmse= 0.76623 val_loss= 1.47999 val_rmse= 1.13722 \t\ttime= 0.58643\n",
      "[*] Epoch: 1945 train_loss= 1.01555 train_rmse= 0.76898 val_loss= 1.48287 val_rmse= 1.14016 \t\ttime= 0.62234\n",
      "[*] Epoch: 1946 train_loss= 1.01153 train_rmse= 0.77072 val_loss= 1.48733 val_rmse= 1.14696 \t\ttime= 0.55053\n",
      "[*] Epoch: 1947 train_loss= 1.01071 train_rmse= 0.77330 val_loss= 1.49058 val_rmse= 1.15252 \t\ttime= 0.61835\n",
      "[*] Epoch: 1948 train_loss= 1.01057 train_rmse= 0.77394 val_loss= 1.49246 val_rmse= 1.15451 \t\ttime= 0.64527\n",
      "[*] Epoch: 1949 train_loss= 1.00618 train_rmse= 0.76669 val_loss= 1.49440 val_rmse= 1.15548 \t\ttime= 0.59042\n",
      "[*] Epoch: 1950 train_loss= 1.01938 train_rmse= 0.77746 val_loss= 1.49450 val_rmse= 1.15437 \t\ttime= 0.56748\n",
      "[*] Epoch: 1951 train_loss= 1.00842 train_rmse= 0.77047 val_loss= 1.49333 val_rmse= 1.15238 \t\ttime= 0.59441\n",
      "[*] Epoch: 1952 train_loss= 1.00718 train_rmse= 0.76524 val_loss= 1.49197 val_rmse= 1.15068 \t\ttime= 0.58843\n",
      "[*] Epoch: 1953 train_loss= 1.00415 train_rmse= 0.76948 val_loss= 1.49232 val_rmse= 1.15236 \t\ttime= 0.56250\n",
      "[*] Epoch: 1954 train_loss= 1.03808 train_rmse= 0.79107 val_loss= 1.48953 val_rmse= 1.15004 \t\ttime= 0.60339\n",
      "[*] Epoch: 1955 train_loss= 1.01456 train_rmse= 0.77431 val_loss= 1.48603 val_rmse= 1.14603 \t\ttime= 0.64328\n",
      "[*] Epoch: 1956 train_loss= 1.02076 train_rmse= 0.77701 val_loss= 1.48296 val_rmse= 1.14228 \t\ttime= 0.56549\n",
      "[*] Epoch: 1957 train_loss= 1.01894 train_rmse= 0.77572 val_loss= 1.48002 val_rmse= 1.13810 \t\ttime= 0.62034\n",
      "[*] Epoch: 1958 train_loss= 1.00565 train_rmse= 0.76525 val_loss= 1.47890 val_rmse= 1.13517 \t\ttime= 0.62433\n",
      "[*] Epoch: 1959 train_loss= 1.02235 train_rmse= 0.77294 val_loss= 1.48068 val_rmse= 1.13620 \t\ttime= 0.54554\n",
      "[*] Epoch: 1960 train_loss= 1.02701 train_rmse= 0.78695 val_loss= 1.48233 val_rmse= 1.13866 \t\ttime= 0.61136\n",
      "[*] Epoch: 1961 train_loss= 1.01934 train_rmse= 0.77739 val_loss= 1.48627 val_rmse= 1.14470 \t\ttime= 0.63530\n",
      "[*] Epoch: 1962 train_loss= 1.02836 train_rmse= 0.78435 val_loss= 1.48945 val_rmse= 1.14918 \t\ttime= 0.64826\n",
      "[*] Epoch: 1963 train_loss= 1.03491 train_rmse= 0.78765 val_loss= 1.49018 val_rmse= 1.15097 \t\ttime= 0.63530\n",
      "[*] Epoch: 1964 train_loss= 1.02804 train_rmse= 0.78468 val_loss= 1.48816 val_rmse= 1.14915 \t\ttime= 0.58145\n",
      "[*] Epoch: 1965 train_loss= 1.01076 train_rmse= 0.76678 val_loss= 1.48903 val_rmse= 1.14950 \t\ttime= 0.63331\n",
      "[*] Epoch: 1966 train_loss= 1.03891 train_rmse= 0.78970 val_loss= 1.48707 val_rmse= 1.14606 \t\ttime= 0.53956\n",
      "[*] Epoch: 1967 train_loss= 1.02290 train_rmse= 0.77458 val_loss= 1.48593 val_rmse= 1.14394 \t\ttime= 0.58743\n",
      "[*] Epoch: 1968 train_loss= 1.00420 train_rmse= 0.76268 val_loss= 1.48624 val_rmse= 1.14517 \t\ttime= 0.64328\n",
      "[*] Epoch: 1969 train_loss= 1.02218 train_rmse= 0.77769 val_loss= 1.48776 val_rmse= 1.14814 \t\ttime= 0.56847\n",
      "[*] Epoch: 1970 train_loss= 1.01683 train_rmse= 0.77796 val_loss= 1.48777 val_rmse= 1.14854 \t\ttime= 0.64229\n",
      "[*] Epoch: 1971 train_loss= 1.05412 train_rmse= 0.80567 val_loss= 1.48425 val_rmse= 1.14491 \t\ttime= 0.62234\n",
      "[*] Epoch: 1972 train_loss= 1.00040 train_rmse= 0.76441 val_loss= 1.48159 val_rmse= 1.14137 \t\ttime= 0.56748\n",
      "[*] Epoch: 1973 train_loss= 1.01491 train_rmse= 0.77078 val_loss= 1.48078 val_rmse= 1.13927 \t\ttime= 0.59142\n",
      "[*] Epoch: 1974 train_loss= 1.02084 train_rmse= 0.77998 val_loss= 1.48051 val_rmse= 1.13889 \t\ttime= 0.61935\n",
      "[*] Epoch: 1975 train_loss= 1.03043 train_rmse= 0.78194 val_loss= 1.48055 val_rmse= 1.14063 \t\ttime= 0.65924\n",
      "[*] Epoch: 1976 train_loss= 1.01752 train_rmse= 0.77593 val_loss= 1.48135 val_rmse= 1.14249 \t\ttime= 0.58942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 1977 train_loss= 1.00980 train_rmse= 0.76740 val_loss= 1.48508 val_rmse= 1.14635 \t\ttime= 0.58144\n",
      "[*] Epoch: 1978 train_loss= 1.00389 train_rmse= 0.76627 val_loss= 1.48789 val_rmse= 1.14824 \t\ttime= 0.61934\n",
      "[*] Epoch: 1979 train_loss= 1.01281 train_rmse= 0.77091 val_loss= 1.48880 val_rmse= 1.14879 \t\ttime= 0.55651\n",
      "[*] Epoch: 1980 train_loss= 1.00151 train_rmse= 0.76283 val_loss= 1.48823 val_rmse= 1.14787 \t\ttime= 0.60239\n",
      "[*] Epoch: 1981 train_loss= 1.00655 train_rmse= 0.76969 val_loss= 1.48649 val_rmse= 1.14667 \t\ttime= 0.64670\n",
      "[*] Epoch: 1982 train_loss= 0.99925 train_rmse= 0.76004 val_loss= 1.48774 val_rmse= 1.14821 \t\ttime= 0.60039\n",
      "[*] Epoch: 1983 train_loss= 1.01441 train_rmse= 0.77296 val_loss= 1.49039 val_rmse= 1.15082 \t\ttime= 0.62232\n",
      "[*] Epoch: 1984 train_loss= 1.02561 train_rmse= 0.78020 val_loss= 1.49219 val_rmse= 1.15234 \t\ttime= 0.60139\n",
      "[*] Epoch: 1985 train_loss= 1.01156 train_rmse= 0.77128 val_loss= 1.49325 val_rmse= 1.15247 \t\ttime= 0.57047\n",
      "[*] Epoch: 1986 train_loss= 1.01331 train_rmse= 0.77126 val_loss= 1.49249 val_rmse= 1.15064 \t\ttime= 0.58543\n",
      "[*] Epoch: 1987 train_loss= 1.01552 train_rmse= 0.77352 val_loss= 1.49062 val_rmse= 1.14849 \t\ttime= 0.59441\n",
      "[*] Epoch: 1988 train_loss= 1.02279 train_rmse= 0.77888 val_loss= 1.48849 val_rmse= 1.14690 \t\ttime= 0.65724\n",
      "[*] Epoch: 1989 train_loss= 1.03070 train_rmse= 0.78338 val_loss= 1.48363 val_rmse= 1.14212 \t\ttime= 0.56349\n",
      "[*] Epoch: 1990 train_loss= 1.03522 train_rmse= 0.78520 val_loss= 1.47834 val_rmse= 1.13644 \t\ttime= 0.62134\n",
      "[*] Epoch: 1991 train_loss= 1.02420 train_rmse= 0.77646 val_loss= 1.47727 val_rmse= 1.13512 \t\ttime= 0.60538\n",
      "[*] Epoch: 1992 train_loss= 1.01444 train_rmse= 0.76731 val_loss= 1.48109 val_rmse= 1.13914 \t\ttime= 0.55153\n",
      "[*] Epoch: 1993 train_loss= 1.01187 train_rmse= 0.77172 val_loss= 1.48585 val_rmse= 1.14449 \t\ttime= 0.67420\n",
      "[*] Epoch: 1994 train_loss= 1.03621 train_rmse= 0.78831 val_loss= 1.48691 val_rmse= 1.14522 \t\ttime= 0.64428\n",
      "[*] Epoch: 1995 train_loss= 1.01156 train_rmse= 0.76920 val_loss= 1.48719 val_rmse= 1.14431 \t\ttime= 0.64727\n",
      "[*] Epoch: 1996 train_loss= 1.01321 train_rmse= 0.76945 val_loss= 1.48889 val_rmse= 1.14554 \t\ttime= 0.62333\n",
      "[*] Epoch: 1997 train_loss= 1.00549 train_rmse= 0.76901 val_loss= 1.49000 val_rmse= 1.14727 \t\ttime= 0.60638\n",
      "[*] Epoch: 1998 train_loss= 1.01387 train_rmse= 0.77665 val_loss= 1.48904 val_rmse= 1.14717 \t\ttime= 0.58045\n",
      "[*] Epoch: 1999 train_loss= 1.01274 train_rmse= 0.76743 val_loss= 1.49019 val_rmse= 1.14994 \t\ttime= 0.59042\n",
      "[*] Epoch: 2000 train_loss= 1.01597 train_rmse= 0.77762 val_loss= 1.49141 val_rmse= 1.15134 \t\ttime= 0.59740\n",
      "[*] Epoch: 2001 train_loss= 1.01531 train_rmse= 0.77748 val_loss= 1.49224 val_rmse= 1.15176 \t\ttime= 0.64626\n",
      "[*] Epoch: 2002 train_loss= 1.00581 train_rmse= 0.76510 val_loss= 1.49076 val_rmse= 1.14973 \t\ttime= 0.55153\n",
      "[*] Epoch: 2003 train_loss= 1.02952 train_rmse= 0.78108 val_loss= 1.48677 val_rmse= 1.14515 \t\ttime= 0.61236\n",
      "[*] Epoch: 2004 train_loss= 1.01367 train_rmse= 0.77303 val_loss= 1.48597 val_rmse= 1.14403 \t\ttime= 0.59539\n",
      "[*] Epoch: 2005 train_loss= 0.99321 train_rmse= 0.76122 val_loss= 1.48670 val_rmse= 1.14554 \t\ttime= 0.53457\n",
      "[*] Epoch: 2006 train_loss= 1.00255 train_rmse= 0.76612 val_loss= 1.48929 val_rmse= 1.14933 \t\ttime= 0.60538\n",
      "[*] Epoch: 2007 train_loss= 1.02996 train_rmse= 0.78273 val_loss= 1.49091 val_rmse= 1.15131 \t\ttime= 0.62034\n",
      "[*] Epoch: 2008 train_loss= 1.02160 train_rmse= 0.77967 val_loss= 1.49126 val_rmse= 1.15092 \t\ttime= 0.64228\n",
      "[*] Epoch: 2009 train_loss= 1.02171 train_rmse= 0.77330 val_loss= 1.49312 val_rmse= 1.15228 \t\ttime= 0.62633\n",
      "[*] Epoch: 2010 train_loss= 1.01693 train_rmse= 0.77594 val_loss= 1.49365 val_rmse= 1.15213 \t\ttime= 0.63530\n",
      "[*] Epoch: 2011 train_loss= 1.02967 train_rmse= 0.78308 val_loss= 1.48979 val_rmse= 1.14729 \t\ttime= 0.61037\n",
      "[*] Epoch: 2012 train_loss= 1.01648 train_rmse= 0.77540 val_loss= 1.48330 val_rmse= 1.14075 \t\ttime= 0.55552\n",
      "[*] Epoch: 2013 train_loss= 1.00585 train_rmse= 0.76367 val_loss= 1.47982 val_rmse= 1.13673 \t\ttime= 0.60339\n",
      "[*] Epoch: 2014 train_loss= 1.01682 train_rmse= 0.77294 val_loss= 1.48174 val_rmse= 1.13904 \t\ttime= 0.67620\n",
      "[*] Epoch: 2015 train_loss= 1.01624 train_rmse= 0.77360 val_loss= 1.48387 val_rmse= 1.14147 \t\ttime= 0.57945\n",
      "[*] Epoch: 2016 train_loss= 1.00861 train_rmse= 0.76660 val_loss= 1.48747 val_rmse= 1.14552 \t\ttime= 0.60937\n",
      "[*] Epoch: 2017 train_loss= 1.01578 train_rmse= 0.77467 val_loss= 1.48865 val_rmse= 1.14645 \t\ttime= 0.61935\n",
      "[*] Epoch: 2018 train_loss= 1.00678 train_rmse= 0.76658 val_loss= 1.48848 val_rmse= 1.14645 \t\ttime= 0.53457\n",
      "[*] Epoch: 2019 train_loss= 1.01411 train_rmse= 0.77445 val_loss= 1.48930 val_rmse= 1.14762 \t\ttime= 0.62034\n",
      "[*] Epoch: 2020 train_loss= 1.00852 train_rmse= 0.76948 val_loss= 1.49022 val_rmse= 1.14880 \t\ttime= 0.65824\n",
      "[*] Epoch: 2021 train_loss= 1.02996 train_rmse= 0.78104 val_loss= 1.48832 val_rmse= 1.14738 \t\ttime= 0.62234\n",
      "[*] Epoch: 2022 train_loss= 1.01769 train_rmse= 0.77744 val_loss= 1.48746 val_rmse= 1.14642 \t\ttime= 0.62234\n",
      "[*] Epoch: 2023 train_loss= 1.01201 train_rmse= 0.77199 val_loss= 1.48694 val_rmse= 1.14583 \t\ttime= 0.60239\n",
      "[*] Epoch: 2024 train_loss= 1.02682 train_rmse= 0.78196 val_loss= 1.48519 val_rmse= 1.14368 \t\ttime= 0.61634\n",
      "[*] Epoch: 2025 train_loss= 1.01266 train_rmse= 0.77250 val_loss= 1.48314 val_rmse= 1.14148 \t\ttime= 0.57946\n",
      "[*] Epoch: 2026 train_loss= 1.00859 train_rmse= 0.76849 val_loss= 1.48251 val_rmse= 1.14064 \t\ttime= 0.63032\n",
      "[*] Epoch: 2027 train_loss= 1.02535 train_rmse= 0.77579 val_loss= 1.48158 val_rmse= 1.13938 \t\ttime= 0.71509\n",
      "[*] Epoch: 2028 train_loss= 1.01225 train_rmse= 0.77017 val_loss= 1.48287 val_rmse= 1.14086 \t\ttime= 0.60538\n",
      "[*] Epoch: 2029 train_loss= 1.02056 train_rmse= 0.77818 val_loss= 1.48718 val_rmse= 1.14582 \t\ttime= 0.63331\n",
      "[*] Epoch: 2030 train_loss= 1.01373 train_rmse= 0.77365 val_loss= 1.49156 val_rmse= 1.15002 \t\ttime= 0.61835\n",
      "[*] Epoch: 2031 train_loss= 1.00791 train_rmse= 0.77151 val_loss= 1.49311 val_rmse= 1.15114 \t\ttime= 0.57247\n",
      "[*] Epoch: 2032 train_loss= 1.01241 train_rmse= 0.78119 val_loss= 1.49234 val_rmse= 1.15008 \t\ttime= 0.67021\n",
      "[*] Epoch: 2033 train_loss= 1.01740 train_rmse= 0.77477 val_loss= 1.49146 val_rmse= 1.14926 \t\ttime= 0.67320\n",
      "[*] Epoch: 2034 train_loss= 1.01444 train_rmse= 0.77336 val_loss= 1.49103 val_rmse= 1.14894 \t\ttime= 0.58543\n",
      "[*] Epoch: 2035 train_loss= 1.01195 train_rmse= 0.77034 val_loss= 1.48905 val_rmse= 1.14694 \t\ttime= 0.65425\n",
      "[*] Epoch: 2036 train_loss= 1.01702 train_rmse= 0.77384 val_loss= 1.48454 val_rmse= 1.14170 \t\ttime= 0.66323\n",
      "[*] Epoch: 2037 train_loss= 1.01258 train_rmse= 0.77006 val_loss= 1.48082 val_rmse= 1.13698 \t\ttime= 0.55452\n",
      "[*] Epoch: 2038 train_loss= 1.01960 train_rmse= 0.77596 val_loss= 1.47759 val_rmse= 1.13396 \t\ttime= 0.59740\n",
      "[*] Epoch: 2039 train_loss= 1.01783 train_rmse= 0.78316 val_loss= 1.47774 val_rmse= 1.13422 \t\ttime= 0.61934\n",
      "[*] Epoch: 2040 train_loss= 1.01521 train_rmse= 0.77919 val_loss= 1.47858 val_rmse= 1.13537 \t\ttime= 0.65824\n",
      "[*] Epoch: 2041 train_loss= 0.99563 train_rmse= 0.75972 val_loss= 1.48201 val_rmse= 1.13962 \t\ttime= 0.59441\n",
      "[*] Epoch: 2042 train_loss= 1.00266 train_rmse= 0.76095 val_loss= 1.48828 val_rmse= 1.14757 \t\ttime= 0.59641\n",
      "[*] Epoch: 2043 train_loss= 1.03430 train_rmse= 0.78744 val_loss= 1.48926 val_rmse= 1.14914 \t\ttime= 0.61137\n",
      "[*] Epoch: 2044 train_loss= 1.02073 train_rmse= 0.78189 val_loss= 1.48920 val_rmse= 1.14845 \t\ttime= 0.54255\n",
      "[*] Epoch: 2045 train_loss= 1.02118 train_rmse= 0.77613 val_loss= 1.48853 val_rmse= 1.14652 \t\ttime= 0.61037\n",
      "[*] Epoch: 2046 train_loss= 1.02052 train_rmse= 0.78068 val_loss= 1.48819 val_rmse= 1.14471 \t\ttime= 0.63130\n",
      "[*] Epoch: 2047 train_loss= 1.01882 train_rmse= 0.77820 val_loss= 1.48732 val_rmse= 1.14367 \t\ttime= 0.57347\n",
      "[*] Epoch: 2048 train_loss= 1.00507 train_rmse= 0.77355 val_loss= 1.48728 val_rmse= 1.14543 \t\ttime= 0.58643\n",
      "[*] Epoch: 2049 train_loss= 1.01120 train_rmse= 0.77304 val_loss= 1.48583 val_rmse= 1.14509 \t\ttime= 0.60139\n",
      "[*] Epoch: 2050 train_loss= 0.99775 train_rmse= 0.75948 val_loss= 1.48771 val_rmse= 1.14687 \t\ttime= 0.58344\n",
      "[*] Epoch: 2051 train_loss= 1.00483 train_rmse= 0.76512 val_loss= 1.49137 val_rmse= 1.14957 \t\ttime= 0.57945\n",
      "[*] Epoch: 2052 train_loss= 1.01235 train_rmse= 0.77623 val_loss= 1.49222 val_rmse= 1.14912 \t\ttime= 0.59841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 2053 train_loss= 1.01227 train_rmse= 0.77016 val_loss= 1.49246 val_rmse= 1.14976 \t\ttime= 0.66622\n",
      "[*] Epoch: 2054 train_loss= 1.02298 train_rmse= 0.77552 val_loss= 1.49022 val_rmse= 1.14891 \t\ttime= 0.54056\n",
      "[*] Epoch: 2055 train_loss= 1.01299 train_rmse= 0.77215 val_loss= 1.48813 val_rmse= 1.14776 \t\ttime= 0.57047\n",
      "[*] Epoch: 2056 train_loss= 1.00842 train_rmse= 0.77434 val_loss= 1.48682 val_rmse= 1.14606 \t\ttime= 0.59940\n",
      "[*] Epoch: 2057 train_loss= 0.99634 train_rmse= 0.75654 val_loss= 1.48857 val_rmse= 1.14641 \t\ttime= 0.54554\n",
      "[*] Epoch: 2058 train_loss= 1.01241 train_rmse= 0.77231 val_loss= 1.48799 val_rmse= 1.14423 \t\ttime= 0.59840\n",
      "[*] Epoch: 2059 train_loss= 1.01775 train_rmse= 0.77366 val_loss= 1.48743 val_rmse= 1.14335 \t\ttime= 0.64129\n",
      "[*] Epoch: 2060 train_loss= 1.01738 train_rmse= 0.77070 val_loss= 1.48716 val_rmse= 1.14504 \t\ttime= 0.64427\n",
      "[*] Epoch: 2061 train_loss= 1.01381 train_rmse= 0.77399 val_loss= 1.48953 val_rmse= 1.14942 \t\ttime= 0.59142\n",
      "[*] Epoch: 2062 train_loss= 1.01479 train_rmse= 0.77451 val_loss= 1.49213 val_rmse= 1.15275 \t\ttime= 0.61336\n",
      "[*] Epoch: 2063 train_loss= 1.03252 train_rmse= 0.78704 val_loss= 1.49238 val_rmse= 1.15206 \t\ttime= 0.61635\n",
      "[*] Epoch: 2064 train_loss= 1.02730 train_rmse= 0.78265 val_loss= 1.48978 val_rmse= 1.14802 \t\ttime= 0.55252\n",
      "[*] Epoch: 2065 train_loss= 0.99927 train_rmse= 0.76617 val_loss= 1.48585 val_rmse= 1.14325 \t\ttime= 0.61236\n",
      "[*] Epoch: 2066 train_loss= 1.03484 train_rmse= 0.78444 val_loss= 1.48056 val_rmse= 1.13781 \t\ttime= 0.66522\n",
      "[*] Epoch: 2067 train_loss= 1.01930 train_rmse= 0.77445 val_loss= 1.47972 val_rmse= 1.13808 \t\ttime= 0.60638\n",
      "[*] Epoch: 2068 train_loss= 1.00491 train_rmse= 0.75878 val_loss= 1.48356 val_rmse= 1.14266 \t\ttime= 0.59042\n",
      "[*] Epoch: 2069 train_loss= 1.00919 train_rmse= 0.77251 val_loss= 1.48606 val_rmse= 1.14476 \t\ttime= 0.61436\n",
      "[*] Epoch: 2070 train_loss= 1.00866 train_rmse= 0.76781 val_loss= 1.48790 val_rmse= 1.14627 \t\ttime= 0.54255\n",
      "[*] Epoch: 2071 train_loss= 1.00870 train_rmse= 0.76524 val_loss= 1.49096 val_rmse= 1.14848 \t\ttime= 0.57845\n",
      "[*] Epoch: 2072 train_loss= 1.02186 train_rmse= 0.77659 val_loss= 1.49319 val_rmse= 1.15020 \t\ttime= 0.57347\n",
      "[*] Epoch: 2073 train_loss= 1.01207 train_rmse= 0.77310 val_loss= 1.49544 val_rmse= 1.15295 \t\ttime= 0.68018\n",
      "[*] Epoch: 2074 train_loss= 1.01426 train_rmse= 0.77483 val_loss= 1.49531 val_rmse= 1.15384 \t\ttime= 0.55452\n",
      "[*] Epoch: 2075 train_loss= 1.01664 train_rmse= 0.77727 val_loss= 1.49371 val_rmse= 1.15311 \t\ttime= 0.61535\n",
      "[*] Epoch: 2076 train_loss= 1.03073 train_rmse= 0.78672 val_loss= 1.48952 val_rmse= 1.14855 \t\ttime= 0.60039\n",
      "[*] Epoch: 2077 train_loss= 1.02456 train_rmse= 0.78208 val_loss= 1.48566 val_rmse= 1.14288 \t\ttime= 0.52161\n",
      "[*] Epoch: 2078 train_loss= 1.01889 train_rmse= 0.77975 val_loss= 1.48209 val_rmse= 1.13707 \t\ttime= 0.61136\n",
      "[*] Epoch: 2079 train_loss= 1.00479 train_rmse= 0.77345 val_loss= 1.48022 val_rmse= 1.13484 \t\ttime= 0.63131\n",
      "[*] Epoch: 2080 train_loss= 1.01802 train_rmse= 0.77635 val_loss= 1.47849 val_rmse= 1.13390 \t\ttime= 0.60738\n",
      "[*] Epoch: 2081 train_loss= 1.00923 train_rmse= 0.77113 val_loss= 1.47725 val_rmse= 1.13506 \t\ttime= 0.60438\n",
      "[*] Epoch: 2082 train_loss= 1.00367 train_rmse= 0.76111 val_loss= 1.48084 val_rmse= 1.14027 \t\ttime= 0.59641\n",
      "[*] Epoch: 2083 train_loss= 1.03009 train_rmse= 0.78003 val_loss= 1.48534 val_rmse= 1.14532 \t\ttime= 0.60837\n",
      "[*] Epoch: 2084 train_loss= 1.02471 train_rmse= 0.78001 val_loss= 1.48831 val_rmse= 1.14792 \t\ttime= 0.52760\n",
      "[*] Epoch: 2085 train_loss= 1.02082 train_rmse= 0.78026 val_loss= 1.48885 val_rmse= 1.14708 \t\ttime= 0.63131\n",
      "[*] Epoch: 2086 train_loss= 1.00163 train_rmse= 0.76470 val_loss= 1.48949 val_rmse= 1.14692 \t\ttime= 0.69913\n",
      "[*] Epoch: 2087 train_loss= 1.01000 train_rmse= 0.77044 val_loss= 1.48994 val_rmse= 1.14759 \t\ttime= 0.56549\n",
      "[*] Epoch: 2088 train_loss= 1.00297 train_rmse= 0.76641 val_loss= 1.48776 val_rmse= 1.14616 \t\ttime= 0.59341\n",
      "[*] Epoch: 2089 train_loss= 1.00749 train_rmse= 0.76985 val_loss= 1.48522 val_rmse= 1.14337 \t\ttime= 0.62732\n",
      "[*] Epoch: 2090 train_loss= 1.00768 train_rmse= 0.77075 val_loss= 1.48485 val_rmse= 1.14310 \t\ttime= 0.55950\n",
      "[*] Epoch: 2091 train_loss= 0.99021 train_rmse= 0.75471 val_loss= 1.48683 val_rmse= 1.14441 \t\ttime= 0.59541\n",
      "[*] Epoch: 2092 train_loss= 1.01097 train_rmse= 0.77019 val_loss= 1.48628 val_rmse= 1.14312 \t\ttime= 0.61735\n",
      "[*] Epoch: 2093 train_loss= 1.00650 train_rmse= 0.76695 val_loss= 1.48554 val_rmse= 1.14201 \t\ttime= 0.65226\n",
      "[*] Epoch: 2094 train_loss= 1.00892 train_rmse= 0.76946 val_loss= 1.48415 val_rmse= 1.14124 \t\ttime= 0.59342\n",
      "[*] Epoch: 2095 train_loss= 1.02016 train_rmse= 0.77655 val_loss= 1.48299 val_rmse= 1.14082 \t\ttime= 0.57845\n",
      "[*] Epoch: 2096 train_loss= 1.00790 train_rmse= 0.76769 val_loss= 1.48380 val_rmse= 1.14258 \t\ttime= 0.60338\n",
      "[*] Epoch: 2097 train_loss= 1.00740 train_rmse= 0.76478 val_loss= 1.48918 val_rmse= 1.14780 \t\ttime= 0.54754\n",
      "[*] Epoch: 2098 train_loss= 1.00916 train_rmse= 0.77108 val_loss= 1.49167 val_rmse= 1.14992 \t\ttime= 0.60838\n",
      "[*] Epoch: 2099 train_loss= 1.00847 train_rmse= 0.76718 val_loss= 1.49116 val_rmse= 1.14911 \t\ttime= 0.67619\n",
      "[*] Epoch: 2100 train_loss= 0.99285 train_rmse= 0.76183 val_loss= 1.48908 val_rmse= 1.14685 \t\ttime= 0.56151\n",
      "[*] Epoch: 2101 train_loss= 1.00681 train_rmse= 0.76800 val_loss= 1.48729 val_rmse= 1.14529 \t\ttime= 0.59441\n",
      "[*] Epoch: 2102 train_loss= 1.00949 train_rmse= 0.77184 val_loss= 1.48637 val_rmse= 1.14463 \t\ttime= 0.60937\n",
      "[*] Epoch: 2103 train_loss= 1.00744 train_rmse= 0.77033 val_loss= 1.48648 val_rmse= 1.14426 \t\ttime= 0.57147\n",
      "[*] Epoch: 2104 train_loss= 0.99730 train_rmse= 0.75890 val_loss= 1.48754 val_rmse= 1.14437 \t\ttime= 0.59441\n",
      "[*] Epoch: 2105 train_loss= 1.04014 train_rmse= 0.78913 val_loss= 1.48488 val_rmse= 1.14116 \t\ttime= 0.59142\n",
      "[*] Epoch: 2106 train_loss= 1.01691 train_rmse= 0.77057 val_loss= 1.48297 val_rmse= 1.13895 \t\ttime= 0.65823\n",
      "[*] Epoch: 2107 train_loss= 1.00425 train_rmse= 0.76345 val_loss= 1.48171 val_rmse= 1.13781 \t\ttime= 0.56848\n",
      "[*] Epoch: 2108 train_loss= 1.01697 train_rmse= 0.77501 val_loss= 1.47988 val_rmse= 1.13681 \t\ttime= 0.59341\n",
      "[*] Epoch: 2109 train_loss= 1.02207 train_rmse= 0.77774 val_loss= 1.47809 val_rmse= 1.13533 \t\ttime= 0.60040\n",
      "[*] Epoch: 2110 train_loss= 1.00171 train_rmse= 0.76269 val_loss= 1.47971 val_rmse= 1.13733 \t\ttime= 0.55053\n",
      "[*] Epoch: 2111 train_loss= 1.00968 train_rmse= 0.77489 val_loss= 1.48010 val_rmse= 1.13680 \t\ttime= 0.61236\n",
      "[*] Epoch: 2112 train_loss= 1.01207 train_rmse= 0.77579 val_loss= 1.48094 val_rmse= 1.13691 \t\ttime= 0.63131\n",
      "[*] Epoch: 2113 train_loss= 1.01542 train_rmse= 0.77243 val_loss= 1.47921 val_rmse= 1.13477 \t\ttime= 0.62433\n",
      "[*] Epoch: 2114 train_loss= 1.00770 train_rmse= 0.76692 val_loss= 1.47620 val_rmse= 1.13225 \t\ttime= 0.60439\n",
      "[*] Epoch: 2115 train_loss= 0.99967 train_rmse= 0.76276 val_loss= 1.47443 val_rmse= 1.13119 \t\ttime= 0.62932\n",
      "[*] Epoch: 2116 train_loss= 1.02638 train_rmse= 0.78195 val_loss= 1.47528 val_rmse= 1.13296 \t\ttime= 0.59142\n",
      "[*] Epoch: 2117 train_loss= 1.02187 train_rmse= 0.77540 val_loss= 1.47823 val_rmse= 1.13587 \t\ttime= 0.55950\n",
      "[*] Epoch: 2118 train_loss= 1.00183 train_rmse= 0.75959 val_loss= 1.48313 val_rmse= 1.14005 \t\ttime= 0.62233\n",
      "[*] Epoch: 2119 train_loss= 0.99939 train_rmse= 0.76306 val_loss= 1.48735 val_rmse= 1.14415 \t\ttime= 0.64926\n",
      "[*] Epoch: 2120 train_loss= 0.99386 train_rmse= 0.76045 val_loss= 1.48908 val_rmse= 1.14673 \t\ttime= 0.57447\n",
      "[*] Epoch: 2121 train_loss= 1.02192 train_rmse= 0.77942 val_loss= 1.48890 val_rmse= 1.14799 \t\ttime= 0.60837\n",
      "[*] Epoch: 2122 train_loss= 1.00961 train_rmse= 0.77101 val_loss= 1.48876 val_rmse= 1.14934 \t\ttime= 0.60339\n",
      "[*] Epoch: 2123 train_loss= 0.99770 train_rmse= 0.76554 val_loss= 1.48852 val_rmse= 1.14932 \t\ttime= 0.56649\n",
      "[*] Epoch: 2124 train_loss= 1.00173 train_rmse= 0.76733 val_loss= 1.48668 val_rmse= 1.14649 \t\ttime= 0.60638\n",
      "[*] Epoch: 2125 train_loss= 0.99231 train_rmse= 0.76050 val_loss= 1.48640 val_rmse= 1.14457 \t\ttime= 0.61236\n",
      "[*] Epoch: 2126 train_loss= 0.98821 train_rmse= 0.75908 val_loss= 1.48653 val_rmse= 1.14328 \t\ttime= 0.64926\n",
      "[*] Epoch: 2127 train_loss= 1.00450 train_rmse= 0.76727 val_loss= 1.48542 val_rmse= 1.14183 \t\ttime= 0.59341\n",
      "[*] Epoch: 2128 train_loss= 1.01392 train_rmse= 0.77263 val_loss= 1.48477 val_rmse= 1.14243 \t\ttime= 0.58344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 2129 train_loss= 0.99220 train_rmse= 0.75733 val_loss= 1.48420 val_rmse= 1.14331 \t\ttime= 0.60339\n",
      "[*] Epoch: 2130 train_loss= 1.01024 train_rmse= 0.77019 val_loss= 1.48523 val_rmse= 1.14484 \t\ttime= 0.56449\n",
      "[*] Epoch: 2131 train_loss= 0.99783 train_rmse= 0.76103 val_loss= 1.48663 val_rmse= 1.14514 \t\ttime= 0.60538\n",
      "[*] Epoch: 2132 train_loss= 1.00076 train_rmse= 0.76542 val_loss= 1.48733 val_rmse= 1.14420 \t\ttime= 0.65224\n",
      "[*] Epoch: 2133 train_loss= 1.00455 train_rmse= 0.76651 val_loss= 1.48804 val_rmse= 1.14456 \t\ttime= 0.56250\n",
      "[*] Epoch: 2134 train_loss= 0.99710 train_rmse= 0.76459 val_loss= 1.48813 val_rmse= 1.14514 \t\ttime= 0.59243\n",
      "[*] Epoch: 2135 train_loss= 1.02201 train_rmse= 0.77722 val_loss= 1.48830 val_rmse= 1.14522 \t\ttime= 0.59940\n",
      "[*] Epoch: 2136 train_loss= 1.00318 train_rmse= 0.76458 val_loss= 1.48801 val_rmse= 1.14451 \t\ttime= 0.58045\n",
      "[*] Epoch: 2137 train_loss= 1.00196 train_rmse= 0.76913 val_loss= 1.48763 val_rmse= 1.14347 \t\ttime= 0.57746\n",
      "[*] Epoch: 2138 train_loss= 0.99933 train_rmse= 0.76320 val_loss= 1.48608 val_rmse= 1.14220 \t\ttime= 0.59739\n",
      "[*] Epoch: 2139 train_loss= 1.00450 train_rmse= 0.76725 val_loss= 1.48346 val_rmse= 1.14072 \t\ttime= 0.66821\n",
      "[*] Epoch: 2140 train_loss= 1.00492 train_rmse= 0.76580 val_loss= 1.48499 val_rmse= 1.14373 \t\ttime= 0.58345\n",
      "[*] Epoch: 2141 train_loss= 1.01281 train_rmse= 0.76976 val_loss= 1.48723 val_rmse= 1.14571 \t\ttime= 0.60339\n",
      "[*] Epoch: 2142 train_loss= 1.01175 train_rmse= 0.76738 val_loss= 1.48937 val_rmse= 1.14733 \t\ttime= 0.61336\n",
      "[*] Epoch: 2143 train_loss= 0.99492 train_rmse= 0.76275 val_loss= 1.49209 val_rmse= 1.14957 \t\ttime= 0.53457\n",
      "[*] Epoch: 2144 train_loss= 1.00180 train_rmse= 0.76069 val_loss= 1.49417 val_rmse= 1.15178 \t\ttime= 0.61137\n",
      "[*] Epoch: 2145 train_loss= 1.00938 train_rmse= 0.77268 val_loss= 1.49459 val_rmse= 1.15256 \t\ttime= 0.61136\n",
      "[*] Epoch: 2146 train_loss= 0.99959 train_rmse= 0.76623 val_loss= 1.49526 val_rmse= 1.15494 \t\ttime= 0.62333\n",
      "[*] Epoch: 2147 train_loss= 1.00708 train_rmse= 0.76769 val_loss= 1.49669 val_rmse= 1.15750 \t\ttime= 0.59840\n",
      "[*] Epoch: 2148 train_loss= 1.00805 train_rmse= 0.77788 val_loss= 1.49551 val_rmse= 1.15572 \t\ttime= 0.62532\n",
      "[*] Epoch: 2149 train_loss= 1.00164 train_rmse= 0.76734 val_loss= 1.49253 val_rmse= 1.15091 \t\ttime= 0.59840\n",
      "[*] Epoch: 2150 train_loss= 1.01777 train_rmse= 0.77476 val_loss= 1.49071 val_rmse= 1.14786 \t\ttime= 0.54454\n",
      "[*] Epoch: 2151 train_loss= 1.00678 train_rmse= 0.77111 val_loss= 1.48927 val_rmse= 1.14779 \t\ttime= 0.58743\n",
      "[*] Epoch: 2152 train_loss= 1.00851 train_rmse= 0.77077 val_loss= 1.48716 val_rmse= 1.14692 \t\ttime= 0.67121\n",
      "[*] Epoch: 2153 train_loss= 0.99739 train_rmse= 0.75774 val_loss= 1.48726 val_rmse= 1.14732 \t\ttime= 0.56946\n",
      "[*] Epoch: 2154 train_loss= 1.00555 train_rmse= 0.77016 val_loss= 1.48723 val_rmse= 1.14673 \t\ttime= 0.62333\n",
      "[*] Epoch: 2155 train_loss= 1.01890 train_rmse= 0.77589 val_loss= 1.48657 val_rmse= 1.14488 \t\ttime= 0.60538\n",
      "[*] Epoch: 2156 train_loss= 0.99399 train_rmse= 0.76098 val_loss= 1.48842 val_rmse= 1.14629 \t\ttime= 0.58244\n",
      "[*] Epoch: 2157 train_loss= 1.01694 train_rmse= 0.77758 val_loss= 1.48770 val_rmse= 1.14561 \t\ttime= 0.59241\n",
      "[*] Epoch: 2158 train_loss= 1.00084 train_rmse= 0.76700 val_loss= 1.48553 val_rmse= 1.14416 \t\ttime= 0.61236\n",
      "[*] Epoch: 2159 train_loss= 1.01958 train_rmse= 0.78346 val_loss= 1.48333 val_rmse= 1.14152 \t\ttime= 0.62832\n",
      "[*] Epoch: 2160 train_loss= 1.01212 train_rmse= 0.77621 val_loss= 1.48226 val_rmse= 1.13990 \t\ttime= 0.58045\n",
      "[*] Epoch: 2161 train_loss= 1.00094 train_rmse= 0.76369 val_loss= 1.48272 val_rmse= 1.13918 \t\ttime= 0.58344\n",
      "[*] Epoch: 2162 train_loss= 1.01082 train_rmse= 0.76950 val_loss= 1.48131 val_rmse= 1.13703 \t\ttime= 0.61835\n",
      "[*] Epoch: 2163 train_loss= 1.00596 train_rmse= 0.76197 val_loss= 1.48207 val_rmse= 1.13787 \t\ttime= 0.58643\n",
      "[*] Epoch: 2164 train_loss= 0.99510 train_rmse= 0.75688 val_loss= 1.48546 val_rmse= 1.14281 \t\ttime= 0.63730\n",
      "[*] Epoch: 2165 train_loss= 1.01487 train_rmse= 0.77538 val_loss= 1.48766 val_rmse= 1.14668 \t\ttime= 0.66921\n",
      "[*] Epoch: 2166 train_loss= 1.01309 train_rmse= 0.77139 val_loss= 1.48874 val_rmse= 1.14802 \t\ttime= 0.56449\n",
      "[*] Epoch: 2167 train_loss= 1.00401 train_rmse= 0.76366 val_loss= 1.49090 val_rmse= 1.14928 \t\ttime= 0.59740\n",
      "[*] Epoch: 2168 train_loss= 1.01093 train_rmse= 0.76938 val_loss= 1.49067 val_rmse= 1.14789 \t\ttime= 0.61635\n",
      "[*] Epoch: 2169 train_loss= 1.01275 train_rmse= 0.77355 val_loss= 1.49104 val_rmse= 1.14864 \t\ttime= 0.58543\n",
      "[*] Epoch: 2170 train_loss= 1.00294 train_rmse= 0.76966 val_loss= 1.49133 val_rmse= 1.15017 \t\ttime= 0.59042\n",
      "[*] Epoch: 2171 train_loss= 1.00923 train_rmse= 0.77186 val_loss= 1.48914 val_rmse= 1.14839 \t\ttime= 0.64827\n",
      "[*] Epoch: 2172 train_loss= 0.99545 train_rmse= 0.76519 val_loss= 1.48825 val_rmse= 1.14643 \t\ttime= 0.70412\n",
      "[*] Epoch: 2173 train_loss= 1.00734 train_rmse= 0.77093 val_loss= 1.48696 val_rmse= 1.14415 \t\ttime= 0.65924\n",
      "[*] Epoch: 2174 train_loss= 0.99528 train_rmse= 0.75669 val_loss= 1.48810 val_rmse= 1.14507 \t\ttime= 0.61835\n",
      "[*] Epoch: 2175 train_loss= 0.99040 train_rmse= 0.75470 val_loss= 1.48864 val_rmse= 1.14663 \t\ttime= 0.61635\n",
      "[*] Epoch: 2176 train_loss= 0.98661 train_rmse= 0.75444 val_loss= 1.49221 val_rmse= 1.15149 \t\ttime= 0.57047\n",
      "[*] Epoch: 2177 train_loss= 0.99504 train_rmse= 0.76267 val_loss= 1.49417 val_rmse= 1.15411 \t\ttime= 0.64827\n",
      "[*] Epoch: 2178 train_loss= 1.01571 train_rmse= 0.77079 val_loss= 1.49542 val_rmse= 1.15507 \t\ttime= 0.67818\n",
      "[*] Epoch: 2179 train_loss= 0.99454 train_rmse= 0.76350 val_loss= 1.49519 val_rmse= 1.15411 \t\ttime= 0.60139\n",
      "[*] Epoch: 2180 train_loss= 1.00676 train_rmse= 0.76925 val_loss= 1.49279 val_rmse= 1.15072 \t\ttime= 0.65924\n",
      "[*] Epoch: 2181 train_loss= 0.99917 train_rmse= 0.76889 val_loss= 1.48921 val_rmse= 1.14691 \t\ttime= 0.63331\n",
      "[*] Epoch: 2182 train_loss= 1.00509 train_rmse= 0.77164 val_loss= 1.48748 val_rmse= 1.14497 \t\ttime= 0.53956\n",
      "[*] Epoch: 2183 train_loss= 1.00868 train_rmse= 0.77239 val_loss= 1.48607 val_rmse= 1.14363 \t\ttime= 0.57247\n",
      "[*] Epoch: 2184 train_loss= 0.98892 train_rmse= 0.76137 val_loss= 1.48420 val_rmse= 1.14231 \t\ttime= 0.62134\n",
      "[*] Epoch: 2185 train_loss= 1.00177 train_rmse= 0.76721 val_loss= 1.48446 val_rmse= 1.14245 \t\ttime= 0.63231\n",
      "[*] Epoch: 2186 train_loss= 1.00994 train_rmse= 0.76962 val_loss= 1.48686 val_rmse= 1.14402 \t\ttime= 0.58045\n",
      "[*] Epoch: 2187 train_loss= 1.00339 train_rmse= 0.76838 val_loss= 1.48797 val_rmse= 1.14494 \t\ttime= 0.58543\n",
      "[*] Epoch: 2188 train_loss= 0.99410 train_rmse= 0.75920 val_loss= 1.48949 val_rmse= 1.14716 \t\ttime= 0.62034\n",
      "[*] Epoch: 2189 train_loss= 1.00778 train_rmse= 0.77001 val_loss= 1.48984 val_rmse= 1.14903 \t\ttime= 0.52460\n",
      "[*] Epoch: 2190 train_loss= 1.00450 train_rmse= 0.76900 val_loss= 1.49049 val_rmse= 1.15003 \t\ttime= 0.59840\n",
      "[*] Epoch: 2191 train_loss= 0.99941 train_rmse= 0.76533 val_loss= 1.49183 val_rmse= 1.15086 \t\ttime= 0.64926\n",
      "[*] Epoch: 2192 train_loss= 1.00894 train_rmse= 0.77330 val_loss= 1.49035 val_rmse= 1.14825 \t\ttime= 0.55650\n",
      "[*] Epoch: 2193 train_loss= 1.02055 train_rmse= 0.77837 val_loss= 1.48510 val_rmse= 1.14231 \t\ttime= 0.59940\n",
      "[*] Epoch: 2194 train_loss= 0.99780 train_rmse= 0.76154 val_loss= 1.47964 val_rmse= 1.13684 \t\ttime= 0.62333\n",
      "[*] Epoch: 2195 train_loss= 1.00057 train_rmse= 0.76233 val_loss= 1.47754 val_rmse= 1.13559 \t\ttime= 0.57945\n",
      "[*] Epoch: 2196 train_loss= 1.00037 train_rmse= 0.76426 val_loss= 1.47663 val_rmse= 1.13411 \t\ttime= 0.60438\n",
      "[*] Epoch: 2197 train_loss= 1.00090 train_rmse= 0.75817 val_loss= 1.47926 val_rmse= 1.13490 \t\ttime= 0.59441\n",
      "[*] Epoch: 2198 train_loss= 0.99854 train_rmse= 0.76605 val_loss= 1.48237 val_rmse= 1.13662 \t\ttime= 0.65724\n",
      "[*] Epoch: 2199 train_loss= 1.00532 train_rmse= 0.76465 val_loss= 1.48492 val_rmse= 1.14026 \t\ttime= 0.55950\n",
      "[*] Epoch: 2200 train_loss= 1.00444 train_rmse= 0.76902 val_loss= 1.48792 val_rmse= 1.14491 \t\ttime= 0.60738\n",
      "[*] Epoch: 2201 train_loss= 1.00556 train_rmse= 0.77006 val_loss= 1.48944 val_rmse= 1.14820 \t\ttime= 0.59441\n",
      "[*] Epoch: 2202 train_loss= 1.01751 train_rmse= 0.77942 val_loss= 1.48851 val_rmse= 1.14755 \t\ttime= 0.54853\n",
      "[*] Epoch: 2203 train_loss= 1.00225 train_rmse= 0.76331 val_loss= 1.48806 val_rmse= 1.14599 \t\ttime= 0.61635\n",
      "[*] Epoch: 2204 train_loss= 1.02358 train_rmse= 0.78042 val_loss= 1.48783 val_rmse= 1.14400 \t\ttime= 0.63831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 2205 train_loss= 0.99822 train_rmse= 0.76384 val_loss= 1.48801 val_rmse= 1.14327 \t\ttime= 0.60738\n",
      "[*] Epoch: 2206 train_loss= 0.99641 train_rmse= 0.76308 val_loss= 1.48708 val_rmse= 1.14359 \t\ttime= 0.59541\n",
      "[*] Epoch: 2207 train_loss= 0.99553 train_rmse= 0.76079 val_loss= 1.48473 val_rmse= 1.14322 \t\ttime= 0.61037\n",
      "[*] Epoch: 2208 train_loss= 1.00207 train_rmse= 0.76712 val_loss= 1.48313 val_rmse= 1.14254 \t\ttime= 0.59840\n",
      "[*] Epoch: 2209 train_loss= 0.99778 train_rmse= 0.76020 val_loss= 1.48577 val_rmse= 1.14437 \t\ttime= 0.56948\n",
      "[*] Epoch: 2210 train_loss= 1.00294 train_rmse= 0.76223 val_loss= 1.48897 val_rmse= 1.14607 \t\ttime= 0.59840\n",
      "[*] Epoch: 2211 train_loss= 0.99171 train_rmse= 0.75966 val_loss= 1.49064 val_rmse= 1.14782 \t\ttime= 0.67121\n",
      "[*] Epoch: 2212 train_loss= 1.00648 train_rmse= 0.76669 val_loss= 1.49120 val_rmse= 1.15010 \t\ttime= 0.58144\n",
      "[*] Epoch: 2213 train_loss= 0.99794 train_rmse= 0.75874 val_loss= 1.49010 val_rmse= 1.15162 \t\ttime= 0.61037\n",
      "[*] Epoch: 2214 train_loss= 1.02346 train_rmse= 0.78390 val_loss= 1.48746 val_rmse= 1.14976 \t\ttime= 0.60439\n",
      "[*] Epoch: 2215 train_loss= 0.99867 train_rmse= 0.76438 val_loss= 1.48667 val_rmse= 1.14798 \t\ttime= 0.56150\n",
      "[*] Epoch: 2216 train_loss= 1.00459 train_rmse= 0.76870 val_loss= 1.48512 val_rmse= 1.14379 \t\ttime= 0.58943\n",
      "[*] Epoch: 2217 train_loss= 1.00676 train_rmse= 0.77323 val_loss= 1.48319 val_rmse= 1.13902 \t\ttime= 0.62333\n",
      "[*] Epoch: 2218 train_loss= 0.99812 train_rmse= 0.76146 val_loss= 1.48332 val_rmse= 1.13787 \t\ttime= 0.65325\n",
      "[*] Epoch: 2219 train_loss= 0.99850 train_rmse= 0.76775 val_loss= 1.48308 val_rmse= 1.13865 \t\ttime= 0.59541\n",
      "[*] Epoch: 2220 train_loss= 0.99095 train_rmse= 0.76054 val_loss= 1.48349 val_rmse= 1.13991 \t\ttime= 0.61137\n",
      "[*] Epoch: 2221 train_loss= 0.99172 train_rmse= 0.75764 val_loss= 1.48614 val_rmse= 1.14376 \t\ttime= 0.60738\n",
      "[*] Epoch: 2222 train_loss= 1.00345 train_rmse= 0.76244 val_loss= 1.48836 val_rmse= 1.14604 \t\ttime= 0.53756\n",
      "[*] Epoch: 2223 train_loss= 0.99606 train_rmse= 0.76150 val_loss= 1.49036 val_rmse= 1.14781 \t\ttime= 0.62932\n",
      "[*] Epoch: 2224 train_loss= 0.99160 train_rmse= 0.75930 val_loss= 1.49220 val_rmse= 1.14899 \t\ttime= 0.66323\n",
      "[*] Epoch: 2225 train_loss= 0.98198 train_rmse= 0.75062 val_loss= 1.49472 val_rmse= 1.15198 \t\ttime= 0.56948\n",
      "[*] Epoch: 2226 train_loss= 0.99170 train_rmse= 0.75650 val_loss= 1.49565 val_rmse= 1.15327 \t\ttime= 0.59042\n",
      "[*] Epoch: 2227 train_loss= 0.99303 train_rmse= 0.76098 val_loss= 1.49557 val_rmse= 1.15341 \t\ttime= 0.61137\n",
      "[*] Epoch: 2228 train_loss= 0.99270 train_rmse= 0.75960 val_loss= 1.49318 val_rmse= 1.15200 \t\ttime= 0.57048\n",
      "[*] Epoch: 2229 train_loss= 0.99317 train_rmse= 0.76338 val_loss= 1.49025 val_rmse= 1.14893 \t\ttime= 0.59541\n",
      "[*] Epoch: 2230 train_loss= 0.98648 train_rmse= 0.75416 val_loss= 1.48912 val_rmse= 1.14672 \t\ttime= 0.61934\n",
      "[*] Epoch: 2231 train_loss= 1.00194 train_rmse= 0.76978 val_loss= 1.48838 val_rmse= 1.14478 \t\ttime= 0.65425\n",
      "[*] Epoch: 2232 train_loss= 1.00422 train_rmse= 0.76984 val_loss= 1.48825 val_rmse= 1.14391 \t\ttime= 0.57546\n",
      "[*] Epoch: 2233 train_loss= 0.98549 train_rmse= 0.75500 val_loss= 1.48809 val_rmse= 1.14438 \t\ttime= 0.60039\n",
      "[*] Epoch: 2234 train_loss= 0.99659 train_rmse= 0.75940 val_loss= 1.48963 val_rmse= 1.14806 \t\ttime= 0.59740\n",
      "[*] Epoch: 2235 train_loss= 1.00307 train_rmse= 0.76179 val_loss= 1.49143 val_rmse= 1.15123 \t\ttime= 0.55252\n",
      "[*] Epoch: 2236 train_loss= 0.99704 train_rmse= 0.76069 val_loss= 1.49335 val_rmse= 1.15241 \t\ttime= 0.61735\n",
      "[*] Epoch: 2237 train_loss= 1.00530 train_rmse= 0.77214 val_loss= 1.49368 val_rmse= 1.15127 \t\ttime= 0.64629\n",
      "[*] Epoch: 2238 train_loss= 0.98953 train_rmse= 0.75850 val_loss= 1.49321 val_rmse= 1.15002 \t\ttime= 0.58444\n",
      "[*] Epoch: 2239 train_loss= 0.99561 train_rmse= 0.76117 val_loss= 1.49075 val_rmse= 1.14789 \t\ttime= 0.61037\n",
      "[*] Epoch: 2240 train_loss= 1.00254 train_rmse= 0.77081 val_loss= 1.48744 val_rmse= 1.14451 \t\ttime= 0.61236\n",
      "[*] Epoch: 2241 train_loss= 1.00205 train_rmse= 0.76847 val_loss= 1.48567 val_rmse= 1.14261 \t\ttime= 0.59142\n",
      "[*] Epoch: 2242 train_loss= 0.99384 train_rmse= 0.76631 val_loss= 1.48408 val_rmse= 1.14052 \t\ttime= 0.59641\n",
      "[*] Epoch: 2243 train_loss= 0.99655 train_rmse= 0.76024 val_loss= 1.48483 val_rmse= 1.14135 \t\ttime= 0.58244\n",
      "[*] Epoch: 2244 train_loss= 1.00521 train_rmse= 0.76683 val_loss= 1.48533 val_rmse= 1.14171 \t\ttime= 0.68816\n",
      "[*] Epoch: 2245 train_loss= 1.00173 train_rmse= 0.76319 val_loss= 1.48473 val_rmse= 1.14138 \t\ttime= 0.55851\n",
      "[*] Epoch: 2246 train_loss= 0.99266 train_rmse= 0.76102 val_loss= 1.48624 val_rmse= 1.14374 \t\ttime= 0.63629\n",
      "[*] Epoch: 2247 train_loss= 0.98770 train_rmse= 0.75640 val_loss= 1.48819 val_rmse= 1.14617 \t\ttime= 0.62234\n",
      "[*] Epoch: 2248 train_loss= 1.00017 train_rmse= 0.76851 val_loss= 1.48965 val_rmse= 1.14814 \t\ttime= 0.60339\n",
      "[*] Epoch: 2249 train_loss= 1.00640 train_rmse= 0.77375 val_loss= 1.49107 val_rmse= 1.14993 \t\ttime= 0.62034\n",
      "[*] Epoch: 2250 train_loss= 1.00155 train_rmse= 0.77081 val_loss= 1.49136 val_rmse= 1.15004 \t\ttime= 0.66322\n",
      "[*] Epoch: 2251 train_loss= 0.98298 train_rmse= 0.75338 val_loss= 1.49076 val_rmse= 1.14951 \t\ttime= 0.56349\n",
      "[*] Epoch: 2252 train_loss= 1.00740 train_rmse= 0.76996 val_loss= 1.48956 val_rmse= 1.14821 \t\ttime= 0.61934\n",
      "[*] Epoch: 2253 train_loss= 0.97426 train_rmse= 0.74861 val_loss= 1.48824 val_rmse= 1.14668 \t\ttime= 0.61436\n",
      "[*] Epoch: 2254 train_loss= 1.00128 train_rmse= 0.76403 val_loss= 1.48653 val_rmse= 1.14541 \t\ttime= 0.57446\n",
      "[*] Epoch: 2255 train_loss= 1.00612 train_rmse= 0.77225 val_loss= 1.48420 val_rmse= 1.14391 \t\ttime= 0.58743\n",
      "[*] Epoch: 2256 train_loss= 0.99589 train_rmse= 0.76089 val_loss= 1.48445 val_rmse= 1.14369 \t\ttime= 0.58543\n",
      "[*] Epoch: 2257 train_loss= 0.99505 train_rmse= 0.75865 val_loss= 1.48526 val_rmse= 1.14432 \t\ttime= 0.66722\n",
      "[*] Epoch: 2258 train_loss= 0.98215 train_rmse= 0.75503 val_loss= 1.48610 val_rmse= 1.14496 \t\ttime= 0.54754\n",
      "[*] Epoch: 2259 train_loss= 1.00289 train_rmse= 0.76557 val_loss= 1.48675 val_rmse= 1.14558 \t\ttime= 0.60738\n",
      "[*] Epoch: 2260 train_loss= 0.99951 train_rmse= 0.76651 val_loss= 1.48741 val_rmse= 1.14585 \t\ttime= 0.63031\n",
      "[*] Epoch: 2261 train_loss= 1.00507 train_rmse= 0.76727 val_loss= 1.48699 val_rmse= 1.14479 \t\ttime= 0.56948\n",
      "[*] Epoch: 2262 train_loss= 1.01025 train_rmse= 0.77255 val_loss= 1.48691 val_rmse= 1.14500 \t\ttime= 0.61535\n",
      "[*] Epoch: 2263 train_loss= 1.00629 train_rmse= 0.77164 val_loss= 1.48603 val_rmse= 1.14487 \t\ttime= 0.66224\n",
      "[*] Epoch: 2264 train_loss= 0.99706 train_rmse= 0.76431 val_loss= 1.48486 val_rmse= 1.14365 \t\ttime= 0.61236\n",
      "[*] Epoch: 2265 train_loss= 0.99884 train_rmse= 0.76396 val_loss= 1.48207 val_rmse= 1.14078 \t\ttime= 0.64029\n",
      "[*] Epoch: 2266 train_loss= 1.01019 train_rmse= 0.77068 val_loss= 1.47951 val_rmse= 1.13765 \t\ttime= 0.59940\n",
      "[*] Epoch: 2267 train_loss= 0.99869 train_rmse= 0.76013 val_loss= 1.48000 val_rmse= 1.13736 \t\ttime= 0.59042\n",
      "[*] Epoch: 2268 train_loss= 1.00616 train_rmse= 0.76562 val_loss= 1.48139 val_rmse= 1.13826 \t\ttime= 0.58045\n",
      "[*] Epoch: 2269 train_loss= 1.00170 train_rmse= 0.76580 val_loss= 1.48276 val_rmse= 1.13949 \t\ttime= 0.60239\n",
      "[*] Epoch: 2270 train_loss= 1.00908 train_rmse= 0.76956 val_loss= 1.48438 val_rmse= 1.14212 \t\ttime= 0.66223\n",
      "[*] Epoch: 2271 train_loss= 1.01586 train_rmse= 0.78221 val_loss= 1.48668 val_rmse= 1.14501 \t\ttime= 0.55851\n",
      "[*] Epoch: 2272 train_loss= 0.98033 train_rmse= 0.75574 val_loss= 1.48944 val_rmse= 1.14788 \t\ttime= 0.60139\n",
      "[*] Epoch: 2273 train_loss= 0.98165 train_rmse= 0.75759 val_loss= 1.49036 val_rmse= 1.14859 \t\ttime= 0.58244\n",
      "[*] Epoch: 2274 train_loss= 1.01939 train_rmse= 0.77911 val_loss= 1.48688 val_rmse= 1.14397 \t\ttime= 0.55351\n",
      "[*] Epoch: 2275 train_loss= 0.99706 train_rmse= 0.76222 val_loss= 1.48500 val_rmse= 1.14131 \t\ttime= 0.60538\n",
      "[*] Epoch: 2276 train_loss= 1.00114 train_rmse= 0.76632 val_loss= 1.48323 val_rmse= 1.13917 \t\ttime= 0.63729\n",
      "[*] Epoch: 2277 train_loss= 1.00239 train_rmse= 0.76923 val_loss= 1.48193 val_rmse= 1.13893 \t\ttime= 0.62533\n",
      "[*] Epoch: 2278 train_loss= 0.99639 train_rmse= 0.76259 val_loss= 1.47956 val_rmse= 1.13781 \t\ttime= 0.61934\n",
      "[*] Epoch: 2279 train_loss= 1.00229 train_rmse= 0.76450 val_loss= 1.47909 val_rmse= 1.13807 \t\ttime= 0.58845\n",
      "[*] Epoch: 2280 train_loss= 0.99735 train_rmse= 0.76433 val_loss= 1.47958 val_rmse= 1.13790 \t\ttime= 0.59641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 2281 train_loss= 0.99772 train_rmse= 0.76635 val_loss= 1.48050 val_rmse= 1.13804 \t\ttime= 0.54654\n",
      "[*] Epoch: 2282 train_loss= 0.98311 train_rmse= 0.75223 val_loss= 1.48273 val_rmse= 1.13977 \t\ttime= 0.63530\n",
      "[*] Epoch: 2283 train_loss= 1.00923 train_rmse= 0.77074 val_loss= 1.48242 val_rmse= 1.13971 \t\ttime= 0.67819\n",
      "[*] Epoch: 2284 train_loss= 0.98769 train_rmse= 0.75429 val_loss= 1.48300 val_rmse= 1.14193 \t\ttime= 0.59142\n",
      "[*] Epoch: 2285 train_loss= 0.99913 train_rmse= 0.76369 val_loss= 1.48501 val_rmse= 1.14566 \t\ttime= 0.59940\n",
      "[*] Epoch: 2286 train_loss= 0.99525 train_rmse= 0.76348 val_loss= 1.48663 val_rmse= 1.14731 \t\ttime= 0.67121\n",
      "[*] Epoch: 2287 train_loss= 1.00333 train_rmse= 0.76975 val_loss= 1.48786 val_rmse= 1.14777 \t\ttime= 0.56848\n",
      "[*] Epoch: 2288 train_loss= 1.00286 train_rmse= 0.77201 val_loss= 1.48792 val_rmse= 1.14651 \t\ttime= 0.62732\n",
      "[*] Epoch: 2289 train_loss= 1.00058 train_rmse= 0.76467 val_loss= 1.48848 val_rmse= 1.14677 \t\ttime= 0.65325\n",
      "[*] Epoch: 2290 train_loss= 0.97712 train_rmse= 0.74945 val_loss= 1.49042 val_rmse= 1.14987 \t\ttime= 0.64627\n",
      "[*] Epoch: 2291 train_loss= 1.00686 train_rmse= 0.77040 val_loss= 1.49165 val_rmse= 1.15297 \t\ttime= 0.60339\n",
      "[*] Epoch: 2292 train_loss= 0.99398 train_rmse= 0.76234 val_loss= 1.49169 val_rmse= 1.15255 \t\ttime= 0.61735\n",
      "[*] Epoch: 2293 train_loss= 1.00417 train_rmse= 0.77097 val_loss= 1.49020 val_rmse= 1.14981 \t\ttime= 0.60039\n",
      "[*] Epoch: 2294 train_loss= 0.99373 train_rmse= 0.75922 val_loss= 1.48933 val_rmse= 1.14723 \t\ttime= 0.59940\n",
      "[*] Epoch: 2295 train_loss= 1.00306 train_rmse= 0.77039 val_loss= 1.48805 val_rmse= 1.14465 \t\ttime= 0.64727\n",
      "[*] Epoch: 2296 train_loss= 0.98826 train_rmse= 0.75444 val_loss= 1.48838 val_rmse= 1.14631 \t\ttime= 0.69714\n",
      "[*] Epoch: 2297 train_loss= 1.00246 train_rmse= 0.76889 val_loss= 1.48783 val_rmse= 1.14651 \t\ttime= 0.55651\n",
      "[*] Epoch: 2298 train_loss= 0.99392 train_rmse= 0.75690 val_loss= 1.48933 val_rmse= 1.14765 \t\ttime= 0.63131\n",
      "[*] Epoch: 2299 train_loss= 1.00578 train_rmse= 0.77058 val_loss= 1.48896 val_rmse= 1.14628 \t\ttime= 0.64527\n",
      "[*] Epoch: 2300 train_loss= 0.99970 train_rmse= 0.76492 val_loss= 1.48891 val_rmse= 1.14533 \t\ttime= 0.59142\n",
      "[*] Epoch: 2301 train_loss= 1.00394 train_rmse= 0.76767 val_loss= 1.48620 val_rmse= 1.14296 \t\ttime= 0.65126\n",
      "[*] Epoch: 2302 train_loss= 0.98507 train_rmse= 0.75570 val_loss= 1.48514 val_rmse= 1.14235 \t\ttime= 0.68816\n",
      "[*] Epoch: 2303 train_loss= 0.98986 train_rmse= 0.75536 val_loss= 1.48667 val_rmse= 1.14539 \t\ttime= 0.60438\n",
      "[*] Epoch: 2304 train_loss= 1.01670 train_rmse= 0.77481 val_loss= 1.48979 val_rmse= 1.14917 \t\ttime= 0.61436\n",
      "[*] Epoch: 2305 train_loss= 0.99263 train_rmse= 0.76272 val_loss= 1.49103 val_rmse= 1.14982 \t\ttime= 0.60936\n",
      "[*] Epoch: 2306 train_loss= 1.00105 train_rmse= 0.76814 val_loss= 1.49024 val_rmse= 1.14812 \t\ttime= 0.56648\n",
      "[*] Epoch: 2307 train_loss= 0.98984 train_rmse= 0.76192 val_loss= 1.48789 val_rmse= 1.14511 \t\ttime= 0.61236\n",
      "[*] Epoch: 2308 train_loss= 0.99384 train_rmse= 0.75790 val_loss= 1.48681 val_rmse= 1.14455 \t\ttime= 0.62134\n",
      "[*] Epoch: 2309 train_loss= 1.00744 train_rmse= 0.77537 val_loss= 1.48650 val_rmse= 1.14491 \t\ttime= 0.63131\n",
      "[*] Epoch: 2310 train_loss= 1.00400 train_rmse= 0.76595 val_loss= 1.48600 val_rmse= 1.14446 \t\ttime= 0.58245\n",
      "[*] Epoch: 2311 train_loss= 0.99377 train_rmse= 0.76556 val_loss= 1.48712 val_rmse= 1.14495 \t\ttime= 0.62333\n",
      "[*] Epoch: 2312 train_loss= 1.01028 train_rmse= 0.77167 val_loss= 1.48749 val_rmse= 1.14460 \t\ttime= 0.61634\n",
      "[*] Epoch: 2313 train_loss= 0.98525 train_rmse= 0.75718 val_loss= 1.48673 val_rmse= 1.14397 \t\ttime= 0.52958\n",
      "[*] Epoch: 2314 train_loss= 0.99940 train_rmse= 0.76623 val_loss= 1.48324 val_rmse= 1.14136 \t\ttime= 0.60239\n",
      "[*] Epoch: 2315 train_loss= 0.99374 train_rmse= 0.76329 val_loss= 1.48116 val_rmse= 1.13953 \t\ttime= 0.65126\n",
      "[*] Epoch: 2316 train_loss= 0.99417 train_rmse= 0.76083 val_loss= 1.48009 val_rmse= 1.13860 \t\ttime= 0.59042\n",
      "[*] Epoch: 2317 train_loss= 1.00173 train_rmse= 0.76563 val_loss= 1.48447 val_rmse= 1.14377 \t\ttime= 0.60339\n",
      "[*] Epoch: 2318 train_loss= 1.00907 train_rmse= 0.77026 val_loss= 1.48646 val_rmse= 1.14592 \t\ttime= 0.61635\n",
      "[*] Epoch: 2319 train_loss= 1.00067 train_rmse= 0.76563 val_loss= 1.48663 val_rmse= 1.14617 \t\ttime= 0.57646\n",
      "[*] Epoch: 2320 train_loss= 0.98942 train_rmse= 0.75961 val_loss= 1.48588 val_rmse= 1.14561 \t\ttime= 0.59641\n",
      "[*] Epoch: 2321 train_loss= 0.99406 train_rmse= 0.75511 val_loss= 1.48870 val_rmse= 1.14811 \t\ttime= 0.57446\n",
      "[*] Epoch: 2322 train_loss= 0.98844 train_rmse= 0.76033 val_loss= 1.49266 val_rmse= 1.15241 \t\ttime= 0.65724\n",
      "[*] Epoch: 2323 train_loss= 1.00702 train_rmse= 0.76642 val_loss= 1.49365 val_rmse= 1.15410 \t\ttime= 0.54554\n",
      "[*] Epoch: 2324 train_loss= 1.00003 train_rmse= 0.76659 val_loss= 1.49335 val_rmse= 1.15431 \t\ttime= 0.61037\n",
      "[*] Epoch: 2325 train_loss= 0.99915 train_rmse= 0.76301 val_loss= 1.49198 val_rmse= 1.15340 \t\ttime= 0.59541\n",
      "[*] Epoch: 2326 train_loss= 1.00378 train_rmse= 0.76986 val_loss= 1.49409 val_rmse= 1.15516 \t\ttime= 0.53856\n",
      "[*] Epoch: 2327 train_loss= 0.99089 train_rmse= 0.76212 val_loss= 1.49373 val_rmse= 1.15396 \t\ttime= 0.60837\n",
      "[*] Epoch: 2328 train_loss= 0.98886 train_rmse= 0.76002 val_loss= 1.49230 val_rmse= 1.15184 \t\ttime= 0.65126\n",
      "[*] Epoch: 2329 train_loss= 1.00349 train_rmse= 0.76613 val_loss= 1.49151 val_rmse= 1.15028 \t\ttime= 0.62034\n",
      "[*] Epoch: 2330 train_loss= 1.00135 train_rmse= 0.76363 val_loss= 1.49327 val_rmse= 1.15249 \t\ttime= 0.58743\n",
      "[*] Epoch: 2331 train_loss= 0.99082 train_rmse= 0.76297 val_loss= 1.49348 val_rmse= 1.15349 \t\ttime= 0.57247\n",
      "[*] Epoch: 2332 train_loss= 0.99775 train_rmse= 0.76757 val_loss= 1.49296 val_rmse= 1.15290 \t\ttime= 0.62633\n",
      "[*] Epoch: 2333 train_loss= 0.98942 train_rmse= 0.75266 val_loss= 1.49122 val_rmse= 1.15048 \t\ttime= 0.54953\n",
      "[*] Epoch: 2334 train_loss= 0.98535 train_rmse= 0.75512 val_loss= 1.48957 val_rmse= 1.14802 \t\ttime= 0.58743\n",
      "[*] Epoch: 2335 train_loss= 1.00119 train_rmse= 0.76684 val_loss= 1.48650 val_rmse= 1.14385 \t\ttime= 0.66722\n",
      "[*] Epoch: 2336 train_loss= 0.99585 train_rmse= 0.76239 val_loss= 1.48374 val_rmse= 1.14071 \t\ttime= 0.58743\n",
      "[*] Epoch: 2337 train_loss= 0.98774 train_rmse= 0.75652 val_loss= 1.48235 val_rmse= 1.13876 \t\ttime= 0.60039\n",
      "[*] Epoch: 2338 train_loss= 0.99137 train_rmse= 0.76089 val_loss= 1.48204 val_rmse= 1.13903 \t\ttime= 0.60040\n",
      "[*] Epoch: 2339 train_loss= 0.98219 train_rmse= 0.75363 val_loss= 1.48208 val_rmse= 1.14013 \t\ttime= 0.56848\n",
      "[*] Epoch: 2340 train_loss= 0.98092 train_rmse= 0.75248 val_loss= 1.48432 val_rmse= 1.14280 \t\ttime= 0.62633\n",
      "[*] Epoch: 2341 train_loss= 1.01140 train_rmse= 0.77277 val_loss= 1.48570 val_rmse= 1.14431 \t\ttime= 0.61835\n",
      "[*] Epoch: 2342 train_loss= 1.00354 train_rmse= 0.77009 val_loss= 1.48740 val_rmse= 1.14476 \t\ttime= 0.64526\n",
      "[*] Epoch: 2343 train_loss= 0.99051 train_rmse= 0.75533 val_loss= 1.49032 val_rmse= 1.14786 \t\ttime= 0.57546\n",
      "[*] Epoch: 2344 train_loss= 0.98741 train_rmse= 0.75770 val_loss= 1.49222 val_rmse= 1.15082 \t\ttime= 0.58644\n",
      "[*] Epoch: 2345 train_loss= 1.00057 train_rmse= 0.76784 val_loss= 1.49185 val_rmse= 1.15107 \t\ttime= 0.61735\n",
      "[*] Epoch: 2346 train_loss= 0.97815 train_rmse= 0.74994 val_loss= 1.49281 val_rmse= 1.15257 \t\ttime= 0.55950\n",
      "[*] Epoch: 2347 train_loss= 0.98168 train_rmse= 0.75391 val_loss= 1.49366 val_rmse= 1.15259 \t\ttime= 0.60339\n",
      "[*] Epoch: 2348 train_loss= 0.99597 train_rmse= 0.76005 val_loss= 1.49361 val_rmse= 1.15150 \t\ttime= 0.68417\n",
      "[*] Epoch: 2349 train_loss= 1.00050 train_rmse= 0.76454 val_loss= 1.49383 val_rmse= 1.15168 \t\ttime= 0.58145\n",
      "[*] Epoch: 2350 train_loss= 0.99505 train_rmse= 0.76224 val_loss= 1.49457 val_rmse= 1.15299 \t\ttime= 0.61535\n",
      "[*] Epoch: 2351 train_loss= 0.97768 train_rmse= 0.74549 val_loss= 1.49743 val_rmse= 1.15699 \t\ttime= 0.60239\n",
      "[*] Epoch: 2352 train_loss= 0.99248 train_rmse= 0.76359 val_loss= 1.49810 val_rmse= 1.15802 \t\ttime= 0.57446\n",
      "[*] Epoch: 2353 train_loss= 1.00473 train_rmse= 0.77345 val_loss= 1.49855 val_rmse= 1.15759 \t\ttime= 0.59242\n",
      "[*] Epoch: 2354 train_loss= 1.00030 train_rmse= 0.76895 val_loss= 1.49642 val_rmse= 1.15407 \t\ttime= 0.58244\n",
      "[*] Epoch: 2355 train_loss= 0.99054 train_rmse= 0.76519 val_loss= 1.49413 val_rmse= 1.15159 \t\ttime= 0.66223\n",
      "[*] Epoch: 2356 train_loss= 0.99678 train_rmse= 0.76460 val_loss= 1.48994 val_rmse= 1.14769 \t\ttime= 0.58344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 2357 train_loss= 0.99022 train_rmse= 0.75541 val_loss= 1.48730 val_rmse= 1.14538 \t\ttime= 0.60338\n",
      "[*] Epoch: 2358 train_loss= 1.01049 train_rmse= 0.76795 val_loss= 1.48717 val_rmse= 1.14452 \t\ttime= 0.62733\n",
      "[*] Epoch: 2359 train_loss= 0.99263 train_rmse= 0.76234 val_loss= 1.48741 val_rmse= 1.14439 \t\ttime= 0.52660\n",
      "[*] Epoch: 2360 train_loss= 0.99065 train_rmse= 0.75920 val_loss= 1.48779 val_rmse= 1.14432 \t\ttime= 0.60738\n",
      "[*] Epoch: 2361 train_loss= 0.99993 train_rmse= 0.76180 val_loss= 1.48903 val_rmse= 1.14677 \t\ttime= 0.65127\n",
      "[*] Epoch: 2362 train_loss= 0.98464 train_rmse= 0.75909 val_loss= 1.48948 val_rmse= 1.14865 \t\ttime= 0.63031\n",
      "[*] Epoch: 2363 train_loss= 0.98136 train_rmse= 0.75621 val_loss= 1.49067 val_rmse= 1.15062 \t\ttime= 0.66821\n",
      "[*] Epoch: 2364 train_loss= 0.99578 train_rmse= 0.76382 val_loss= 1.49180 val_rmse= 1.15125 \t\ttime= 0.64128\n",
      "[*] Epoch: 2365 train_loss= 0.98827 train_rmse= 0.75893 val_loss= 1.49428 val_rmse= 1.15203 \t\ttime= 0.61436\n",
      "[*] Epoch: 2366 train_loss= 0.99539 train_rmse= 0.76288 val_loss= 1.49546 val_rmse= 1.15211 \t\ttime= 0.64128\n",
      "[*] Epoch: 2367 train_loss= 0.98555 train_rmse= 0.75596 val_loss= 1.49554 val_rmse= 1.15255 \t\ttime= 0.63929\n",
      "[*] Epoch: 2368 train_loss= 0.98739 train_rmse= 0.75313 val_loss= 1.49545 val_rmse= 1.15456 \t\ttime= 0.65125\n",
      "[*] Epoch: 2369 train_loss= 0.99161 train_rmse= 0.75513 val_loss= 1.49638 val_rmse= 1.15700 \t\ttime= 0.62234\n",
      "[*] Epoch: 2370 train_loss= 0.99992 train_rmse= 0.76363 val_loss= 1.49574 val_rmse= 1.15574 \t\ttime= 0.62433\n",
      "[*] Epoch: 2371 train_loss= 0.99542 train_rmse= 0.76584 val_loss= 1.49573 val_rmse= 1.15379 \t\ttime= 0.63530\n",
      "[*] Epoch: 2372 train_loss= 1.00058 train_rmse= 0.77070 val_loss= 1.49491 val_rmse= 1.15134 \t\ttime= 0.58843\n",
      "[*] Epoch: 2373 train_loss= 0.98227 train_rmse= 0.75024 val_loss= 1.49427 val_rmse= 1.15090 \t\ttime= 0.63231\n",
      "[*] Epoch: 2374 train_loss= 0.98403 train_rmse= 0.76063 val_loss= 1.49179 val_rmse= 1.14989 \t\ttime= 0.70312\n",
      "[*] Epoch: 2375 train_loss= 0.98174 train_rmse= 0.75605 val_loss= 1.49172 val_rmse= 1.14997 \t\ttime= 0.57547\n",
      "[*] Epoch: 2376 train_loss= 0.98537 train_rmse= 0.75655 val_loss= 1.49230 val_rmse= 1.14995 \t\ttime= 0.62134\n",
      "[*] Epoch: 2377 train_loss= 0.97989 train_rmse= 0.75801 val_loss= 1.49260 val_rmse= 1.15041 \t\ttime= 0.63331\n",
      "[*] Epoch: 2378 train_loss= 0.99310 train_rmse= 0.76179 val_loss= 1.49237 val_rmse= 1.15091 \t\ttime= 0.57446\n",
      "[*] Epoch: 2379 train_loss= 0.98551 train_rmse= 0.75495 val_loss= 1.49225 val_rmse= 1.15163 \t\ttime= 0.62532\n",
      "[*] Epoch: 2380 train_loss= 0.97436 train_rmse= 0.74496 val_loss= 1.49347 val_rmse= 1.15217 \t\ttime= 0.65525\n",
      "[*] Epoch: 2381 train_loss= 0.99848 train_rmse= 0.76595 val_loss= 1.49259 val_rmse= 1.14965 \t\ttime= 0.62034\n",
      "[*] Epoch: 2382 train_loss= 0.99189 train_rmse= 0.75925 val_loss= 1.49309 val_rmse= 1.14926 \t\ttime= 0.62832\n",
      "[*] Epoch: 2383 train_loss= 1.01243 train_rmse= 0.77372 val_loss= 1.49031 val_rmse= 1.14716 \t\ttime= 0.60837\n",
      "[*] Epoch: 2384 train_loss= 0.98980 train_rmse= 0.75788 val_loss= 1.48652 val_rmse= 1.14439 \t\ttime= 0.59541\n",
      "[*] Epoch: 2385 train_loss= 0.98210 train_rmse= 0.75400 val_loss= 1.48503 val_rmse= 1.14311 \t\ttime= 0.61934\n",
      "[*] Epoch: 2386 train_loss= 0.98283 train_rmse= 0.75265 val_loss= 1.48616 val_rmse= 1.14397 \t\ttime= 0.61137\n",
      "[*] Epoch: 2387 train_loss= 0.99383 train_rmse= 0.75840 val_loss= 1.48897 val_rmse= 1.14533 \t\ttime= 0.67519\n",
      "[*] Epoch: 2388 train_loss= 0.98705 train_rmse= 0.76074 val_loss= 1.48990 val_rmse= 1.14525 \t\ttime= 0.58045\n",
      "[*] Epoch: 2389 train_loss= 0.98938 train_rmse= 0.75762 val_loss= 1.48969 val_rmse= 1.14509 \t\ttime= 0.59940\n",
      "[*] Epoch: 2390 train_loss= 0.99705 train_rmse= 0.76893 val_loss= 1.48834 val_rmse= 1.14470 \t\ttime= 0.61535\n",
      "[*] Epoch: 2391 train_loss= 0.98395 train_rmse= 0.75737 val_loss= 1.48763 val_rmse= 1.14562 \t\ttime= 0.53258\n",
      "[*] Epoch: 2392 train_loss= 1.00053 train_rmse= 0.76170 val_loss= 1.48840 val_rmse= 1.14791 \t\ttime= 0.61536\n",
      "[*] Epoch: 2393 train_loss= 0.97376 train_rmse= 0.74504 val_loss= 1.49219 val_rmse= 1.15171 \t\ttime= 0.60938\n",
      "[*] Epoch: 2394 train_loss= 1.01311 train_rmse= 0.77518 val_loss= 1.49361 val_rmse= 1.15119 \t\ttime= 0.63031\n",
      "[*] Epoch: 2395 train_loss= 0.98786 train_rmse= 0.76111 val_loss= 1.49556 val_rmse= 1.15140 \t\ttime= 0.64428\n",
      "[*] Epoch: 2396 train_loss= 0.98322 train_rmse= 0.75378 val_loss= 1.49728 val_rmse= 1.15295 \t\ttime= 0.62034\n",
      "[*] Epoch: 2397 train_loss= 0.99518 train_rmse= 0.76761 val_loss= 1.49614 val_rmse= 1.15298 \t\ttime= 0.61235\n",
      "[*] Epoch: 2398 train_loss= 0.98264 train_rmse= 0.75186 val_loss= 1.49512 val_rmse= 1.15344 \t\ttime= 0.58345\n",
      "[*] Epoch: 2399 train_loss= 0.99220 train_rmse= 0.75805 val_loss= 1.49583 val_rmse= 1.15448 \t\ttime= 0.63929\n",
      "[*] Epoch: 2400 train_loss= 0.98619 train_rmse= 0.76073 val_loss= 1.49693 val_rmse= 1.15488 \t\ttime= 0.78988\n",
      "[*] Epoch: 2401 train_loss= 0.99770 train_rmse= 0.76554 val_loss= 1.49702 val_rmse= 1.15362 \t\ttime= 0.65824\n",
      "[*] Epoch: 2402 train_loss= 0.99968 train_rmse= 0.76359 val_loss= 1.49461 val_rmse= 1.15043 \t\ttime= 0.69813\n",
      "[*] Epoch: 2403 train_loss= 0.98875 train_rmse= 0.75726 val_loss= 1.49290 val_rmse= 1.14921 \t\ttime= 0.63331\n",
      "[*] Epoch: 2404 train_loss= 0.99599 train_rmse= 0.76773 val_loss= 1.49210 val_rmse= 1.14905 \t\ttime= 0.61136\n",
      "[*] Epoch: 2405 train_loss= 0.99753 train_rmse= 0.76177 val_loss= 1.49308 val_rmse= 1.15019 \t\ttime= 0.64926\n",
      "[*] Epoch: 2406 train_loss= 1.02370 train_rmse= 0.78563 val_loss= 1.49042 val_rmse= 1.14672 \t\ttime= 0.68616\n",
      "[*] Epoch: 2407 train_loss= 0.98090 train_rmse= 0.75035 val_loss= 1.49007 val_rmse= 1.14451 \t\ttime= 0.58045\n",
      "[*] Epoch: 2408 train_loss= 0.99443 train_rmse= 0.75773 val_loss= 1.48993 val_rmse= 1.14414 \t\ttime= 0.63231\n",
      "[*] Epoch: 2409 train_loss= 0.98982 train_rmse= 0.75770 val_loss= 1.48959 val_rmse= 1.14471 \t\ttime= 0.67719\n",
      "[*] Epoch: 2410 train_loss= 0.99756 train_rmse= 0.76176 val_loss= 1.49021 val_rmse= 1.14650 \t\ttime= 0.84230\n",
      "[*] Epoch: 2411 train_loss= 0.99772 train_rmse= 0.76477 val_loss= 1.49250 val_rmse= 1.14888 \t\ttime= 0.89320\n",
      "[*] Epoch: 2412 train_loss= 0.99992 train_rmse= 0.76711 val_loss= 1.49418 val_rmse= 1.15042 \t\ttime= 0.78518\n",
      "[*] Epoch: 2413 train_loss= 0.98459 train_rmse= 0.75532 val_loss= 1.49471 val_rmse= 1.15078 \t\ttime= 0.85019\n",
      "[*] Epoch: 2414 train_loss= 0.98011 train_rmse= 0.75284 val_loss= 1.49350 val_rmse= 1.14948 \t\ttime= 0.76864\n",
      "[*] Epoch: 2415 train_loss= 0.97256 train_rmse= 0.74577 val_loss= 1.49312 val_rmse= 1.14945 \t\ttime= 0.81311\n",
      "[*] Epoch: 2416 train_loss= 0.99297 train_rmse= 0.76270 val_loss= 1.49372 val_rmse= 1.15037 \t\ttime= 0.92835\n",
      "[*] Epoch: 2417 train_loss= 0.98528 train_rmse= 0.75685 val_loss= 1.49221 val_rmse= 1.14905 \t\ttime= 0.79588\n",
      "[*] Epoch: 2418 train_loss= 0.99349 train_rmse= 0.76192 val_loss= 1.48922 val_rmse= 1.14567 \t\ttime= 0.81435\n",
      "[*] Epoch: 2419 train_loss= 0.97980 train_rmse= 0.74966 val_loss= 1.48942 val_rmse= 1.14486 \t\ttime= 0.74357\n",
      "[*] Epoch: 2420 train_loss= 0.99325 train_rmse= 0.76518 val_loss= 1.49204 val_rmse= 1.14761 \t\ttime= 0.83917\n",
      "[*] Epoch: 2421 train_loss= 0.99662 train_rmse= 0.76588 val_loss= 1.49281 val_rmse= 1.14905 \t\ttime= 0.82484\n",
      "[*] Epoch: 2422 train_loss= 0.99830 train_rmse= 0.76368 val_loss= 1.49170 val_rmse= 1.14779 \t\ttime= 0.68713\n",
      "[*] Epoch: 2423 train_loss= 1.00513 train_rmse= 0.76982 val_loss= 1.48892 val_rmse= 1.14557 \t\ttime= 0.88785\n",
      "[*] Epoch: 2424 train_loss= 0.99267 train_rmse= 0.75863 val_loss= 1.48862 val_rmse= 1.14593 \t\ttime= 0.70196\n",
      "[*] Epoch: 2425 train_loss= 0.98817 train_rmse= 0.75736 val_loss= 1.49092 val_rmse= 1.14894 \t\ttime= 0.78263\n",
      "[*] Epoch: 2426 train_loss= 0.97678 train_rmse= 0.74976 val_loss= 1.49425 val_rmse= 1.15294 \t\ttime= 0.87978\n",
      "[*] Epoch: 2427 train_loss= 0.99710 train_rmse= 0.76587 val_loss= 1.49536 val_rmse= 1.15410 \t\ttime= 0.74068\n",
      "[*] Epoch: 2428 train_loss= 0.99924 train_rmse= 0.76351 val_loss= 1.49605 val_rmse= 1.15363 \t\ttime= 0.79446\n",
      "[*] Epoch: 2429 train_loss= 0.99391 train_rmse= 0.76161 val_loss= 1.49522 val_rmse= 1.15216 \t\ttime= 0.78911\n",
      "[*] Epoch: 2430 train_loss= 0.98770 train_rmse= 0.75887 val_loss= 1.49492 val_rmse= 1.15154 \t\ttime= 0.84411\n",
      "[*] Epoch: 2431 train_loss= 0.98632 train_rmse= 0.75620 val_loss= 1.49410 val_rmse= 1.15183 \t\ttime= 0.78142\n",
      "[*] Epoch: 2432 train_loss= 0.99784 train_rmse= 0.76934 val_loss= 1.49179 val_rmse= 1.15077 \t\ttime= 0.74850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Epoch: 2433 train_loss= 0.98983 train_rmse= 0.76095 val_loss= 1.49105 val_rmse= 1.15036 \t\ttime= 0.84914\n",
      "[*] Epoch: 2434 train_loss= 0.99364 train_rmse= 0.75950 val_loss= 1.49072 val_rmse= 1.14873 \t\ttime= 0.74975\n",
      "[*] Epoch: 2435 train_loss= 0.99182 train_rmse= 0.76452 val_loss= 1.49234 val_rmse= 1.14818 \t\ttime= 0.86148\n",
      "[*] Epoch: 2436 train_loss= 0.98585 train_rmse= 0.75549 val_loss= 1.49358 val_rmse= 1.14918 \t\ttime= 0.87762\n",
      "[*] Epoch: 2437 train_loss= 0.98502 train_rmse= 0.75266 val_loss= 1.49467 val_rmse= 1.15173 \t\ttime= 0.73830\n",
      "[*] Epoch: 2438 train_loss= 1.00325 train_rmse= 0.76607 val_loss= 1.49532 val_rmse= 1.15407 \t\ttime= 0.78244\n",
      "[*] Epoch: 2439 train_loss= 0.97970 train_rmse= 0.75513 val_loss= 1.49571 val_rmse= 1.15416 \t\ttime= 0.76511\n",
      "[*] Epoch: 2440 train_loss= 0.99737 train_rmse= 0.76521 val_loss= 1.49581 val_rmse= 1.15210 \t\ttime= 0.81356\n",
      "[*] Epoch: 2441 train_loss= 0.99244 train_rmse= 0.76453 val_loss= 1.49614 val_rmse= 1.15064 \t\ttime= 0.81682\n",
      "[*] Epoch: 2442 train_loss= 0.98826 train_rmse= 0.75673 val_loss= 1.49414 val_rmse= 1.14923 \t\ttime= 0.78196\n",
      "[*] Epoch: 2443 train_loss= 0.99071 train_rmse= 0.75775 val_loss= 1.49330 val_rmse= 1.15002 \t\ttime= 0.77976\n",
      "[*] Epoch: 2444 train_loss= 0.99420 train_rmse= 0.75941 val_loss= 1.49384 val_rmse= 1.15133 \t\ttime= 0.73999\n",
      "[*] Epoch: 2445 train_loss= 0.98636 train_rmse= 0.75676 val_loss= 1.49433 val_rmse= 1.15134 \t\ttime= 0.79286\n",
      "[*] Epoch: 2446 train_loss= 0.99439 train_rmse= 0.75755 val_loss= 1.49596 val_rmse= 1.15197 \t\ttime= 0.87969\n",
      "[*] Epoch: 2447 train_loss= 0.96993 train_rmse= 0.74690 val_loss= 1.49765 val_rmse= 1.15334 \t\ttime= 0.76054\n",
      "[*] Epoch: 2448 train_loss= 0.98797 train_rmse= 0.75451 val_loss= 1.49829 val_rmse= 1.15527 \t\ttime= 0.88822\n",
      "[*] Epoch: 2449 train_loss= 0.98843 train_rmse= 0.76010 val_loss= 1.49783 val_rmse= 1.15640 \t\ttime= 0.77692\n",
      "[*] Epoch: 2450 train_loss= 0.98323 train_rmse= 0.75813 val_loss= 1.49718 val_rmse= 1.15642 \t\ttime= 0.78320\n",
      "[*] Epoch: 2451 train_loss= 1.01918 train_rmse= 0.77945 val_loss= 1.49429 val_rmse= 1.15327 \t\ttime= 0.89029\n",
      "[*] Epoch: 2452 train_loss= 0.99280 train_rmse= 0.77086 val_loss= 1.49203 val_rmse= 1.14974 \t\ttime= 0.76567\n",
      "[*] Epoch: 2453 train_loss= 1.00004 train_rmse= 0.77061 val_loss= 1.48846 val_rmse= 1.14425 \t\ttime= 0.85340\n",
      "[*] Epoch: 2454 train_loss= 0.99304 train_rmse= 0.75840 val_loss= 1.48541 val_rmse= 1.14067 \t\ttime= 0.71127\n",
      "[*] Epoch: 2455 train_loss= 0.99634 train_rmse= 0.76117 val_loss= 1.48477 val_rmse= 1.14158 \t\ttime= 0.78167\n",
      "[*] Epoch: 2456 train_loss= 0.98289 train_rmse= 0.75067 val_loss= 1.48650 val_rmse= 1.14383 \t\ttime= 0.83783\n",
      "[*] Epoch: 2457 train_loss= 0.99322 train_rmse= 0.75474 val_loss= 1.49064 val_rmse= 1.14738 \t\ttime= 0.74370\n",
      "[*] Epoch: 2458 train_loss= 0.98503 train_rmse= 0.75540 val_loss= 1.49525 val_rmse= 1.15092 \t\ttime= 0.77417\n",
      "[*] Epoch: 2459 train_loss= 0.98259 train_rmse= 0.75369 val_loss= 1.49897 val_rmse= 1.15398 \t\ttime= 0.73401\n",
      "[*] Epoch: 2460 train_loss= 0.99198 train_rmse= 0.75774 val_loss= 1.50046 val_rmse= 1.15623 \t\ttime= 0.80903\n",
      "[*] Epoch: 2461 train_loss= 1.00514 train_rmse= 0.76846 val_loss= 1.49884 val_rmse= 1.15537 \t\ttime= 0.88644\n",
      "[*] Epoch: 2462 train_loss= 0.99514 train_rmse= 0.76189 val_loss= 1.49635 val_rmse= 1.15276 \t\ttime= 0.77348\n",
      "[*] Epoch: 2463 train_loss= 0.99881 train_rmse= 0.76573 val_loss= 1.49555 val_rmse= 1.15147 \t\ttime= 0.80118\n",
      "[*] Epoch: 2464 train_loss= 0.99123 train_rmse= 0.76168 val_loss= 1.49628 val_rmse= 1.15214 \t\ttime= 0.71985\n",
      "[*] Epoch: 2465 train_loss= 0.98845 train_rmse= 0.75990 val_loss= 1.49647 val_rmse= 1.15297 \t\ttime= 0.76276\n",
      "[*] Epoch: 2466 train_loss= 1.00159 train_rmse= 0.76480 val_loss= 1.49437 val_rmse= 1.15150 \t\ttime= 0.86170\n",
      "[*] Epoch: 2467 train_loss= 0.98517 train_rmse= 0.75796 val_loss= 1.49263 val_rmse= 1.15077 \t\ttime= 0.77860\n",
      "[*] Epoch: 2468 train_loss= 0.97749 train_rmse= 0.74903 val_loss= 1.49406 val_rmse= 1.15262 \t\ttime= 0.78974\n",
      "[*] Epoch: 2469 train_loss= 0.97398 train_rmse= 0.74572 val_loss= 1.49668 val_rmse= 1.15416 \t\ttime= 0.71388\n",
      "[*] Epoch: 2470 train_loss= 0.99755 train_rmse= 0.75857 val_loss= 1.49779 val_rmse= 1.15406 \t\ttime= 0.85801\n",
      "[*] Epoch: 2471 train_loss= 0.99017 train_rmse= 0.75818 val_loss= 1.49901 val_rmse= 1.15584 \t\ttime= 0.85824\n",
      "[*] Epoch: 2472 train_loss= 0.99243 train_rmse= 0.75854 val_loss= 1.49761 val_rmse= 1.15594 \t\ttime= 0.77110\n",
      "[*] Epoch: 2473 train_loss= 0.99693 train_rmse= 0.76806 val_loss= 1.49518 val_rmse= 1.15410 \t\ttime= 0.79427\n",
      "[*] Epoch: 2474 train_loss= 0.99907 train_rmse= 0.76695 val_loss= 1.49421 val_rmse= 1.15224 \t\ttime= 0.70413\n",
      "[*] Epoch: 2475 train_loss= 1.00605 train_rmse= 0.77329 val_loss= 1.49223 val_rmse= 1.14849 \t\ttime= 0.80303\n",
      "[*] Epoch: 2476 train_loss= 0.98538 train_rmse= 0.75657 val_loss= 1.49086 val_rmse= 1.14622 \t\ttime= 0.89614\n",
      "[*] Epoch: 2477 train_loss= 0.99067 train_rmse= 0.75695 val_loss= 1.49260 val_rmse= 1.14832 \t\ttime= 0.76707\n",
      "[*] Epoch: 2478 train_loss= 0.99066 train_rmse= 0.75767 val_loss= 1.49371 val_rmse= 1.15110 \t\ttime= 0.76156\n",
      "[*] Epoch: 2479 train_loss= 0.98051 train_rmse= 0.75177 val_loss= 1.49472 val_rmse= 1.15431 \t\ttime= 0.77797\n",
      "[*] Epoch: 2480 train_loss= 0.98989 train_rmse= 0.75679 val_loss= 1.49714 val_rmse= 1.15584 \t\ttime= 0.78056\n",
      "[*] Epoch: 2481 train_loss= 0.98831 train_rmse= 0.76449 val_loss= 1.49998 val_rmse= 1.15715 \t\ttime= 0.82357\n",
      "[*] Epoch: 2482 train_loss= 0.99225 train_rmse= 0.76001 val_loss= 1.50099 val_rmse= 1.15782 \t\ttime= 0.76810\n",
      "[*] Epoch: 2483 train_loss= 0.98098 train_rmse= 0.75568 val_loss= 1.50123 val_rmse= 1.15920 \t\ttime= 0.80000\n",
      "[*] Epoch: 2484 train_loss= 1.00707 train_rmse= 0.77234 val_loss= 1.49773 val_rmse= 1.15690 \t\ttime= 0.74105\n",
      "[*] Epoch: 2485 train_loss= 0.98281 train_rmse= 0.75367 val_loss= 1.49546 val_rmse= 1.15520 \t\ttime= 0.79443\n",
      "[*] Epoch: 2486 train_loss= 0.98096 train_rmse= 0.75252 val_loss= 1.49438 val_rmse= 1.15288 \t\ttime= 0.89478\n",
      "[*] Epoch: 2487 train_loss= 0.99351 train_rmse= 0.76076 val_loss= 1.49256 val_rmse= 1.14939 \t\ttime= 0.73011\n",
      "[*] Epoch: 2488 train_loss= 0.99397 train_rmse= 0.76122 val_loss= 1.49055 val_rmse= 1.14587 \t\ttime= 0.84971\n",
      "[*] Epoch: 2489 train_loss= 0.98430 train_rmse= 0.75747 val_loss= 1.48738 val_rmse= 1.14315 \t\ttime= 0.78315\n",
      "[*] Epoch: 2490 train_loss= 1.00525 train_rmse= 0.77649 val_loss= 1.48538 val_rmse= 1.14270 \t\ttime= 0.81177\n",
      "[*] Epoch: 2491 train_loss= 0.97288 train_rmse= 0.74998 val_loss= 1.48501 val_rmse= 1.14346 \t\ttime= 0.84102\n",
      "[*] Epoch: 2492 train_loss= 1.01801 train_rmse= 0.77659 val_loss= 1.48709 val_rmse= 1.14490 \t\ttime= 0.77057\n",
      "[*] Epoch: 2493 train_loss= 0.98138 train_rmse= 0.74781 val_loss= 1.49347 val_rmse= 1.14961 \t\ttime= 0.80177\n",
      "[*] Epoch: 2494 train_loss= 0.96810 train_rmse= 0.74159 val_loss= 1.49925 val_rmse= 1.15523 \t\ttime= 0.74602\n",
      "[*] Epoch: 2495 train_loss= 0.99389 train_rmse= 0.76061 val_loss= 1.50163 val_rmse= 1.15905 \t\ttime= 0.83699\n",
      "[*] Epoch: 2496 train_loss= 0.97326 train_rmse= 0.75420 val_loss= 1.50450 val_rmse= 1.16342 \t\ttime= 0.86453\n",
      "[*] Epoch: 2497 train_loss= 0.99349 train_rmse= 0.76705 val_loss= 1.50459 val_rmse= 1.16414 \t\ttime= 0.76739\n",
      "[*] Epoch: 2498 train_loss= 0.99072 train_rmse= 0.76016 val_loss= 1.50402 val_rmse= 1.16357 \t\ttime= 0.81516\n",
      "[*] Epoch: 2499 train_loss= 1.01456 train_rmse= 0.78067 val_loss= 1.50042 val_rmse= 1.15933 \t\ttime= 0.71392\n",
      "[*] Epoch: 2500 train_loss= 0.99226 train_rmse= 0.75821 val_loss= 1.49660 val_rmse= 1.15384 \t\ttime= 0.85880\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "for epoch in range(NB_EPOCH):\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    # Run single weight update\n",
    "    # outs = sess.run([model.opt_op, model.loss, model.rmse], feed_dict=train_feed_dict)\n",
    "    # with exponential moving averages\n",
    "    outs = sess.run([model.training_op, model.loss, model.rmse], feed_dict=train_feed_dict)\n",
    "\n",
    "    train_avg_loss = outs[1]\n",
    "    train_rmse = outs[2]\n",
    "\n",
    "    val_avg_loss, val_rmse = sess.run([model.loss, model.rmse], feed_dict=val_feed_dict)\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(\"[*] Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(train_avg_loss),\n",
    "              \"train_rmse=\", \"{:.5f}\".format(train_rmse),\n",
    "              \"val_loss=\", \"{:.5f}\".format(val_avg_loss),\n",
    "              \"val_rmse=\", \"{:.5f}\".format(val_rmse),\n",
    "              \"\\t\\ttime=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if val_rmse < best_val_score:\n",
    "        best_val_score = val_rmse\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if epoch % 20 == 0 and WRITESUMMARY:\n",
    "        # Train set summary\n",
    "        summary = sess.run(merged_summary, feed_dict=train_feed_dict)\n",
    "        train_summary_writer.add_summary(summary, epoch)\n",
    "        train_summary_writer.flush()\n",
    "\n",
    "        # Validation set summary\n",
    "        summary = sess.run(merged_summary, feed_dict=val_feed_dict)\n",
    "        val_summary_writer.add_summary(summary, epoch)\n",
    "        val_summary_writer.flush()\n",
    "\n",
    "    if epoch % 100 == 0 and epoch > 1000 and not TESTING and False:\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, \"tmp/%s_seed%d.ckpt\" % (model.name, DATASEED), global_step=model.global_step)\n",
    "\n",
    "        # load polyak averages\n",
    "        variables_to_restore = model.variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        saver.restore(sess, save_path)\n",
    "\n",
    "        val_avg_loss, val_rmse = sess.run([model.loss, model.rmse], feed_dict=val_feed_dict)\n",
    "\n",
    "        print('polyak val loss = ', val_avg_loss)\n",
    "        print('polyak val rmse = ', val_rmse)\n",
    "\n",
    "        # Load back normal variables\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T09:22:39.978484Z",
     "start_time": "2021-06-28T09:22:15.496216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization Finished!\n",
      "best validation score = 1.0786374 at iteration 315\n",
      "WARNING:tensorflow:From C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from tmp/recommendersideinfogae.ckpt-2500\n",
      "polyak val loss =  1.4909396\n",
      "polyak val rmse =  1.1483742\n",
      "\n",
      "SETTINGS:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [-d {ml_100k,ml_1m,ml_10m,douban,yahoo_music,flixster}]\n",
      "                             [-lr LEARNING_RATE] [-e EPOCHS]\n",
      "                             [-hi HIDDEN HIDDEN] [-fhi FEAT_HIDDEN]\n",
      "                             [-ac {sum,stack}] [-do DROPOUT]\n",
      "                             [-nb NUM_BASIS_FUNCTIONS] [-ds DATA_SEED]\n",
      "                             [-sdir SUMMARIES_DIR] [-nsym | -nleft]\n",
      "                             [-f | -no_f] [-ws | -no_ws] [-t | -v]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: C:\\Users\\maoru\\AppData\\Roaming\\jupyter\\runtime\\kernel-200b2127-e931-4a62-ac6f-ecdd7d7e7bdb.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maoru\\Anaconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# store model including exponential moving averages\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"tmp/%s.ckpt\" % model.name, global_step=model.global_step)\n",
    "\n",
    "\n",
    "if VERBOSE:\n",
    "    print(\"\\nOptimization Finished!\")\n",
    "    print('best validation score =', best_val_score, 'at iteration', best_epoch)\n",
    "\n",
    "\n",
    "if TESTING:\n",
    "    test_avg_loss, test_rmse = sess.run([model.loss, model.rmse], feed_dict=test_feed_dict)\n",
    "    print('test loss = ', test_avg_loss)\n",
    "    print('test rmse = ', test_rmse)\n",
    "\n",
    "    # restore with polyak averages of parameters\n",
    "    variables_to_restore = model.variable_averages.variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    saver.restore(sess, save_path)\n",
    "\n",
    "    test_avg_loss, test_rmse = sess.run([model.loss, model.rmse], feed_dict=test_feed_dict)\n",
    "    print('polyak test loss = ', test_avg_loss)\n",
    "    print('polyak test rmse = ', test_rmse)\n",
    "\n",
    "else:\n",
    "    # restore with polyak averages of parameters\n",
    "    variables_to_restore = model.variable_averages.variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    saver.restore(sess, save_path)\n",
    "\n",
    "    val_avg_loss, val_rmse = sess.run([model.loss, model.rmse], feed_dict=val_feed_dict)\n",
    "    print('polyak val loss = ', val_avg_loss)\n",
    "    print('polyak val rmse = ', val_rmse)\n",
    "\n",
    "print('\\nSETTINGS:\\n')\n",
    "for key, val in sorted(vars(ap.parse_args()).iteritems()):\n",
    "    print(key, val)\n",
    "\n",
    "print('global seed = ', seed)\n",
    "\n",
    "# For parsing results from file\n",
    "results = vars(ap.parse_args()).copy()\n",
    "results.update({'best_val_score': float(best_val_score), 'best_epoch': best_epoch})\n",
    "print(json.dumps(results))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "374.75px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
