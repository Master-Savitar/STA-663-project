{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RShUfvg4FGWs"
   },
   "source": [
    "# The Netﬂix Prize dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlKdPC_TFGWu",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-preprocessing\" data-toc-modified-id=\"Data-preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-All-the-Data\" data-toc-modified-id=\"Importing-All-the-Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Importing All the Data</a></span></li><li><span><a href=\"#Separating-Training-Set-and-Test-Set\" data-toc-modified-id=\"Separating-Training-Set-and-Test-Set-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Separating Training Set and Test Set</a></span></li></ul></li><li><span><a href=\"#Explanatory-data-analysis\" data-toc-modified-id=\"Explanatory-data-analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Explanatory data analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Movie-release-date\" data-toc-modified-id=\"Movie-release-date-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Movie release date</a></span></li><li><span><a href=\"#Rating-distribution\" data-toc-modified-id=\"Rating-distribution-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Rating distribution</a></span></li><li><span><a href=\"#Movie-Rated-Date\" data-toc-modified-id=\"Movie-Rated-Date-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Movie Rated Date</a></span></li></ul></li><li><span><a href=\"#Model-One---Biased-Matrix-Factorization\" data-toc-modified-id=\"Model-One---Biased-Matrix-Factorization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model One - Biased Matrix Factorization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Implementation-of-the-model\" data-toc-modified-id=\"Implementation-of-the-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Implementation of the model</a></span></li><li><span><a href=\"#Fitting-the-model\" data-toc-modified-id=\"Fitting-the-model-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Fitting the model</a></span></li><li><span><a href=\"#Evaluating-the-model\" data-toc-modified-id=\"Evaluating-the-model-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Evaluating the model</a></span></li></ul></li><li><span><a href=\"#Model-Two---AutoRec\" data-toc-modified-id=\"Model-Two---AutoRec-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model Two - AutoRec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Implementation-of-Model\" data-toc-modified-id=\"Implementation-of-Model-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Implementation of Model</a></span></li><li><span><a href=\"#Training-and-Evaluation\" data-toc-modified-id=\"Training-and-Evaluation-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Training and Evaluation</a></span></li></ul></li><li><span><a href=\"#Model-Three---Naive-Bayes\" data-toc-modified-id=\"Model-Three---Naive-Bayes-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Three - Naive Bayes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Implementation-of-the-model\" data-toc-modified-id=\"Implementation-of-the-model-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Implementation of the model</a></span></li><li><span><a href=\"#Evaluating-the-model\" data-toc-modified-id=\"Evaluating-the-model-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Evaluating the model</a></span></li></ul></li><li><span><a href=\"#Model-Four---Neural-Collaborative-Filtering\" data-toc-modified-id=\"Model-Four---Neural-Collaborative-Filtering-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model Four - Neural Collaborative Filtering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Implementation-of-the-model\" data-toc-modified-id=\"Implementation-of-the-model-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Implementation of the model</a></span></li><li><span><a href=\"#Training-the-model-based-on-the-data-split\" data-toc-modified-id=\"Training-the-model-based-on-the-data-split-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Training the model based on the data split</a></span></li><li><span><a href=\"#Plotting-training-and-validation-loss\" data-toc-modified-id=\"Plotting-training-and-validation-loss-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Plotting training and validation loss</a></span></li><li><span><a href=\"#Showing-top-10-movie-recommendations-to-a-user\" data-toc-modified-id=\"Showing-top-10-movie-recommendations-to-a-user-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Showing top 10 movie recommendations to a user</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgQErOtMFGWw"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T09:35:34.957751Z",
     "start_time": "2021-04-01T09:35:30.761756Z"
    },
    "id": "uUmy6GdtFGWx",
    "outputId": "a24ef1c4-c3d9-4867-cac0-3d15f865dd69"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# show plots automatically\n",
    "%matplotlib inline\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm5tkrVjFGWy"
   },
   "outputs": [],
   "source": [
    "# To ensure plots can be displayed properly in Google Colab\n",
    "def enable_plotly_in_cell():\n",
    "    import IPython\n",
    "    from plotly.offline import init_notebook_mode\n",
    "    display(IPython.core.display.HTML('''<script src=\"/static/components/requirejs/require.js\"></script>'''))\n",
    "    init_notebook_mode(connected=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUvGYud4FGWy"
   },
   "source": [
    "### Importing All the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T09:22:31.117260Z",
     "start_time": "2021-04-01T09:22:06.265686Z"
    },
    "id": "BIps8GbnFGWz"
   },
   "outputs": [],
   "source": [
    "# DataFrame to store all imported data\n",
    "data = open('data.csv', mode='w')\n",
    "\n",
    "files = ['combined_data_1.txt',\n",
    "         'combined_data_2.txt',\n",
    "         'combined_data_3.txt',\n",
    "         'combined_data_4.txt'\n",
    "        ]\n",
    "\n",
    "# Remove the line with movie_id: and add a new column of movie_id\n",
    "# Combine all data files into a csv file\n",
    "for file in files:\n",
    "    print(\"Opening file: {}\".format(file))\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.endswith(':'):\n",
    "                movie_id = line.replace(':', '')\n",
    "            else:\n",
    "                data.write(movie_id + ',' + line)\n",
    "                data.write('\\n')\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T09:48:58.750231Z",
     "start_time": "2021-04-01T09:47:15.433674Z"
    },
    "id": "qHsJv6rvFGWz",
    "outputId": "fb2b5c10-5e45-4521-95ee-3f09b4079230"
   },
   "outputs": [],
   "source": [
    "# Read all data into a pd dataframe\n",
    "dataset = pd.read_csv('data.csv', \n",
    "                 names=['movieid', 'userid', 'rating', 'date'])\n",
    "print(dataset.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T09:53:09.470607Z",
     "start_time": "2021-04-01T09:53:04.302889Z"
    },
    "id": "l2GiJVuvFGW0",
    "outputId": "927149f5-7085-4eaa-8626-e46a275cfb12"
   },
   "outputs": [],
   "source": [
    "# Check NaN Values\n",
    "print('number of NaN values in the dataset:', \n",
    "      sum(dataset.isnull().any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9xithwxFGW0"
   },
   "source": [
    "There are 480189 users and 17770 distinct movies in the Netflix Prize dataset. \n",
    "\n",
    "user_id ranges from 1 to 2649429 with gaps. \n",
    "\n",
    "movie_id ranges from 1 to 17770 sequentially. \n",
    "\n",
    "rating can be any integer from 1 to 5. date shows the date when the rating was made and it has the format of YYYY-MM-DD. As a user may rate multiple movies. The dataset has 111631710 rows in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nBvlS-uFGW1"
   },
   "source": [
    "### Separating Training Set and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6TshvABFuum"
   },
   "source": [
    "For my model, I use the qualifying set as the test set of the model. However, the qualifying data is mixed with training dataset and contained in the above dataset as well. \n",
    "\n",
    "Therefore, I first separate the training set and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiDarT3lFGW1"
   },
   "outputs": [],
   "source": [
    "subdata = open('probe.csv', mode='w')\n",
    "files = [\"probe.txt\"]\n",
    "\n",
    "# Remove the line with movie_id: and add a new column of movie_id\n",
    "# Combine all data files into a csv file\n",
    "for file in files:\n",
    "  print(\"Opening file: {}\".format(file))\n",
    "  with open(file) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.endswith(':'):\n",
    "            movie_id = line.replace(':', '')\n",
    "        else:\n",
    "            subdata.write(movie_id + ',' + line)\n",
    "            subdata.write('\\n')\n",
    "subdata.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wk5yWpXJFGW1"
   },
   "outputs": [],
   "source": [
    "probeset = pd.read_csv(\"probe.csv\", names=[\"movie_id\", \"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cU8--lEFGW1"
   },
   "outputs": [],
   "source": [
    "test = probeset.merge(df, on = ['user_id', 'movie_id'],how='left')\n",
    "train = pd.concat([df, test, test]).drop_duplicates(keep = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6CJJgT5FGW2"
   },
   "outputs": [],
   "source": [
    "# Store training set and test set as separate csv files\n",
    "train.to_csv(path_or_buf = 'train.csv')\n",
    "test.to_csv(path_or_buf = 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFY5RUWAFGW2"
   },
   "source": [
    "## Explanatory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ootO0N5qFGW2"
   },
   "source": [
    "### Movie release date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T09:56:28.929259Z",
     "start_time": "2021-04-01T09:56:28.750260Z"
    },
    "id": "bnG3FrEbFGW2",
    "outputId": "408c9a4b-3872-4eed-952a-5e7b61d6e49d"
   },
   "outputs": [],
   "source": [
    "df_title = pd.read_csv('movie_titles.csv', \n",
    "                       encoding = \"ISO-8859-1\", header = None, \n",
    "                       names = ['movieid', 'Year', 'Name'])\n",
    "df_title.set_index('movieid', inplace = True)\n",
    "df_title.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T09:56:31.202475Z",
     "start_time": "2021-04-01T09:56:30.020391Z"
    },
    "id": "N8bvjQv3FGW3",
    "outputId": "09619ea2-0e18-4567-af94-5fcacadda42a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "data = df_title['Year'].value_counts().sort_index()\n",
    "\n",
    "# Create trace\n",
    "trace = go.Scatter(x = data.index,\n",
    "                   y = data.values,\n",
    "                   marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = dict(title = '{} Movies Grouped By Year Of Release'.format(df_title.shape[0]),\n",
    "              xaxis = dict(title = 'Release Year'),\n",
    "              yaxis = dict(title = 'Movies'))\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxHfz2YRFGW3"
   },
   "source": [
    "The number of movies released is stable around 0 during the time period from 1900 to around 1930. Then the number slowly starts to go up after 1930. After 1980, the number of movie released each year starts to rocket. Most movies are released around the time of 2000 in this dataset. This trend is in line with the development of movie industry as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T09:56:35.712803Z",
     "start_time": "2021-04-01T09:56:35.704797Z"
    },
    "id": "ZOoiC5gAFGW4"
   },
   "outputs": [],
   "source": [
    "del df_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iB8yoL3AFGW4"
   },
   "source": [
    "### Rating distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T09:58:03.752584Z",
     "start_time": "2021-04-01T09:56:39.814491Z"
    },
    "id": "BOmzFdupFGW4"
   },
   "outputs": [],
   "source": [
    "# Read all data into a pd dataframe\n",
    "df = pd.read_csv('data.csv', names=['movie_id', 'user_id', 'rating', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T10:05:31.763116Z",
     "start_time": "2021-04-01T10:05:30.630141Z"
    },
    "id": "646hFAM-FGW4",
    "outputId": "320b23aa-9196-45b3-e4c3-6b63b9c14422"
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "data = df['rating'].value_counts().sort_index(ascending=False)\n",
    "\n",
    "# Create trace\n",
    "trace = go.Bar(x = data.index,\n",
    "               text = ['{:.1f} %'.format(val) for val in (data.values / df.shape[0] * 100)],\n",
    "               textposition = 'auto',\n",
    "               textfont = dict(color = '#000000'),\n",
    "               y = data.values,\n",
    "               marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = dict(title = 'Distribution Of {} Netflix-Ratings'.format(df.shape[0]),\n",
    "              xaxis = dict(title = 'Rating'),\n",
    "              yaxis = dict(title = 'Count'))\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5gCN3Z_FGW5"
   },
   "source": [
    "Users rarely rate a movie to be 1 or 2. Most ratings are above 3 and 4 is the rating that is most frequently given. The distribution is probably biased, since only people liking the movies proceed to be customers and others presumably will leave the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5YVXbvXFGW5"
   },
   "source": [
    "### Movie Rated Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T10:05:54.065289Z",
     "start_time": "2021-04-01T10:05:42.542255Z"
    },
    "id": "243dLBUHFGW5",
    "outputId": "ea75ff2e-9eee-415e-f340-2eede1ea0f00"
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "data = df['date'].value_counts()\n",
    "data.sort_index(inplace=True)\n",
    "\n",
    "# Create trace\n",
    "trace = go.Scatter(x = data.index,\n",
    "                   y = data.values,\n",
    "                   marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = dict(title = '{} Movie-Ratings Grouped By Day'.format(df.shape[0]),\n",
    "              xaxis = dict(title = 'Date'),\n",
    "              yaxis = dict(title = 'Ratings'))\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NcJhfg4FGW6"
   },
   "source": [
    "From 2000 to 2003, Netflix has witness few to 0 daily ratings. After 2003, the daily ratings has its first small peak. After that, the daily rating numbers slowly starts to show a rising trend despite fluctuations. The number of ratings increases with time, and most ratings submited in 2005. It is worth noting that there are two unnormal peaks at 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T10:06:54.917800Z",
     "start_time": "2021-04-01T10:06:35.105023Z"
    },
    "id": "amWmzLgMFGW6",
    "outputId": "19ef1c11-b412-4b32-d50e-e86fe9b95665"
   },
   "outputs": [],
   "source": [
    "##### Ratings Per Movie #####\n",
    "# Get data\n",
    "data = df.groupby('movie_id')['rating'].count().clip(upper=9999)\n",
    "\n",
    "# Create trace\n",
    "trace = go.Histogram(x = data.values,\n",
    "                     name = 'Ratings',\n",
    "                     xbins = dict(start = 0,\n",
    "                                  end = 10000,\n",
    "                                  size = 100),\n",
    "                     marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = go.Layout(title = 'Distribution Of Ratings Per Movie (Clipped at 9999)',\n",
    "                   xaxis = dict(title = 'Ratings Per Movie'),\n",
    "                   yaxis = dict(title = 'Count'),\n",
    "                   bargap = 0.2)\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)\n",
    "\n",
    "\n",
    "\n",
    "##### Ratings Per User #####\n",
    "# Get data\n",
    "data = df.groupby('user_id')['rating'].count().clip(upper=199)\n",
    "\n",
    "# Create trace\n",
    "trace = go.Histogram(x = data.values,\n",
    "                     name = 'Ratings',\n",
    "                     xbins = dict(start = 0,\n",
    "                                  end = 200,\n",
    "                                  size = 2),\n",
    "                     marker = dict(color = '#db0000'))\n",
    "# Create layout\n",
    "layout = go.Layout(title = 'Distribution Of Ratings Per User (Clipped at 199)',\n",
    "                   xaxis = dict(title = 'Ratings Per User'),\n",
    "                   yaxis = dict(title = 'Count'),\n",
    "                   bargap = 0.2)\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wC0MhmneFGW7"
   },
   "source": [
    "Both the ratings per movie and the ratings per user have a near-perfect exponential decay. Only a few movies/users have a large number of ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gshNsJKwFGW7"
   },
   "source": [
    "## Model One - Biased Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fokUmIw5FGW8"
   },
   "source": [
    "Matrix Factorization aims to factorize the rating matrix R into a matrix U which is m × k and a matrix V which is n × k. The rows of  U and V are called latent factors, thus the name latent factor models. To explain in more detail, the ith row of U is called the User Factor while the ith row of V is called the item Factor. The ratings are approximated by multiplying matrices U and V."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoQIcl1oFGW8"
   },
   "source": [
    "&nbsp;\n",
    "As for Biased Matrix Factorization, I take each user’s and each item’s specific characteristics into consideration, which is the user bias and item bias, denoted in β and γ.\n",
    "\n",
    "The two models are abbreviated as MFALS(Matrix Factorization with Alternating Least Squares) and BMFALS(Biased Matrix Factorization with Alternating Least Squares) in the following scripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T10:06:59.584992Z",
     "start_time": "2021-04-01T10:06:59.565022Z"
    },
    "id": "Eg-4nyojFGW8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxIdYAlzFGW8"
   },
   "source": [
    "### Implementation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T10:07:01.010406Z",
     "start_time": "2021-04-01T10:07:00.857439Z"
    },
    "id": "Cf-myisLFGW8"
   },
   "outputs": [],
   "source": [
    "class BiasedMatrixFactor():\n",
    "    def __init__(self, k, lamda=0.1, num_iter=1000, bias=False, print_enabled=True):\n",
    "\n",
    "        self.lamda = lamda\n",
    "        self.num_iter = num_iter \n",
    "        self.k = k\n",
    "        self.bias = bias\n",
    "        self.print_enabled = print_enabled\n",
    "        \n",
    "      \n",
    "    def _Root_Mean_Square_Error(self):\n",
    "        \"\"\"\n",
    "        return\n",
    "        1. n*m np.ndarray: the difference between A and estimated value at each entry of A; \n",
    "        2. total loss, the sum of errors for all entries of A. \n",
    "        \"\"\"\n",
    "        if self.bias:\n",
    "            A_est = np.matmul(self.U, self.VT) + np.reshape(self.beta, (self.m_, 1)) + np.reshape(self.gamma, (1, self.n_))\n",
    "        else:\n",
    "            A_est = np.matmul(self.U, self.VT)\n",
    "        diff_error = np.subtract(self.A, A_est)\n",
    "        diff_error[self.nan_cell_mask] = 0       # ignore missing data\n",
    "        sq_error = np.square(diff_error).sum()\n",
    "        num_data = np.size(self.A) - len(self.nan_cell_mask[0])\n",
    "        rmse = np.sqrt(sq_error / num_data)\n",
    "        \n",
    "        return diff_error, rmse\n",
    "    \n",
    "    \n",
    "    def _ALS(self):\n",
    "        \"\"\" _ALS, should \n",
    "        1. update self.U, self.VT and \n",
    "        2. return a 1D np.ndarray which stores the loss after each iteration, \n",
    "            the length of the list should be the number of iteration, \n",
    "            because I include the loss before optimization, i.e. the loss when num_iter = 0 \n",
    "        \"\"\"\n",
    "        loss_list = np.array([])\n",
    "        temp_A = self.A.copy()\n",
    "        self.m_ = np.size(self.A, 0)\n",
    "        self.n_ = np.size(self.A, 1)\n",
    "        \n",
    "        if self.bias:\n",
    "            self.beta = np.random.rand(self.m_, 1)            # beta represents items' biases\n",
    "            self.gamma = np.random.rand(self.n_, 1)           # gamma represents users' biases\n",
    "        \n",
    "        for n in range(self.num_iter):\n",
    "            # calculate the root mean square error\n",
    "            if self.bias:\n",
    "                # update V^T\n",
    "                A_beta = temp_A - np.reshape(self.beta, (self.m_, 1))\n",
    "                U_extend = np.hstack((np.ones(shape=(self.m_, 1)), self.U))\n",
    "                self.VT = np.dot(np.dot(np.linalg.pinv(np.dot(U_extend.T, U_extend)+self.lamda*np.identity(self.k+1)), U_extend.T), A_beta)\n",
    "                self.gamma = self.VT[0, :]\n",
    "                self.VT = self.VT[1:, :]\n",
    "\n",
    "                # update U\n",
    "                A_gamma = temp_A - np.reshape(self.gamma, (1, self.n_))\n",
    "                VT_extend = np.vstack((np.ones(shape=(1, self.n_)), self.VT))\n",
    "                self.U = np.dot(np.dot(np.linalg.pinv(np.dot(VT_extend, VT_extend.T)+self.lamda*np.identity(self.k+1)), VT_extend), A_gamma.T).T\n",
    "                self.beta = self.U[:, 0]\n",
    "                self.U = self.U[:, 1:]\n",
    "                \n",
    "            else:\n",
    "                # update V^T\n",
    "                self.VT = np.dot(np.dot(np.linalg.pinv(np.dot(self.U.T, self.U)+self.lamda*np.identity(self.k)), self.U.T), temp_A)\n",
    "\n",
    "                # update U\n",
    "                self.U = np.dot(np.dot(np.linalg.pinv(np.dot(self.VT, self.VT.T)+self.lamda*np.identity(self.k)), self.VT), temp_A.T).T\n",
    "            \n",
    "            # update the missing values in rating matrix\n",
    "            if self.bias:\n",
    "                A_est = np.matmul(self.U, self.VT) + np.reshape(self.beta, (self.m_, 1)) + np.reshape(self.gamma, (1, self.n_))\n",
    "            else:\n",
    "                A_est = np.matmul(self.U, self.VT)      \n",
    "            temp_A[self.nan_cell_mask] = A_est[self.nan_cell_mask]\n",
    "            diff, rmse = self._Root_Mean_Square_Error()\n",
    "            \n",
    "            # print total loss if needed\n",
    "            if self.print_enabled:\n",
    "                print(\"iteration \" + str(n) + \": \" + str(rmse))\n",
    "                \n",
    "            loss_list = np.append(loss_list, rmse)\n",
    "            \n",
    "        return loss_list        \n",
    "    \n",
    "    \n",
    "    def impute(self):\n",
    "        ## impute the missing values\n",
    "        self.nan_cell_mask = np.where(np.isnan(self.A))\n",
    "        # calculate row mean, if all entries in one row is nan, use population mean\n",
    "        self.A[np.all(np.isnan(self.A), axis=1)] = np.nanmean(self.A)\n",
    "        self.rowmean = np.nanmean(self.A, axis=1)\n",
    "        self.A[self.nan_cell_mask] = np.take(self.rowmean,  self.nan_cell_mask[0])\n",
    "\n",
    "        \n",
    "    def fit(self, A, init_U = None, init_VT = None):\n",
    "        ## initialize U, V \n",
    "        self.A = A.copy()\n",
    "        np.random.seed(0)\n",
    "        self.impute()    # call the method impute\n",
    "        if init_U is None: \n",
    "            self.U = np.random.rand(np.size(self.A, 0), self.k)\n",
    "        else:\n",
    "            self.U = init_U\n",
    "            \n",
    "        if init_VT is None:\n",
    "            self.VT = np.random.rand(self.k, np.size(self.A, 1))\n",
    "        else:\n",
    "            self.VT = init_VT\n",
    "\n",
    "        loss_list = self._ALS()\n",
    "            \n",
    "        return self.U, self.VT, loss_list       # loss_list stores the loss after each iteration\n",
    "    \n",
    "    def predict(self, user_idxes, movie_idxes):\n",
    "        if self.bias:\n",
    "            A_est = np.matmul(self.U, self.VT) + np.reshape(self.beta, (self.m_, 1)) + np.reshape(self.gamma, (1, self.n_))\n",
    "        else:\n",
    "            A_est = np.matmul(self.U, self.VT)\n",
    "        \n",
    "        if len(user_idxes) == len(movie_idxes):\n",
    "            pred_list = [A_est[user_idxes[i]][movie_idxes[i]] for i in range(len(user_idxes))]\n",
    "        else:\n",
    "            raise InputError(\"Inputs must have the same length!!!\")\n",
    "            \n",
    "        return pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UufBo3zZFGW_"
   },
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZEF0FPZFGXB"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\", usecols=[1,2,3])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbYqt6FRFGXC"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv', usecols = [1,2,3])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2FaFkYgFGXC"
   },
   "outputs": [],
   "source": [
    "interval = 100000 # Load by user_id intervals\n",
    "pred_list = []\n",
    "biased_pred_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7zXlUUOFGXC"
   },
   "outputs": [],
   "source": [
    "for i in range(0, int(np.floor(train['user_id'].max() / interval)) + 1):\n",
    "    lower_limit = i*interval\n",
    "    upper_limit = min((i+1)*interval, train['user_id'].max())\n",
    "    \n",
    "    if i == np.floor(train['user_id'].max() / interval):\n",
    "        train_part = train[(train['user_id'] >= lower_limit) & (train['user_id'] <= upper_limit)]\n",
    "        test_part = test[(test['user_id'] >= lower_limit) & (test['user_id'] <= upper_limit)]\n",
    "    else:\n",
    "        train_part = train[(train['user_id'] >= lower_limit) & (train['user_id'] < upper_limit)]\n",
    "        test_part = test[(test['user_id'] >= lower_limit) & (test['user_id'] < upper_limit)]\n",
    "    \n",
    "    A_part = train_part.pivot(index=\"user_id\", columns='movie_id', values='rating')\n",
    "\n",
    "    # Train the MFALS model\n",
    "    MF = BiasedMatrixFactor(k=100, lamda=0.02, num_iter=100, print_enabled=True, bias=False)\n",
    "    U, VT, loss_hist = MF.fit(np.array(A_part))\n",
    "\n",
    "    # Train the BMFALS model\n",
    "    biased_MF = BiasedMatrixFactor(k=100, lamda=0.02, num_iter=100, print_enabled=True, bias=True)\n",
    "    biased_U, biased_VT, biased_loss_hist = biased_MF.fit(np.array(A_part))\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(test_part[\"user_id\"])\n",
    "    test_part[\"user_id\"] = le.transform(test_part[\"user_id\"])\n",
    "    le.fit(test_part[\"movie_id\"])\n",
    "    test_part[\"movie_id\"] = le.transform(test_part[\"movie_id\"])\n",
    "\n",
    "    ## MFASL prediction\n",
    "     pred_ratings = MF.predict(np.array(test_part.user_id), np.array(test_part.movie_id))\n",
    "     pred_list = np.append(pred_list, pred_ratings)\n",
    "\n",
    "    ## BMFALS prediction\n",
    "    biased_pred_ratings = biased_MF.predict(np.array(test_part.user_id), np.array(test_part.movie_id))\n",
    "    biased_pred_list = np.append(biased_pred_list, biased_pred_ratings)\n",
    "\n",
    "    print(\"---------The \", i + 1, \" interval is processed----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OV16lSfoFGXC"
   },
   "source": [
    "### Evaluating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPj1xIrFFGXC"
   },
   "outputs": [],
   "source": [
    "# ALS without biases\n",
    "mse = mean_squared_error(np.array(test['rating']), pred_list)\n",
    "wo_b_rmse = math.sqrt(mse)\n",
    "print(\"The RMSE for ALS model without biases is \", wo_b_rmse)\n",
    "\n",
    "# ALS with biases\n",
    "b_mse = mean_squared_error(np.array(test['rating']), biased_pred_list)\n",
    "b_rmse = math.sqrt(b_mse)\n",
    "print(\"The RMSE for ALS model with biases is \", b_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fad0dt6bFGXD"
   },
   "source": [
    "In terms of loss, it is clear that the implementation Matrix Factorization with bias has a much better performance than the implementation without bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4W133Je_GaLJ"
   },
   "outputs": [],
   "source": [
    "# ALS without biases\n",
    "mse = mean_squared_error(np.array(test['rating']), pred_list)\n",
    "wo_b_rmse = math.sqrt(mse)\n",
    "print(\"The RMSE for ALS model without biases is \", wo_b_rmse)\n",
    "\n",
    "# ALS with biases\n",
    "b_mse = mean_squared_error(np.array(test['rating']), biased_pred_list)\n",
    "b_rmse = math.sqrt(b_mse)\n",
    "print(\"The RMSE for ALS model with biases is \", b_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hz_6aQUGah7"
   },
   "source": [
    "The two models have similar RMSE when it comes to predicting, with the RMSE score of Matrix Factorization without biases being slightly lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Utru5xkAFGXD"
   },
   "source": [
    "## Model Two - AutoRec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fuqO0e9FGXD"
   },
   "source": [
    "AutoRec, short for AutoRec: Autoencoders Meet Collaborative Filtering (Sedhain et.al, 2015), is a neural network-based model in Recommender System. Its main idea is to reduce the dimension by neural network, whose output is the low-dimensional vector with denser information. After obtaining the encoded output, it is fed into the decoder and return a predicted user/item vector as final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5vsAEL4FGXD"
   },
   "source": [
    "&nbsp;\n",
    "The model consists of two parts: encoder and decoder. Encoder compresses the sparse and large user/item vector into dense vectors. On the other side, Decoder returns the vector with same dimensions as the input user/item vector. As for their structure, the encoder is built with a Linear layer and a Sigmoid Layer, whereas the decoder only contains a Linear layer.\n",
    "Apart from being used as a model for recommendation, it could also be viewed as a tool for dimension reduction based on the idea of Autoencoder (D.E. Rumelhart, G.E. Hinton, et.al, 1986). And this could be utilized in further models such as Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T10:49:50.916033Z",
     "start_time": "2021-04-01T10:49:49.644871Z"
    },
    "id": "Sly5CXwKFGXD"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXJbmhMBFGXE"
   },
   "source": [
    "### Implementation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T11:29:51.837244Z",
     "start_time": "2021-04-01T11:29:51.808280Z"
    },
    "id": "JWK03tg1FGXE"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, user_num, item_num, num_latent_variables, penalty_lambda = 1, prog_type='item-based'):\n",
    "        self.prog_type = prog_type\n",
    "        self.penalty_lambda = penalty_lambda\n",
    "        self.loss_list = []\n",
    "        self.val_loss = []\n",
    "        self.best_loss = np.inf\n",
    "        super(Autoencoder, self).__init__()\n",
    "        if prog_type == 'item-based':\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(user_num, num_latent_variables),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(num_latent_variables, user_num),\n",
    "            )\n",
    "        elif prog_type == 'user-based':\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(item_num, num_latent_variables),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(num_latent_variables, item_num),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "    def loss(self, loss_fn, optimizer, decoded, target, cell_mask):\n",
    "        orig_loss = loss_fn(decoded[cell_mask], target[cell_mask])\n",
    "        loss = torch.sqrt(1 / len(cell_mask[0]) * orig_loss)\n",
    "        regularization_loss = 0\n",
    "\n",
    "        for param in optimizer.param_groups:\n",
    "            for w_param in param['params']:\n",
    "                if w_param.data.dim() == 2:\n",
    "                    regularization_loss += torch.t(w_param.data).pow(2).sum()\n",
    "\n",
    "        return orig_loss, loss, loss + self.penalty_lambda * regularization_loss * 0.5\n",
    "\n",
    "    def train_model(self, device, optimizer, train_data, epochs, data_num, test_data, val_num, batch_size_train, prog_type='item-based', log_interval=10, save_model=True):\n",
    "        self.train()\n",
    "        loss_fn = nn.MSELoss(reduction='sum')\n",
    "        rmse = 0\n",
    "        val_loss_per_epoch = 0\n",
    "        cell_mask_train = torch.nonzero(train_data, as_tuple=True)\n",
    "        for epoch in range(epochs):\n",
    "            train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_data, train_data), batch_size=batch_size_train, shuffle=True)\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                cell_mask = torch.nonzero(data, as_tuple=True)\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                _, decoded = self(data)\n",
    "                orig_loss, loss, loss_pred = self.loss(loss_fn, optimizer, decoded, target, cell_mask)\n",
    "                loss_pred.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if batch_idx % log_interval == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "                rmse = rmse + orig_loss.item()\n",
    "            with torch.no_grad():\n",
    "                _, pred = self.predict(device, train_data)\n",
    "\n",
    "                pred[cell_mask_train] = train_data[cell_mask_train]\n",
    "                train_data = pred\n",
    "                if prog_type == 'item-based':\n",
    "                    pred_test = pred[(test_data[1, :], test_data[0, :])]\n",
    "                else:\n",
    "                    pred_test = pred[(test_data[0, :], test_data[1, :])]\n",
    "\n",
    "                val_loss_per_epoch = loss_fn(pred_test, torch.Tensor(test_data[2, :]).to(device))\n",
    "\n",
    "                rmse = rmse / data_num\n",
    "\n",
    "                val_loss_per_epoch = torch.sqrt(1 / val_num * val_loss_per_epoch).item()\n",
    "                print(\"--------Epoch {}-------\\n RMSE: {:.6f}\\t Val_loss: {:.6f}\\n\".format(epoch, rmse, val_loss_per_epoch))       \n",
    "                if save_model:\n",
    "                    if val_loss_per_epoch < self.best_loss:\n",
    "                        self.best_loss = val_loss_per_epoch\n",
    "                        torch.save(self.state_dict(), \"Autorec.pt\")\n",
    "                        print(\"Best model saved at val loss of {:.6f}.\\n\".format(val_loss_per_epoch))\n",
    "\n",
    "            self.loss_list.append(rmse)\n",
    "            self.val_loss.append(val_loss_per_epoch)\n",
    "        \n",
    "        return self.loss_list, self.val_loss, rmse\n",
    "    \n",
    "    def predict(self, device, input_data):\n",
    "        self.eval()\n",
    "        encoded, pred = self(input_data)\n",
    "        \n",
    "        negetive_index = torch.where(pred < 0)\n",
    "        pred[negetive_index] = 0\n",
    "        \n",
    "        return encoded, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o58mAqfzFGXE"
   },
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WD3ZxE_0FGXE"
   },
   "outputs": [],
   "source": [
    "train = train.reset_index()\n",
    "test = test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxMT202FFGXE"
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train[\"user_id\"])\n",
    "train[\"user_id\"] = pd.Series(le.transform(train[\"user_id\"]))\n",
    "test[\"user_id\"] = pd.Series(le.transform(test[\"user_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rS5NA7gsKPqw"
   },
   "source": [
    "First, I fit the model of user-based AutoRec Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQxSyMycFGXF"
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size_train = 80\n",
    "batch_size_test = 500\n",
    "epochs = 40\n",
    "lr = 1e-3\n",
    "gamma = 1e-4\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "interval = 50000\n",
    "save_model = True\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "prog_type = 'user-based'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tF_RQbqdFGXF"
   },
   "outputs": [],
   "source": [
    "movie_num = train['movie_id'].nunique()\n",
    "user_num = train['user_id'].nunique()\n",
    "user_index = train['user_id'].unique()\n",
    "movie_index = train['movie_id'].unique()\n",
    "data_num = len(train)\n",
    "if prog_type == 'item-based':\n",
    "    \n",
    "    A = sparse.coo_matrix((train.rating, (train.user_id, train.movie_id-1)))\n",
    "    del train\n",
    "    \n",
    "    test_matrix = sparse.coo_matrix((test.rating, (test.user_id, test.movie_id-1)))\n",
    "    del test\n",
    "    \n",
    "else:\n",
    "    \n",
    "    A = sparse.coo_matrix((train.rating, (train.user_id, train.movie_id-1)))\n",
    "    del train\n",
    "    \n",
    "    test_matrix = sparse.coo_matrix((test.rating, (test.user_id, test.movie_id)))\n",
    "    del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXFYDxTNFGXF"
   },
   "outputs": [],
   "source": [
    "model = Autoencoder(user_num=A.shape[0], item_num=A.shape[1], num_latent_variables=1024, penalty_lambda=1, prog_type=prog_type).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=gamma)\n",
    "\n",
    "if prog_type == 'item-based':\n",
    "    for i in range(1, int(np.floor(user_num / interval) + 1)):\n",
    "        random_index = random.sample(movie_index.tolist(), interval)\n",
    "        index = np.where(np.in1d(A.col, random_index))\n",
    "        test_index = np.where(np.in1d(test_matrix.col, random_index))\n",
    "        col_le = preprocessing.LabelEncoder()\n",
    "        col_le.fit(A.col[index])\n",
    " \n",
    "        A_part = torch.sparse_coo_tensor(torch.Tensor(np.vstack((A.row[index], col_le.transform(A.col[index])))), torch.Tensor(A.data[index]), torch.Size((user_num, interval))).to(device).to_dense()    \n",
    "\n",
    "        test_data = np.vstack((test_matrix.row[test_index], col_le.transform(test_matrix.col[test_index]), test_matrix.data[test_index]))\n",
    "        val_num = len(test_index[0])\n",
    "        loss_1, val_loss, rmse = model.train_model(device, optimizer, A_part.T, epochs, data_num, test_data, val_num, batch_size_train, 'item-based', log_interval)\n",
    "    \n",
    "elif prog_type == 'user-based':\n",
    "    for i in range(1, int(np.floor(user_num / interval) + 1)):\n",
    "        random_index = random.sample(user_index.tolist(), interval)\n",
    "        index = np.where(np.in1d(A.row, random_index))\n",
    "        test_index = np.where(np.in1d(test_matrix.row, random_index))\n",
    "        col_le = preprocessing.LabelEncoder()\n",
    "        col_le.fit(A.row[index])\n",
    " \n",
    "        A_part = torch.sparse_coo_tensor(torch.Tensor(np.vstack((col_le.transform(A.row[index]), A.col[index]))), torch.Tensor(A.data[index]), torch.Size((interval, movie_num))).to(device).to_dense()    \n",
    "    \n",
    "        test_data = np.vstack((col_le.transform(test_matrix.row[test_index]), test_matrix.col[test_index]-1, test_matrix.data[test_index]))\n",
    "        val_num = len(test_index[0])\n",
    "        loss_1, val_loss, rmse = model.train_model(device, optimizer, A_part, epochs, data_num, test_data, val_num, batch_size_train, 'user-based', log_interval)\n",
    "\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ryM93hlKw9i"
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "Print(\"The RMSE score for user-based AutoRec Model is \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2gBN8mNKYK5"
   },
   "source": [
    "Next, I fit the model of item-based AutoRec Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gU6qzzLGKeA1"
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size_train = 80\n",
    "batch_size_test = 500\n",
    "epochs = 40\n",
    "lr = 1e-3\n",
    "gamma = 1e-4\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "interval = 50000\n",
    "save_model = True\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "prog_type = 'item-based'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdU0FlCuKj_J"
   },
   "outputs": [],
   "source": [
    "model = Autoencoder(user_num=A.shape[0], item_num=A.shape[1], num_latent_variables=1024, penalty_lambda=1, prog_type=prog_type).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=gamma)\n",
    "\n",
    "if prog_type == 'item-based':\n",
    "    for i in range(1, int(np.floor(user_num / interval) + 1)):\n",
    "        random_index = random.sample(movie_index.tolist(), interval)\n",
    "        index = np.where(np.in1d(A.col, random_index))\n",
    "        test_index = np.where(np.in1d(test_matrix.col, random_index))\n",
    "        col_le = preprocessing.LabelEncoder()\n",
    "        col_le.fit(A.col[index])\n",
    " \n",
    "        A_part = torch.sparse_coo_tensor(torch.Tensor(np.vstack((A.row[index], col_le.transform(A.col[index])))), torch.Tensor(A.data[index]), torch.Size((user_num, interval))).to(device).to_dense()    \n",
    "\n",
    "        test_data = np.vstack((test_matrix.row[test_index], col_le.transform(test_matrix.col[test_index]), test_matrix.data[test_index]))\n",
    "        val_num = len(test_index[0])\n",
    "        loss_1, val_loss = model.train_model(device, optimizer, A_part.T, epochs, data_num, test_data, val_num, batch_size_train, 'item-based', log_interval)\n",
    "    \n",
    "elif prog_type == 'user-based':\n",
    "    for i in range(1, int(np.floor(user_num / interval) + 1)):\n",
    "        random_index = random.sample(user_index.tolist(), interval)\n",
    "        index = np.where(np.in1d(A.row, random_index))\n",
    "        test_index = np.where(np.in1d(test_matrix.row, random_index))\n",
    "        col_le = preprocessing.LabelEncoder()\n",
    "        col_le.fit(A.row[index])\n",
    " \n",
    "        A_part = torch.sparse_coo_tensor(torch.Tensor(np.vstack((col_le.transform(A.row[index]), A.col[index]))), torch.Tensor(A.data[index]), torch.Size((interval, movie_num))).to(device).to_dense()    \n",
    "    \n",
    "        test_data = np.vstack((col_le.transform(test_matrix.row[test_index]), test_matrix.col[test_index]-1, test_matrix.data[test_index]))\n",
    "        val_num = len(test_index[0])\n",
    "        loss_1, val_loss = model.train_model(device, optimizer, A_part, epochs, data_num, test_data, val_num, batch_size_train, 'user-based', log_interval)\n",
    "\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRMwInwdQDVi"
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "Print(\"The RMSE score for item-based AutoRec Model is \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsBtuuZBFGXF"
   },
   "source": [
    "## Model Three - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL3qRpGiFGXG"
   },
   "source": [
    "I consider the Naïve Bayes collaborative filtering algorithm for the dataset because Naive Bayes is best suited for categorical input variables. The ratings in the Netflix Prize dataset are five integers, which can be treated as five categories. Moreover, Naive Bayes can handle the problem of overfitting and MemoryError. It also achieve better results in accuracy and performance, which makes it favorable in real-life applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Y2Dh7FHFGXG"
   },
   "source": [
    "### Implementation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_kC_cH_FGXG"
   },
   "outputs": [],
   "source": [
    "class NaiveBayesCF():\n",
    "\n",
    "    def __init__(self, train_user, train_item, train_data, test_user, test_item, test_label, alpha=0.1):\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.train_user = train_user\n",
    "        self.train_item = train_item\n",
    "        self.rating = train_data\n",
    "        self.values = np.unique(train_data)\n",
    "        self.test_user = test_user\n",
    "        self.test_item = test_item\n",
    "        self.test_label = test_label\n",
    "    \n",
    "    \n",
    "    def _Root_Mean_Square_Error(self):\n",
    "        \"\"\"\n",
    "        return\n",
    "        1. n*m np.ndarray: the difference between A and estimated value at each entry of A; \n",
    "        2. total loss, the sum of errors for all entries of A. \n",
    "        \"\"\"\n",
    "        _, _, pred_list, _, _ = self.predict(user_idxes=self.test_user,\n",
    "                                 movie_idxes=self.test_item)\n",
    "        diff_error = pred_list - self.test_label\n",
    "        rmse = np.sqrt(np.square(diff_error).sum() / len(pred_list))\n",
    "        \n",
    "        return diff_error, rmse                    \n",
    "                        \n",
    "    def predict(self, user_idxes, movie_idxes, method=\"weight_average\"):\n",
    "        \"\"\"\n",
    "        method includes \"weight_average\" and \"argmax\"\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        if len(user_idxes) == len(movie_idxes):\n",
    "            pred_user = []\n",
    "            pred_item = []\n",
    "            pred_rating = []\n",
    "            pred_list = []\n",
    "            pred_likeli = []\n",
    "            pred_prior = []\n",
    "            for user_id in np.unique(user_idxes):\n",
    "                u_id = np.where(self.train_user == user_id)\n",
    "                \n",
    "                I_u = movie_idxes[user_idxes == user_id]\n",
    "                rating_u = self.rating[u_id]\n",
    "                item_u = self.train_item[u_id]\n",
    "                \n",
    "                for item in I_u:      # Iu is the movie indexes\n",
    "                    tmp_j = np.where(self.train_item == item)\n",
    "                    tmp_rating = self.rating[tmp_j]\n",
    "                    probability = []\n",
    "                    for (s, vs) in enumerate(self.values):\n",
    "                        prior = 1.0 * (sum(tmp_rating == vs) + self.alpha) / (len(tmp_rating) + self.alpha * len(self.values))\n",
    "                        \n",
    "                        tmp_cond_id = np.where(((self.rating == vs) & (self.train_item == item)))\n",
    "                        tmp_cond_user = self.train_user[tmp_cond_id]    # find all users who have specified ratings for item j to be vs\n",
    "                        tmp_cond_rating = self.rating[tmp_cond_id]      # find all ratings whose raters have specified item j's rating to be vs\n",
    "                        tmp_cond_item = self.train_item[tmp_cond_id]    # find all item\n",
    " \n",
    "                        likelihood = float(1.0)\n",
    "                        for k in np.unique(self.train_item[u_id]):\n",
    "                            tmp_k_u = np.where(item_u == k)\n",
    "                            u_k_rating = np.max(rating_u[tmp_k_u])       # find user u's ratings for item k\n",
    "                            tmp_k = np.where(tmp_cond_item == k)        # find other users who have ratings for item k\n",
    "                            tmp_rating_k = tmp_cond_rating[tmp_k]        # find their ratings\n",
    "                            \n",
    "                            likelihood *= (1.0 * (sum(tmp_rating_k == u_k_rating) + self.alpha) / (len(tmp_k[0]) + self.alpha * len(self.values)))\n",
    "\n",
    "                        probability.append(prior*likelihood)\n",
    "                        pred_prior.append(prior)\n",
    "                        pred_likeli.append(likelihood)\n",
    "                        \n",
    "                    if self.method == \"weight_average\":\n",
    "                        pred_list.append((np.array(probability)*self.values).sum()/(sum(probability))) if sum(probability) != 0 else pred_list.append(2.5)\n",
    "                    elif self.method == \"argmax\":\n",
    "                        pred_list.append(self.values[(probability == max(probability))][0])\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"Cannot use provided optimization method!!!\")\n",
    "                    pred_item.append(item)\n",
    "                    pred_user.append(user_id)\n",
    "                    \n",
    "        else:\n",
    "            raise InputError(\"Inputs must have the same length!!!\")\n",
    "            \n",
    "        return pred_user, pred_item, pred_list, pred_prior, pred_likeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgoPFFPXFGXG"
   },
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5p72viaKFGXH"
   },
   "source": [
    "In order to handle the problem of overfitting, the method of Laplacian smoothing is commonly used. Instead of estimating P(ruj = vs) in a straightforward way, I smooth it with a Laplacian smoothing parameter 𝛼. Here I testified different values from 0.01 to 1 to see how it influences the final result and I find the model performs best when 𝛼 is set to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad83ykR9FGXH"
   },
   "outputs": [],
   "source": [
    "NB = NaiveBayesCF(train['user_id'].values, train['movie_id'].values, train['rating'].values, \n",
    "                  test['user_id'].values, test['movie_id'].values, test['rating'].values, \n",
    "                  alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-EPn75rFGXH"
   },
   "outputs": [],
   "source": [
    "# calculating the rmse\n",
    "_, rmse = NB._Root_Mean_Square_Error()\n",
    "print(\"The RMSE on the test set is \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3rzr3qLFGXH"
   },
   "source": [
    "Since time is quite finite, I only run my user-based Naïve Bayes model on the test set. Just as what I have mentioned above, to overcome low computation efficiency, I divide the test set into twelve parts based on user-id and combine the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL_A7qY5FGXH"
   },
   "source": [
    "## Model Four - Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXdh6tooFGXI"
   },
   "source": [
    "This example demonstrates Neural Collaborative filtering using the Netflix dataset\n",
    "to recommend movies to users.\n",
    "\n",
    "The steps in the model are as follows:\n",
    "\n",
    "1. Map user ID to a \"user vector\" via an embedding matrix\n",
    "2. Map movie ID to a \"movie vector\" via an embedding matrix\n",
    "3. Compute the dot product between the user vector and movie vector, to obtain\n",
    "the a match score between the user and the movie (predicted rating).\n",
    "4. Train the embeddings via gradient descent using all known user-movie pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNK-p43_FGXI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WucQ-AUiFGXI"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv', usecols = [1,2,3])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qrM7aJWFGXJ"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\", usecols=[1,2,3])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naz2zHiSFGXJ"
   },
   "outputs": [],
   "source": [
    "trainids=train['user_id'].unique().tolist()\n",
    "testids=test['user_id'].unique().tolist()\n",
    "trainids.extend(testids)\n",
    "UserIds=list(set(trainids))\n",
    "del trainids,testids\n",
    "\n",
    "\n",
    "trainids=train['movie_id'].unique().tolist()\n",
    "testids=test['movie_id'].unique().tolist()\n",
    "trainids.extend(testids)\n",
    "MovieIds=list(set(trainids))\n",
    "del trainids,testids\n",
    "\n",
    "print(len(UserIds),len(MovieIds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6Dz348lFGXJ"
   },
   "source": [
    "First, need to perform some preprocessing to encode users and movies as integer indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3j-5TLcnFGXJ"
   },
   "outputs": [],
   "source": [
    "user2user_encoded = {x: i for i, x in enumerate(UserIds)}\n",
    "movie2movie_encoded = {x: i for i, x in enumerate(MovieIds)}\n",
    "\n",
    "test[\"user\"] = test[\"user_id\"].map(user2user_encoded)\n",
    "test[\"movie\"] = test[\"movie_id\"].map(movie2movie_encoded)\n",
    "train[\"user\"] = train[\"user_id\"].map(user2user_encoded)\n",
    "train[\"movie\"] = train[\"movie_id\"].map(movie2movie_encoded)\n",
    "\n",
    "num_users = len(UserIds)\n",
    "num_movies = len(MovieIds)\n",
    "train[\"Rating\"] = train[\"Rating\"].values.astype(float)\n",
    "test[\"Rating\"] = test[\"Rating\"].values.astype(float)\n",
    "\n",
    "min_rating = 1\n",
    "max_rating = 5\n",
    "\n",
    "print(\n",
    "    \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n",
    "        num_users, num_movies, min_rating, max_rating\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dUhQVQuFGXJ"
   },
   "outputs": [],
   "source": [
    "trainX = train[[\"user\", \"movie\"]].values.astype(int)\n",
    "# Normalize the targets between 0 and 1. Makes it easy to train.\n",
    "trainy = train[\"Rating\"].apply(lambda x: (x - 1) / (5 - 1)).values\n",
    "\n",
    "testX = test[[\"user\", \"movie\"]].values.astype(int)\n",
    "# Normalize the targets between 0 and 1. Makes it easy to train.\n",
    "testy = test[\"Rating\"].apply(lambda x: (x - 1) / (5 - 1)).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3Xjyk0VFGXK"
   },
   "source": [
    "### Implementation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNUgsa5aFGXK"
   },
   "source": [
    "I embed both users and movies in to 50-dimensional vectors.\n",
    "\n",
    "The model computes a match score between user and movie embeddings via a dot product,\n",
    "and adds a per-movie and per-user bias. The match score is scaled to the `[0, 1]`\n",
    "interval via a sigmoid (since the ratings are normalized to this range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0p4eR8YFGXK"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "\n",
    "class RecommenderNet(keras.Model):\n",
    "    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n",
    "        super(RecommenderNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.user_bias = layers.Embedding(num_users, 1)\n",
    "        self.movie_embedding = layers.Embedding(\n",
    "            num_movies,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.movie_bias = layers.Embedding(num_movies, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        movie_vector = self.movie_embedding(inputs[:, 1])\n",
    "        movie_bias = self.movie_bias(inputs[:, 1])\n",
    "        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n",
    "        # Add all the components (including bias)\n",
    "        x = dot_user_movie + user_bias + movie_bias\n",
    "        # The sigmoid activation forces the rating to between 0 and 1\n",
    "        return tf.nn.sigmoid(x)\n",
    "\n",
    "\n",
    "model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.Adam(lr=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LOAy1d_FGXK"
   },
   "source": [
    "### Training the model based on the data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCqZrNaiFGXK"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=trainX,\n",
    "    y=trainy,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=2,\n",
    "    validation_data=(testX, testy),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cxC5UxmFGXL"
   },
   "outputs": [],
   "source": [
    "model.save_weights('my_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWLL4N9RFGXL"
   },
   "source": [
    "### Plotting training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8IKojKRFGXL"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCsCgthAFGXL"
   },
   "source": [
    "### Showing top 10 movie recommendations to a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7tZkRGwFGXL"
   },
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv(\"movie_titles.csv\",encoding = \"ISO-8859-1\")\n",
    "movie_df=movie_df.iloc[:,[0,1,2]]\n",
    "movie_df.columns=['MovieId','Time','MovieTitle']\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgGYCfshFGXL"
   },
   "outputs": [],
   "source": [
    "user_id = test.user_id.sample(1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjmbQytoFGXM"
   },
   "outputs": [],
   "source": [
    "movies_watched_by_user = test[test.user_id == user_id]\n",
    "movies_watched_by_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kN0diF1AFGXM"
   },
   "outputs": [],
   "source": [
    "movies_not_watched = movie_df[\n",
    "    ~movie_df[\"movie_id\"].isin(movies_watched_by_user.movie_id.values)\n",
    "][\"movie_id\"]\n",
    "movies_not_watched = list(\n",
    "    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))\n",
    ")\n",
    "movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\n",
    "user_encoder = user2user_encoded.get(user_id)\n",
    "user_movie_array = np.hstack(\n",
    "    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjZy5QqkFGXM"
   },
   "outputs": [],
   "source": [
    "user_encoded2user = {x: i for i, x in enumerate(UserIds)}\n",
    "movie_encoded2movie = {i:x for i, x in enumerate(MovieIds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkFJEjaqFGXM"
   },
   "outputs": [],
   "source": [
    "ratings = model.predict(user_movie_array).flatten()\n",
    "top_ratings_indices = ratings.argsort()[-10:][::-1]\n",
    "recommended_movie_ids = [\n",
    "    movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices\n",
    "]\n",
    "\n",
    "print(\"Showing recommendations for user: {}\".format(user_id))\n",
    "print(\"====\" * 9)\n",
    "print(\"Movies with high ratings from user\")\n",
    "print(\"----\" * 8)\n",
    "top_movies_user = (\n",
    "    movies_watched_by_user.sort_values(by=\"Rating\", ascending=False)\n",
    "    .head(5)\n",
    "    .MovieId.values\n",
    ")\n",
    "movie_df_rows = movie_df[movie_df[\"MovieId\"].isin(top_movies_user)]\n",
    "for row in movie_df_rows.itertuples():\n",
    "    print(row.MovieTitle)\n",
    "\n",
    "print(\"----\" * 8)\n",
    "print(\"Top 10 movie recommendations\")\n",
    "print(\"----\" * 8)\n",
    "recommended_movies = movie_df[movie_df[\"MovieId\"].isin(recommended_movie_ids)]\n",
    "for row in recommended_movies.itertuples():\n",
    "    print(row.MovieTitle)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "The Matrix - Recommender System.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "368.417px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
